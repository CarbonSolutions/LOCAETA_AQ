{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preparing Data Center emissions into NEI-SMOKE format\n",
    "\n",
    "\n",
    "This is the emission processing for the second \"prong\" of LOCAETA's three decarbonization strategies, electrification. The goal of the study is assessing the impact of changes of energy demands given a grid scenario (e.g., current and 2050 grid). For example, what will be the emissions increases at power plants in the region of the facility(ies)? We explored the hypothetical scenario which essentially assumes that each power plant in the region (region defined by NREL’s Cambium model) marginally increases its output to collectively meet an additional 300MW load that would be incurred if the data center were connected to the grid.\n",
    "\n",
    "About the emissions generated from df, it is computed for each powerplant facility (EIS ID is a unique identifier). To include these emissions into NEI-SMOKE formated emissions, I need to split the emisisons using 2020 NEI emissions (per EIS ID and per SCC). \n",
    "\n",
    "Here is the emissions scenarios considered and the stretegy I use to prepare NEI-SMOKE style emissions for each scenario: \n",
    "\n",
    "* current_2020  - emissions can be prepared by splitting into each SCC by the NEI 2020 emissions weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Read Data Center emissions and NEI-SMOKE all point source shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "import os, sys\n",
    "\n",
    "# Add the path to the main package directory\n",
    "package_path = os.path.abspath('/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/LOCAETA_AQ')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "import emission_processing\n",
    "\n",
    "# Read the point source emissions\n",
    "original_emis ='/Users/yunhalee/Documents/LOCAETA/RCM/INMAP/evaldata_v1.6.1/2020_nei_emissions/combined_NEI2020_pt_oilgas_ptegu_ptnonipm_w_sectors.shp'\n",
    "nei_all_pt = gpd.read_file(original_emis)\n",
    "\n",
    "# Reset index to ensure proper comparison\n",
    "nei_all_pt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# rename the nei emissions\n",
    "pollutant_cols = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "\n",
    "col_dict = {}\n",
    "for poll in pollutant_cols:\n",
    "    col_dict[poll] = f'{poll}_nei'\n",
    "\n",
    "nei_all_pt.rename(columns = col_dict, inplace=True)\n",
    "\n",
    "nei_all_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "def reformat_DataCenter(df):\n",
    "\n",
    "    # drop the rows if eis is missing\n",
    "    df = df.dropna(subset=['eis'])\n",
    "\n",
    "    # columns I need\n",
    "    pollutant_cols = [col for col in df.columns if '_tons_final' in col]\n",
    "    base_cols = [col for col in df.columns if '_tons_base' in col]\n",
    "    DataCenter_col_names = pollutant_cols + base_cols + ['eis','cambium_gea']\n",
    "\n",
    "    # subset the dataframe \n",
    "    df = df[DataCenter_col_names] \n",
    "\n",
    "    # Define columns as integers\n",
    "    df = df.astype({'eis': 'int64'})\n",
    "\n",
    "    # rename \"eis\" to \"eis_id\"\n",
    "    df.rename(columns={'eis': 'EIS_ID'}, inplace=True)\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def find_minimal_unique_identifier_columns(df, max_combination_size=30):\n",
    "    \"\"\"\n",
    "    Finds the minimal set of columns that uniquely identify rows in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: pandas.DataFrame\n",
    "        max_combination_size: int, maximum number of columns to consider in combinations (avoid long runtime)\n",
    "\n",
    "    Returns:\n",
    "        List of column names or None\n",
    "    \"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    for r in range(1, min(len(cols), max_combination_size) + 1):\n",
    "        for combo in combinations(cols, r):\n",
    "            if not df.duplicated(subset=combo).any():\n",
    "                return list(combo)\n",
    "    return None\n",
    "\n",
    "def mapping_DataCenter_to_nei(nei_with_DataCenter, nei_all_pt, unique_identifier_columns, is_base):\n",
    "\n",
    "    if is_base: \n",
    "        # Column mapping between NEI and eGRID\n",
    "        pollutant_map = {\n",
    "            'NOx': 'NOx_tons_base',\n",
    "            'PM2_5': 'PM2.5_tons_base',\n",
    "            'VOC': 'VOC_tons_base',\n",
    "            'NH3': 'NH3_tons_base',\n",
    "            'SOx': 'SO2_tons_base'\n",
    "        }\n",
    "    else:\n",
    "        pollutant_map = {\n",
    "            'NOx': 'NOx_tons_final',\n",
    "            'PM2_5': 'PM2.5_tons_final',\n",
    "            'VOC': 'VOC_tons_final',\n",
    "            'NH3': 'NH3_tons_final',\n",
    "            'SOx': 'SO2_tons_final'\n",
    "        }\n",
    "\n",
    "    #print(\"before\", nei_with_DataCenter.head())\n",
    "    # Compute and apply split factors per pollutant\n",
    "    for nei_col, DataCenter_col in pollutant_map.items():\n",
    "\n",
    "        print (nei_col, DataCenter_col)\n",
    "        # Group sum for each pollutant by EIS_ID\n",
    "        total_by_eis = nei_with_DataCenter.groupby('EIS_ID')[f'{nei_col}_nei'].transform('sum')\n",
    "        nei_with_DataCenter[f'{nei_col}_total_by_eis'] = total_by_eis\n",
    "\n",
    "        # Default: compute split factor using NEI emissions\n",
    "        split_col = f'{nei_col}_split'\n",
    "        nei_with_DataCenter[split_col] = nei_with_DataCenter[f'{nei_col}_nei'] / total_by_eis.replace(0, pd.NA)\n",
    "\n",
    "        # Find EIS_IDs where total_by_eis is zero but DataCenter_col is non-zero\n",
    "        mask_zero_total = (total_by_eis == 0) & nei_with_DataCenter[DataCenter_col].notna() & (nei_with_DataCenter[DataCenter_col] != 0)\n",
    "\n",
    "        # For these EIS_IDs, assign equal split factor across matching rows\n",
    "        for eid in nei_with_DataCenter.loc[mask_zero_total, 'EIS_ID'].unique():\n",
    "            match_rows = nei_with_DataCenter['EIS_ID'] == eid\n",
    "            n_rows = match_rows.sum()\n",
    "            nei_with_DataCenter.loc[match_rows, split_col] = 1.0 / n_rows\n",
    "\n",
    "        # Now compute eGRID-scaled emissions and save as nei original name\n",
    "        nei_with_DataCenter[f'{nei_col}'] = nei_with_DataCenter[split_col] * nei_with_DataCenter[DataCenter_col]\n",
    "\n",
    "    # OPTIONAL: Drop intermediate split columns\n",
    "    #nei_with_DataCenter.drop(columns=[f'{k}_split' for k in pollutant_map], inplace=True)\n",
    "    # Merge results back into the full NEI dataset\n",
    "\n",
    "    # Merge results back into the full NEI dataset\n",
    "    nei_all_pt_final = nei_all_pt.merge(\n",
    "        nei_with_DataCenter[\n",
    "            unique_identifier_columns + [f'{k}' for k in pollutant_map]\n",
    "        ],\n",
    "        on=unique_identifier_columns,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    for k in pollutant_map:\n",
    "        nei_all_pt_final[f'{k}'] = nei_all_pt_final[f'{k}'].fillna(nei_all_pt_final[f'{k}_nei'])\n",
    "\n",
    "\n",
    "    return nei_all_pt_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DataCenter_dir_path = '/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/'\n",
    "DataCenter_list = [\"current_2020\"] \n",
    "is_base_emission = False\n",
    "\n",
    "# Optional: Set region to a specific cambium_gea value, or set to None to skip subsetting\n",
    "subset_region = \"NorthernGrid_West\"  # \"MISO_South\" # \"SPP_North\" #  \"PJM_East\" # \"MISO_Central\" # \"CAISO\"  #  ,  # Example: \"WECC\", \"ERCOT\", etc. Set to None to process all regions\n",
    "\n",
    "\n",
    "for DataCenter_name in DataCenter_list:\n",
    "\n",
    "    print (\"processing \", DataCenter_name)\n",
    "    DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "\n",
    "    egrid = pd.read_csv(DataCenter_file) \n",
    "    egrid = reformat_DataCenter(egrid)\n",
    "\n",
    "    if subset_region:\n",
    "        egrid = egrid[egrid['cambium_gea'] == subset_region]\n",
    "        if egrid.empty:\n",
    "            print(f\"No records found for region '{subset_region}' in {DataCenter_name}\")\n",
    "            continue\n",
    "        output_suffix = f\"_{subset_region}\"\n",
    "    else:\n",
    "        output_suffix = \"\"\n",
    "\n",
    "    # Filter NEI rows to only those that exist in eGRID\n",
    "    nei_with_DataCenter = nei_all_pt[nei_all_pt['EIS_ID'].isin(egrid['EIS_ID'])].copy()\n",
    "\n",
    "    # Subset only for necessary columns\n",
    "    nei_with_DataCenter.drop(columns=['height', 'diam',\n",
    "        'temp', 'velocity'], inplace=True)\n",
    "\n",
    "    unique_identifier_columns = find_minimal_unique_identifier_columns(nei_with_DataCenter)\n",
    "\n",
    "    if unique_identifier_columns:\n",
    "        print(\"Columns that uniquely identify rows:\", unique_identifier_columns)\n",
    "    else:\n",
    "        print(\"No combination of columns uniquely identifies rows.\")\n",
    "\n",
    "    print(\"filtering\", nei_with_DataCenter.shape)\n",
    "\n",
    "    # Merge eGRID emissions\n",
    "    nei_with_DataCenter = nei_with_DataCenter.merge(egrid, on='EIS_ID', how='left')\n",
    "\n",
    "    print(\"Merging egrid\", nei_with_DataCenter.shape)\n",
    "\n",
    "    nei_all_pt_final = mapping_DataCenter_to_nei(nei_with_DataCenter, nei_all_pt, unique_identifier_columns, is_base = is_base_emission)\n",
    "    \n",
    "    # Save outputs with region suffix\n",
    "    if is_base_emission:\n",
    "        filepath = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}_base_debugging_{output_suffix}.csv\")\n",
    "        nei_with_DataCenter.to_csv(filepath)\n",
    "\n",
    "        filepath = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}_base_{output_suffix}.shp\")\n",
    "        nei_all_pt_final.to_file(filepath, driver='ESRI Shapefile')\n",
    "    else:\n",
    "        filepath = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}_debugging_{output_suffix}.csv\")\n",
    "        nei_with_DataCenter.to_csv(filepath)\n",
    "\n",
    "        filepath = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}_{output_suffix}.shp\")\n",
    "        nei_all_pt_final.to_file(filepath, driver='ESRI Shapefile')\n",
    "\n",
    "    print(nei_all_pt_final.shape, nei_all_pt.shape)\n",
    "    print(nei_all_pt_final['PM2_5'].notna().sum())\n",
    "    print(nei_with_DataCenter['PM2_5'].notna().sum())\n",
    "    print(nei_with_DataCenter['PM2.5_tons_final'].notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define pollutant columns\n",
    "pollutants = ['VOC', 'NOx', 'SOx', 'PM2_5', 'NH3']\n",
    "pollutants = [item +\"_nei\" for item in pollutants]\n",
    "print(pollutants)\n",
    "\n",
    "# Step 1: Find EIS_ID & SCC pairs that include \"ptnonipm\"\n",
    "ptnonipm_keys = (\n",
    "    nei_with_DataCenter[nei_with_DataCenter['source_fil'].str.contains('ptnonipm', case=False, na=False)]\n",
    "    [['EIS_ID', 'SCC']]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# Step 2: Filter original dataframe to include only those EIS_ID & SCCs\n",
    "mask = nei_with_DataCenter.set_index(['EIS_ID', 'SCC']).index.isin(ptnonipm_keys.set_index(['EIS_ID', 'SCC']).index)\n",
    "filtered_df = nei_with_DataCenter[mask].copy()\n",
    "\n",
    "# Step 3: Group by sector and sum pollutants\n",
    "emissions_by_sector = filtered_df.groupby('source_fil')[pollutants].sum()\n",
    "\n",
    "# Step 4: Plot as bar chart (one bar per pollutant, grouped by sector)\n",
    "emissions_by_sector.plot(kind='bar', figsize=(12, 6))\n",
    "plt.ylabel('Total Emissions (tons/year)')\n",
    "plt.title('Total Emissions by Sector (where any EIS_ID+SCC includes \"ptnonipm\")')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Useful debugging script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique IDs\n",
    "ID_name = 'EIS_ID'\n",
    "org_DataCenter_list = egrid[ID_name].unique()\n",
    "DataCenter_list = nei_with_DataCenter[ID_name].unique()\n",
    "org_nei_list = nei_all_pt[ID_name].unique()\n",
    "nei_list = nei_all_pt_final[ID_name].unique()\n",
    "\n",
    "print(len(org_DataCenter_list), len(DataCenter_list), len(nei_list),len(org_nei_list))\n",
    "\n",
    "\n",
    "# find the rows where two columns are different\n",
    "missing_pm25_DataCenter = nei_with_DataCenter[\n",
    "    nei_with_DataCenter['PM2_5_DataCenter'].isna() & \n",
    "    nei_with_DataCenter['PM2.5_tons_final'].notna()\n",
    "]\n",
    "\n",
    "# filtering column based on string\n",
    "missing_pm25_DataCenter.filter(regex ='tons_final|total_by_eis')\n",
    "\n",
    "\n",
    "# get a certain ID facility\n",
    "print(nei_all_pt[(nei_all_pt['EIS_ID'] == 1028611)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Compare NEI2020 against Current grid base emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_map = {\n",
    "    'NOx': 'NOx_tons_base',\n",
    "    'PM2_5': 'PM2.5_tons_base',\n",
    "    'VOC': 'VOC_tons_base',\n",
    "    'NH3': 'NH3_tons_base',\n",
    "    'SOx': 'SO2_tons_base'\n",
    "\n",
    "}\n",
    "\n",
    "# Compute and apply split factors per pollutant\n",
    "for nei_col, DataCenter_col in pollutant_map.items():\n",
    "    # Scatter plot of total_by_eis vs. eGRID\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(\n",
    "        nei_with_DataCenter[f'{nei_col}_total_by_eis'],\n",
    "        nei_with_DataCenter[DataCenter_col] - nei_with_DataCenter[f'{nei_col}_total_by_eis'],\n",
    "        alpha=0.5\n",
    "    )\n",
    "    plt.xlabel(f\"Total {nei_col} by EIS_ID in NEI\")\n",
    "    plt.ylabel(f\"{DataCenter_col} - '{nei_col}_total_by_eis'\")\n",
    "    plt.title(f\"{nei_col} NEI Total vs difference with eGRID base Emissions\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Evaluate new egrid emissions formatted for NEI-SMOKE style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "\n",
    "DataCenter_dir_path = '/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/'\n",
    "egrids_list = [\"current_2020\"]\n",
    "\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_final_map = {\n",
    "    'NOx': 'NOx_tons_final',\n",
    "    'PM2_5': 'PM2.5_tons_final',\n",
    "    'VOC': 'VOC_tons_final',\n",
    "    'NH3': 'NH3_tons_final',\n",
    "    'SOx': 'SO2_tons_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_base_map = {\n",
    "    'NOx': 'NOx_tons_base',\n",
    "    'PM2_5': 'PM2.5_tons_base',\n",
    "    'VOC': 'VOC_tons_base',\n",
    "    'NH3': 'NH3_tons_base',\n",
    "    'SOx': 'SO2_tons_base'\n",
    "}\n",
    "\n",
    "is_base_emission = True\n",
    "\n",
    "# nei emissions column names\n",
    "pollutant_cols = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "\n",
    "for DataCenter_name in egrids_list:\n",
    "\n",
    "    if is_base_emission: \n",
    "        pollutant_map = pollutant_base_map\n",
    "        file_path = os.path.join(DataCenter_dir_path, f'{DataCenter_name}_base.shp')  \n",
    "    else:\n",
    "        pollutant_map = pollutant_final_map\n",
    "        file_path = os.path.join(DataCenter_dir_path, f'{DataCenter_name}.shp')  \n",
    "\n",
    "    # read emission scenario\n",
    "    final_DataCenter_emis = gpd.read_file(file_path) \n",
    "\n",
    "    # Reset index to ensure proper comparison\n",
    "    final_DataCenter_emis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Subset rows where actual egrid emissions are available (egrid ≠ nei for any pollutant)\n",
    "    mask = pd.concat([\n",
    "        final_DataCenter_emis[k] != final_DataCenter_emis[f'{k}_nei']\n",
    "        for k in pollutant_cols\n",
    "    ], axis=1).any(axis=1)\n",
    "\n",
    "    final_DataCenter_emis = final_DataCenter_emis[mask]\n",
    "\n",
    "    # Compute group sums for each pollutant by EIS_ID\n",
    "    group_sums = final_DataCenter_emis.groupby('EIS_ID')[[k for k in pollutant_cols]].sum().reset_index()\n",
    "\n",
    "    group_sums.head()\n",
    "\n",
    "    print (\"processing \", DataCenter_name)\n",
    "    original_DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "    original_DataCenter = pd.read_csv(original_DataCenter_file) \n",
    "    original_DataCenter = reformat_DataCenter(original_DataCenter)\n",
    "\n",
    "\n",
    "    # Merge for comparison\n",
    "    comparison_df = group_sums.merge(original_DataCenter, on='EIS_ID')\n",
    "    comparison_df.head()\n",
    "\n",
    "    # Scatter plots\n",
    "    for nei, egrid in pollutant_map.items():\n",
    "        x = comparison_df[nei]\n",
    "        y = comparison_df[egrid] - comparison_df[nei]\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(x, y, alpha=0.6, edgecolors='k')\n",
    "        plt.xlabel(f'{nei} NEI')\n",
    "        plt.ylabel(f'{nei} base - nei')\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.title(f'{nei} Comparison - {DataCenter_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Plot the total emissions (either final or diff_final) by Species for all egrid scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Dictionary to store total emissions\n",
    "DataCenter_sum = {}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_diff_map = {\n",
    "    'NOx': 'NOx_tons_dif_final',\n",
    "    'PM2_5': 'PM2.5_tons_dif_final',\n",
    "    'VOC': 'VOC_tons_dif_final',\n",
    "    'NH3': 'NH3_tons_dif_final',\n",
    "    'SOx': 'SO2_tons_dif_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_final_map = {\n",
    "    'NOx': 'NOx_tons_final',\n",
    "    'PM2_5': 'PM2.5_tons_final',\n",
    "    'VOC': 'VOC_tons_final',\n",
    "    'NH3': 'NH3_tons_final',\n",
    "    'SOx': 'SO2_tons_final'\n",
    "}\n",
    "\n",
    "pollutant_map = pollutant_diff_map\n",
    "\n",
    "egrids_list = [\"current_2020\"]\n",
    "\n",
    "for DataCenter_name in egrids_list: \n",
    "    print (\"processing \", DataCenter_name)\n",
    "    original_DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "    original_DataCenter = pd.read_csv(original_DataCenter_file) \n",
    "\n",
    "    # Store totals per pollutant\n",
    "    for nei, egrid in pollutant_map.items():\n",
    "        DataCenter_sum.setdefault(nei, {})[DataCenter_name] = original_DataCenter[egrid].sum()\n",
    "\n",
    "    print(DataCenter_sum)\n",
    "\n",
    "output_dir = f'/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/{egrids_list[0]}/'    \n",
    "os.makedirs(output_dir, exist_ok =True)\n",
    "\n",
    "# Convert to DataFrame: rows = pollutant, columns = egrid cases\n",
    "emissions_df = pd.DataFrame(DataCenter_sum).T  # Transpose so pollutants are rows\n",
    "\n",
    "# Plotting\n",
    "ax = emissions_df.plot(kind='bar', figsize=(10, 6))\n",
    "ax.set_ylabel(\"Total Emissions\")\n",
    "ax.set_title(\"Total Difference Emissions by Pollutant and eGRID Case\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Data Center Scenario\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, f'Total_Difference.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Emissions changes from current_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_diff_map = {\n",
    "    'NOx': 'NOx_tons_dif_final',\n",
    "    'PM2_5': 'PM2.5_tons_dif_final',\n",
    "    'VOC': 'VOC_tons_dif_final',\n",
    "    'NH3': 'NH3_tons_dif_final',\n",
    "    'SOx': 'SO2_tons_dif_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_final_map = {\n",
    "    'NOx': 'NOx_tons_final',\n",
    "    'PM2_5': 'PM2.5_tons_final',\n",
    "    'VOC': 'VOC_tons_final',\n",
    "    'NH3': 'NH3_tons_final',\n",
    "    'SOx': 'SO2_tons_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_base_map = {\n",
    "    'NOx': 'NOx_tons_base',\n",
    "    'PM2_5': 'PM2.5_tons_base',\n",
    "    'VOC': 'VOC_tons_base',\n",
    "    'NH3': 'NH3_tons_base',\n",
    "    'SOx': 'SO2_tons_base'\n",
    "}\n",
    "\n",
    "egrids_list = [\"current_2020\"] # ,\"decarb95_2050\",\"highREcost_2050\"]\n",
    "\n",
    "def convert_DataCenter_csv_to_df(egrids_list, pollutant_map):\n",
    "    # Dictionary to store total emissions\n",
    "    DataCenter_sum = {}\n",
    "\n",
    "\n",
    "    for DataCenter_name in egrids_list: \n",
    "        print (\"processing \", DataCenter_name)\n",
    "        original_DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "        original_DataCenter = pd.read_csv(original_DataCenter_file) \n",
    "\n",
    "        # Store totals per pollutant\n",
    "        for nei, egrid in pollutant_map.items():\n",
    "            DataCenter_sum.setdefault(nei, {})[DataCenter_name] = original_DataCenter[egrid].sum()\n",
    "\n",
    "        print(DataCenter_sum)\n",
    "\n",
    "    output_dir = f'/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/{egrids_list[0]}/'    \n",
    "    os.makedirs(output_dir, exist_ok =True)\n",
    "\n",
    "    # Convert to DataFrame: rows = pollutant, columns = egrid cases\n",
    "    emissions_df = pd.DataFrame(DataCenter_sum).T  # Transpose so pollutants are rows\n",
    "    return emissions_df\n",
    "\n",
    "emissions_diff = convert_DataCenter_csv_to_df(egrids_list, pollutant_diff_map)\n",
    "\n",
    "# Plotting\n",
    "ax = emissions_diff.plot(kind='bar', figsize=(10, 6))\n",
    "ax.set_ylabel(\"Total Emissions\")\n",
    "ax.set_title(\"Total Difference Emissions by Pollutant and eGRID Case\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"eGRID Case\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, f'Total_Difference.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emissions_base = convert_DataCenter_csv_to_df(egrids_list, pollutant_base_map)\n",
    "\n",
    "emissions_base['decarb95_2050 - current_2020'] = emissions_base['decarb95_2050'] - emissions_base['current_2020']\n",
    "emissions_base['highREcost_2050 - current_2020'] = emissions_base['highREcost_2050'] - emissions_base['current_2020']\n",
    "\n",
    "emissions_base.drop(columns=egrids_list, inplace=True)\n",
    "emissions_base\n",
    "\n",
    "output_dir = f'/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/{egrids_list[1]}_base/' \n",
    "os.makedirs(output_dir, exist_ok =True)\n",
    "\n",
    "# Plotting\n",
    "ax = emissions_base.plot(kind='bar', figsize=(10, 6))\n",
    "ax.set_ylabel(\"Total Emissions\")\n",
    "ax.set_title(\"Total Difference Emissions: future eGRID scenario - current_2020\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"eGRID scenarios\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, f'Total_Difference_based_on_current_2020.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
