{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preparing Data Center emissions into NEI-SMOKE format\n",
    "\n",
    "\n",
    "This is the emission processing for the second \"prong\" of LOCAETA's three decarbonization strategies, electrification. The goal of the study is assessing the impact of changes of energy demands given a grid scenario (e.g., current and 2050 grid). For example, what will be the emissions increases at power plants in the region of the facility(ies)? We explored the hypothetical scenario which essentially assumes that each power plant in the region (region defined by NREL’s Cambium model) marginally increases its output to collectively meet an additional 300MW load that would be incurred if the data center were connected to the grid.\n",
    "\n",
    "About the emissions generated from df, it is computed for each powerplant facility (EIS ID is a unique identifier). To include these emissions into NEI-SMOKE formated emissions, I need to split the emisisons using 2020 NEI emissions (per EIS ID and per SCC). \n",
    "\n",
    "Here is the emissions scenarios considered and the stretegy I use to prepare NEI-SMOKE style emissions for each scenario: \n",
    "\n",
    "* current_2020  - emissions can be prepared by splitting into each SCC by the NEI 2020 emissions weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Read Data Center emissions and NEI-SMOKE all point source shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "import os, sys\n",
    "\n",
    "# Add the path to the main package directory\n",
    "package_path = os.path.abspath('/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/LOCAETA_AQ')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "import emission_processing\n",
    "\n",
    "# Read the point source emissions\n",
    "original_emis ='/Users/yunhalee/Documents/LOCAETA/RCM/INMAP/evaldata_v1.6.1/2020_nei_emissions/combined_NEI2020_pt_oilgas_ptegu_ptnonipm_w_sectors.shp'\n",
    "nei_all_pt = gpd.read_file(original_emis)\n",
    "\n",
    "# Reset index to ensure proper comparison\n",
    "nei_all_pt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# rename the nei emissions\n",
    "pollutant_cols = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "\n",
    "col_dict = {}\n",
    "for poll in pollutant_cols:\n",
    "    col_dict[poll] = f'{poll}_nei'\n",
    "\n",
    "nei_all_pt.rename(columns = col_dict, inplace=True)\n",
    "\n",
    "nei_all_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "def reformat_DataCenter(df):\n",
    "\n",
    "    # columns I need\n",
    "    pollutant_cols = [col for col in df.columns if '_tons_final' in col]\n",
    "    base_cols = [col for col in df.columns if '_tons_base' in col]\n",
    "    DataCenter_col_names = pollutant_cols + base_cols + ['eis','cambium_gea','DOE/EIA ORIS plant or facility code']\n",
    "\n",
    "    # subset the dataframe \n",
    "    df = df[DataCenter_col_names] \n",
    "\n",
    "    # Total before grouping\n",
    "    total_before = df[pollutant_cols + base_cols].sum()\n",
    "\n",
    "    # drop the rows if eis is missing\n",
    "    df = df.dropna(subset=['eis'])\n",
    "\n",
    "    # Total after grouping\n",
    "    total_after = df[pollutant_cols + base_cols].sum()\n",
    "\n",
    "    # Define columns as integers\n",
    "    df = df.astype({'eis': 'int64', 'DOE/EIA ORIS plant or facility code': 'int64'})\n",
    "\n",
    "    # rename columns\n",
    "    df.rename(columns={'eis': 'EIS_ID', 'DOE/EIA ORIS plant or facility code': \"oris_ID\"}, inplace=True)\n",
    "\n",
    "    # Group by EIS_ID and aggregate emissions and cambium_gea\n",
    "    df_grouped = df.groupby('EIS_ID').agg({\n",
    "        **{col: 'sum' for col in pollutant_cols + base_cols},\n",
    "        'cambium_gea': 'first' \n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    # Check if Totals are preserved\n",
    "    print('before :', total_before, 'after: ', total_after) \n",
    "    #print(\"Reformatted df with cambium_gea:\", df_grouped.head())\n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def find_minimal_unique_identifier_columns(df, max_combination_size=30):\n",
    "    \"\"\"\n",
    "    Finds the minimal set of columns that uniquely identify rows in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: pandas.DataFrame\n",
    "        max_combination_size: int, maximum number of columns to consider in combinations (avoid long runtime)\n",
    "\n",
    "    Returns:\n",
    "        List of column names or None\n",
    "    \"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    for r in range(1, min(len(cols), max_combination_size) + 1):\n",
    "        for combo in combinations(cols, r):\n",
    "            if not df.duplicated(subset=combo).any():\n",
    "                return list(combo)\n",
    "    return None\n",
    "\n",
    "def mapping_DataCenter_to_nei(nei_with_DataCenter, nei_all_pt, unique_identifier_columns, is_base):\n",
    "\n",
    "    if is_base: \n",
    "        # Column mapping between NEI and eGRID\n",
    "        pollutant_map = {\n",
    "            'NOx': 'NOx_tons_base',\n",
    "            'PM2_5': 'PM2.5_tons_base',\n",
    "            'VOC': 'VOC_tons_base',\n",
    "            'NH3': 'NH3_tons_base',\n",
    "            'SOx': 'SO2_tons_base'\n",
    "        }\n",
    "    else:\n",
    "        pollutant_map = {\n",
    "            'NOx': 'NOx_tons_final',\n",
    "            'PM2_5': 'PM2.5_tons_final',\n",
    "            'VOC': 'VOC_tons_final',\n",
    "            'NH3': 'NH3_tons_final',\n",
    "            'SOx': 'SO2_tons_final'\n",
    "        }\n",
    "\n",
    "    # add Boolean to track back the data center data later\n",
    "    nei_with_DataCenter['was_mapped'] = True  # add flag\n",
    "\n",
    "\n",
    "    # Compute and apply split factors per pollutant\n",
    "    for nei_col, DataCenter_col in pollutant_map.items():\n",
    "\n",
    "        print (nei_col, DataCenter_col)\n",
    "        # Group sum for each pollutant by EIS_ID\n",
    "        total_by_eis = nei_with_DataCenter.groupby('EIS_ID')[f'{nei_col}_nei'].transform('sum')\n",
    "        nei_with_DataCenter[f'{nei_col}_total_by_eis'] = total_by_eis\n",
    "\n",
    "        # Default: compute split factor using NEI emissions\n",
    "        split_col = f'{nei_col}_split'\n",
    "        nei_with_DataCenter[split_col] = nei_with_DataCenter[f'{nei_col}_nei'] / total_by_eis.replace(0, pd.NA)\n",
    "\n",
    "        # Find EIS_IDs where total_by_eis is zero but DataCenter_col is non-zero\n",
    "        mask_zero_total = (total_by_eis == 0) & nei_with_DataCenter[DataCenter_col].notna() & (nei_with_DataCenter[DataCenter_col] != 0)\n",
    "\n",
    "        print(f\"{nei_col}: # fallback allocations due to zero NEI = {mask_zero_total.sum()}\")\n",
    "\n",
    "        # For these EIS_IDs, assign equal split factor across matching rows\n",
    "        for eid in nei_with_DataCenter.loc[mask_zero_total, 'EIS_ID'].unique():\n",
    "            match_rows = nei_with_DataCenter['EIS_ID'] == eid\n",
    "            n_rows = match_rows.sum()\n",
    "            nei_with_DataCenter.loc[match_rows, split_col] = 1.0 / n_rows\n",
    "\n",
    "        # Now compute eGRID-scaled emissions and save as nei original name\n",
    "        nei_with_DataCenter[f'{nei_col}'] = nei_with_DataCenter[split_col] * nei_with_DataCenter[DataCenter_col]\n",
    "\n",
    "    # OPTIONAL: Drop intermediate split columns\n",
    "    #nei_with_DataCenter.drop(columns=[f'{k}_split' for k in pollutant_map], inplace=True)\n",
    "    # Merge results back into the full NEI dataset\n",
    "\n",
    "    # Merge results back into the full NEI dataset\n",
    "    nei_all_pt_final = nei_all_pt.merge(\n",
    "        nei_with_DataCenter[ \n",
    "            unique_identifier_columns + [\"was_mapped\",'cambium_gea'] + [f'{k}' for k in pollutant_map]\n",
    "        ],\n",
    "        on=unique_identifier_columns,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    gdf_subset = nei_all_pt_final[nei_all_pt_final['was_mapped'] == True]\n",
    "    print(\"base dataframe size \", gdf_subset.shape, nei_with_DataCenter.shape)\n",
    "    print(\"before filling; subset nei sum \", gdf_subset[['PM2_5_nei', 'NH3_nei', 'VOC_nei', 'NOx_nei', 'SOx_nei']].sum())\n",
    "    print(\"before filling; subset base sum \", gdf_subset[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum())\n",
    "    print(\"before filling; nei_all_pt_final base sum \", nei_all_pt_final[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum()) \n",
    "\n",
    "    # fill the empty rows with NEI dataset\n",
    "    for k in pollutant_map:\n",
    "        nei_all_pt_final[f'{k}'] = nei_all_pt_final[f'{k}'].fillna(nei_all_pt_final[f'{k}_nei'])\n",
    "        nei_all_pt_final[f'{k}_diff'] = nei_all_pt_final[f'{k}'] - nei_all_pt_final[f'{k}_nei']\n",
    "\n",
    "    # Define difference columns\n",
    "    diff_cols = ['VOC_diff', 'NH3_diff', 'NOx_diff', 'SOx_diff', 'PM2_5_diff']\n",
    "\n",
    "    # Mask for rows that were mapped\n",
    "    mapped_mask = nei_all_pt_final['was_mapped'] == True\n",
    "\n",
    "    # Mask for rows with no difference in any pollutant\n",
    "    no_change_mask = (nei_all_pt_final[diff_cols] == 0).all(axis=1)\n",
    "\n",
    "    # Combine masks\n",
    "    mapped_but_unchanged = nei_all_pt_final[mapped_mask & no_change_mask]\n",
    "\n",
    "    # Show result\n",
    "    print(\"Number of rows where emissions were mapped but did not change:\", mapped_but_unchanged.shape[0])\n",
    "\n",
    "    # Remove rows where all values in specified columns are zero\n",
    "    gdf_subset = nei_all_pt_final[nei_all_pt_final['was_mapped'] == True]\n",
    "    print(\"subset dataframe size \", gdf_subset.shape, nei_with_DataCenter.shape)\n",
    "    print(\"subset nei sum \", gdf_subset[['PM2_5_nei', 'NH3_nei', 'VOC_nei', 'NOx_nei', 'SOx_nei']].sum())\n",
    "    print(\"subset base sum \", gdf_subset[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum())\n",
    "\n",
    "    # drop the unnecessary columns\n",
    "    nei_all_pt_final.drop(columns=[f'{k}_diff' for k in pollutant_map], inplace=True)\n",
    "    #nei_all_pt_final.drop(columns=[f'{k}_nei' for k in pollutant_map], inplace=True)\n",
    "\n",
    "    return nei_all_pt_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#DataCenter_dir_path = '/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/'\n",
    "#DataCenter_dir_path = '/Users/yunhalee/Documents/LOCAETA/Electrification/UpdatedSCC_noelecwaste/'\n",
    "DataCenter_list = [\"current_easyhard\"] \n",
    "\n",
    "for DataCenter_name in DataCenter_list:\n",
    "\n",
    "    print (\"processing \", DataCenter_name)\n",
    "    #DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "    DataCenter_file = os.path.join(DataCenter_dir_path, f'UpdatedSCC_nowaste_pp_{DataCenter_name}.csv')\n",
    "\n",
    "    egrid = pd.read_csv(DataCenter_file)\n",
    "\n",
    "    print(\"original data\", egrid[['PM2.5_tons_base', 'NH3_tons_base', 'VOC_tons_base', 'NOx_tons_base', 'SO2_tons_base']].sum())\n",
    "    egrid = reformat_DataCenter(egrid)\n",
    "    print(\"after grouping\", egrid[['PM2.5_tons_base', 'NH3_tons_base', 'VOC_tons_base', 'NOx_tons_base', 'SO2_tons_base']].sum())\n",
    "\n",
    "\n",
    "    # Filter NEI rows to only those that exist in eGRID\n",
    "    nei_with_DataCenter = nei_all_pt[nei_all_pt['EIS_ID'].isin(egrid['EIS_ID'])].copy()\n",
    "\n",
    "    # Subset only for necessary columns\n",
    "    nei_with_DataCenter.drop(columns=['height', 'diam',\n",
    "        'temp', 'velocity'], inplace=True)\n",
    "\n",
    "    unique_identifier_columns = find_minimal_unique_identifier_columns(nei_with_DataCenter)\n",
    "\n",
    "    if unique_identifier_columns:\n",
    "        print(\"Columns that uniquely identify rows:\", unique_identifier_columns)\n",
    "    else:\n",
    "        print(\"No combination of columns uniquely identifies rows.\")\n",
    "\n",
    "    print(\"filtering\", nei_with_DataCenter.shape)\n",
    "\n",
    "    # Merge eGRID emissions\n",
    "    nei_with_DataCenter = nei_with_DataCenter.merge(egrid, on='EIS_ID', how='left')\n",
    "\n",
    "    print(\"Merging egrid\", nei_with_DataCenter.shape)\n",
    "\n",
    "\n",
    "    # for is_base_emission in [True, False]: \n",
    "    for is_base_emission in [False]: \n",
    "        nei_all_pt_final = mapping_DataCenter_to_nei(nei_with_DataCenter, nei_all_pt, unique_identifier_columns, is_base = is_base_emission)\n",
    "\n",
    "        # Split into two GeoDataFrames based on was_mapped\n",
    "        mapped_df = nei_all_pt_final[nei_all_pt_final['was_mapped'] == True].copy()\n",
    "        unmapped_df = nei_all_pt_final[nei_all_pt_final['was_mapped'] != True].copy()\n",
    "\n",
    "        print(\"final size \", mapped_df.shape, unmapped_df.shape)\n",
    "\n",
    "        # Save outputs with region suffix\n",
    "        if is_base_emission:\n",
    "\n",
    "            # Save the mapped data with runname in the filename\n",
    "            if not mapped_df.empty:\n",
    "                mapped_filename = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}_base.shp\")\n",
    "                mapped_df.to_file(mapped_filename, driver='ESRI Shapefile')\n",
    "                print(f\"Saved mapped data to {mapped_filename}\")\n",
    "\n",
    "            # Save the rest (unmapped data) as rest_NEI\n",
    "            if not unmapped_df.empty:\n",
    "                rest_filename = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}_base_rest_NEI.shp\")\n",
    "                unmapped_df.to_file(rest_filename, driver='ESRI Shapefile')\n",
    "                print(f\"Saved unmapped NEI data to {rest_filename}\")\n",
    "        else:\n",
    "            # Save the mapped data with runname in the filename\n",
    "            if not mapped_df.empty:\n",
    "                mapped_filename = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}.shp\")\n",
    "                mapped_df.to_file(mapped_filename, driver='ESRI Shapefile')\n",
    "                print(f\"Saved mapped data to {mapped_filename}\")\n",
    "\n",
    "            # Save the rest (unmapped data) as rest_NEI\n",
    "            if not unmapped_df.empty:\n",
    "                rest_filename = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}_rest_NEI.shp\")\n",
    "                unmapped_df.to_file(rest_filename, driver='ESRI Shapefile')\n",
    "                print(f\"Saved unmapped NEI data to {rest_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Sub region case for Data Center emissions\n",
    "\n",
    "For sub-region case, it needs to use the all \"base\" emissions except for the sub-region, which needs to use \"final\" emissions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set region to a specific cambium_gea value, or set to None to skip subsetting\n",
    "subset_region_list = [\"NorthernGrid_West\", \"MISO_South\",\"SPP_North\", \"PJM_East\", \"MISO_Central\", \"CAISO\" ] #  ,  # Example: \"WECC\", \"ERCOT\", etc. Set to None to process all regions\n",
    "\n",
    "DataCenter_dir_path = '/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/'\n",
    "DataCenter_list = [\"current_2020\"] \n",
    "\n",
    "pollutant_cols = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "\n",
    "emission_summary = []\n",
    "\n",
    "for DataCenter_name in DataCenter_list:\n",
    "\n",
    "    print (\"processing \", DataCenter_name)\n",
    "    DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "    egrid = pd.read_csv(DataCenter_file) \n",
    "\n",
    "    # Read the point source emissions\n",
    "    base_emis =f'/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/{DataCenter_name}_base.shp'\n",
    "    gdf_base = gpd.read_file(base_emis)\n",
    "    gdf_base.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Read the point source emissions\n",
    "    final_emis =f'/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/{DataCenter_name}.shp'\n",
    "    gdf_final = gpd.read_file(final_emis)\n",
    "    gdf_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for subset_region in subset_region_list:\n",
    "\n",
    "        print(\"subset is happening for \", subset_region)\n",
    "\n",
    "        base_subset = gdf_base[gdf_base['cambium_ge'] != subset_region]\n",
    "        final_subset = gdf_final[gdf_final['cambium_ge'] == subset_region]\n",
    "\n",
    "        # Compare two sums of emissions\n",
    "        base_region = gdf_base[gdf_base['cambium_ge'] == subset_region]\n",
    "        summary_entry = {\n",
    "            'Region': subset_region,\n",
    "        }\n",
    "\n",
    "        for pol in pollutant_cols:\n",
    "            summary_entry[f'{pol}_tons_base'] = base_region[pol].sum()\n",
    "            summary_entry[f'{pol}_tons_final'] = final_subset[pol].sum()\n",
    "\n",
    "        emission_summary.append(summary_entry)\n",
    "                  \n",
    "        combined_gdf = pd.concat([base_subset, final_subset], ignore_index=True)\n",
    "\n",
    "        print(\"# of rows must be same: \", gdf_base.shape, gdf_final.shape, combined_gdf.shape)\n",
    "\n",
    "        if combined_gdf.shape[0] == base_subset.shape[0] + final_subset.shape[0]: \n",
    "            print (f\"GOOD : # of row by {subset_region} is {final_subset.shape[0]}\")\n",
    "        else:\n",
    "            print (f\"BAD : {subset_region} doesn't result in same total rows {base_subset.shape[0]} {final_subset.shape[0]}  {combined_gdf.shape[0]}\")\n",
    "\n",
    "        filename = os.path.join(DataCenter_dir_path, f\"{DataCenter_name}_{subset_region}.shp\")\n",
    "        combined_gdf.to_file(filename, driver='ESRI Shapefile')\n",
    "        print(f\"Saved {subset_region} emissions to {filename}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "summary_df = pd.DataFrame(emission_summary)\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = f'/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/{DataCenter_name}_emission_summary_by_region.csv'\n",
    "summary_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Emission summary saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "egrid_reform = reformat_DataCenter(egrid)\n",
    "egrid_reform.drop(columns=['EIS_ID'], inplace=True)\n",
    "egrid_grouped = egrid_reform.groupby(by ='cambium_gea').sum().reset_index()\n",
    "egrid_grouped.to_csv(f'/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/{DataCenter_name}_emission_summary_by_region_from_csv.csv')\n",
    "\n",
    "egrid_subset = egrid_grouped[ (egrid_grouped['cambium_gea'].isin(summary_df['Region']))]\n",
    "\n",
    "egrid_subset.sort_values(by = 'cambium_gea', inplace=True)\n",
    "summary_df.sort_values(by = 'Region', inplace=True)\n",
    "egrid_subset.sort_index(axis=1, inplace=True)\n",
    "summary_df.sort_index(axis=1, inplace=True)\n",
    "\n",
    "summary_df.rename(columns={'SOx_tons_base': 'SO2_tons_base', 'SOx_tons_final': 'SO2_tons_final', 'PM2.5_tons_base': 'PM2_5_tons_base', 'PM2.5_tons_final': 'PM2_5_tons_final', 'Region': 'cambium_gea'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "egrid_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure pollutant order is consistent\n",
    "summary_df['Pollutant'] = pd.Categorical(summary_df['Pollutant'], categories=pollutant_cols, ordered=True)\n",
    "\n",
    "# Melt for easier plotting\n",
    "df_melted = summary_df.melt(\n",
    "    id_vars=['DataCenter', 'Region', 'Pollutant'],\n",
    "    value_vars=['Base_Sum', 'Final_Sum'],\n",
    "    var_name='Scenario',\n",
    "    value_name='Emissions'\n",
    ")\n",
    "\n",
    "# Plot: Facet by Region\n",
    "g = sns.catplot(\n",
    "    data=df_melted,\n",
    "    x='Pollutant', y='Emissions', hue='Scenario',\n",
    "    col='Region', col_wrap=3,\n",
    "    kind='bar', height=4, aspect=1.2,\n",
    "    sharey=False\n",
    ")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Pollutant\", \"Emissions (tons)\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Check how much data center emissions are from ptnonipm sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define pollutant columns\n",
    "pollutants = ['VOC', 'NOx', 'SOx', 'PM2_5', 'NH3']\n",
    "#pollutants = [item +\"_nei\" for item in pollutants]\n",
    "print(pollutants)\n",
    "\n",
    "# Step 1: Find EIS_ID & SCC pairs that include \"ptnonipm\"\n",
    "ptnonipm_keys = (\n",
    "    nei_with_DataCenter[nei_with_DataCenter['source_fil'].str.contains('ptnonipm', case=False, na=False)]\n",
    "    [['EIS_ID', 'SCC']]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# Step 2: Filter original dataframe to include only those EIS_ID & SCCs\n",
    "mask = nei_with_DataCenter.set_index(['EIS_ID', 'SCC']).index.isin(ptnonipm_keys.set_index(['EIS_ID', 'SCC']).index)\n",
    "filtered_df = nei_with_DataCenter[mask].copy()\n",
    "\n",
    "# Step 3: Group by sector and sum pollutants\n",
    "emissions_by_sector = filtered_df.groupby('source_fil')[pollutants].sum()\n",
    "\n",
    "# Step 4: Plot as bar chart (one bar per pollutant, grouped by sector)\n",
    "emissions_by_sector.plot(kind='bar', figsize=(12, 6))\n",
    "plt.ylabel('Total Emissions (tons/year)')\n",
    "plt.title('Total Emissions by Sector (where any EIS_ID+SCC includes \"ptnonipm\")')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Useful debugging script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique IDs\n",
    "ID_name = 'EIS_ID'\n",
    "org_DataCenter_list = egrid[ID_name].unique()\n",
    "DataCenter_list = nei_with_DataCenter[ID_name].unique()\n",
    "org_nei_list = nei_all_pt[ID_name].unique()\n",
    "nei_list = nei_all_pt_final[ID_name].unique()\n",
    "\n",
    "print(len(org_DataCenter_list), len(DataCenter_list), len(nei_list),len(org_nei_list))\n",
    "\n",
    "\n",
    "# find the rows where two columns are different\n",
    "missing_pm25_DataCenter = nei_with_DataCenter[\n",
    "    nei_with_DataCenter['PM2_5_DataCenter'].isna() & \n",
    "    nei_with_DataCenter['PM2.5_tons_final'].notna()\n",
    "]\n",
    "\n",
    "# filtering column based on string\n",
    "missing_pm25_DataCenter.filter(regex ='tons_final|total_by_eis')\n",
    "\n",
    "\n",
    "# get a certain ID facility\n",
    "print(nei_all_pt[(nei_all_pt['EIS_ID'] == 1028611)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Compare NEI2020 against Current grid base emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_map = {\n",
    "    'NOx': 'NOx_tons_base',\n",
    "    'PM2_5': 'PM2.5_tons_base',\n",
    "    'VOC': 'VOC_tons_base',\n",
    "    'NH3': 'NH3_tons_base',\n",
    "    'SOx': 'SO2_tons_base'\n",
    "\n",
    "}\n",
    "\n",
    "# Compute and apply split factors per pollutant\n",
    "for nei_col, DataCenter_col in pollutant_map.items():\n",
    "    # Scatter plot of total_by_eis vs. eGRID\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(\n",
    "        nei_with_DataCenter[f'{nei_col}_total_by_eis'],\n",
    "        nei_with_DataCenter[DataCenter_col] - nei_with_DataCenter[f'{nei_col}_total_by_eis'],\n",
    "        alpha=0.5\n",
    "    )\n",
    "    plt.xlabel(f\"Total {nei_col} by EIS_ID in NEI\")\n",
    "    plt.ylabel(f\"{DataCenter_col} - '{nei_col}_total_by_eis'\")\n",
    "    plt.title(f\"{nei_col} NEI Total vs difference with eGRID base Emissions\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Evaluate new egrid emissions formatted for NEI-SMOKE style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "\n",
    "DataCenter_dir_path = '/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/'\n",
    "egrids_list = [\"current_2020\"]\n",
    "\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_final_map = {\n",
    "    'NOx': 'NOx_tons_final',\n",
    "    'PM2_5': 'PM2.5_tons_final',\n",
    "    'VOC': 'VOC_tons_final',\n",
    "    'NH3': 'NH3_tons_final',\n",
    "    'SOx': 'SO2_tons_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_base_map = {\n",
    "    'NOx': 'NOx_tons_base',\n",
    "    'PM2_5': 'PM2.5_tons_base',\n",
    "    'VOC': 'VOC_tons_base',\n",
    "    'NH3': 'NH3_tons_base',\n",
    "    'SOx': 'SO2_tons_base'\n",
    "}\n",
    "\n",
    "is_base_emission = True\n",
    "\n",
    "# nei emissions column names\n",
    "pollutant_cols = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "\n",
    "for DataCenter_name in egrids_list:\n",
    "\n",
    "    if is_base_emission: \n",
    "        pollutant_map = pollutant_base_map\n",
    "        file_path = os.path.join(DataCenter_dir_path, f'{DataCenter_name}_base.shp')  \n",
    "    else:\n",
    "        pollutant_map = pollutant_final_map\n",
    "        file_path = os.path.join(DataCenter_dir_path, f'{DataCenter_name}.shp')  \n",
    "\n",
    "    # read emission scenario\n",
    "    final_DataCenter_emis = gpd.read_file(file_path) \n",
    "\n",
    "    # Reset index to ensure proper comparison\n",
    "    final_DataCenter_emis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Subset rows where actual egrid emissions are available (egrid ≠ nei for any pollutant)\n",
    "    mask = pd.concat([\n",
    "        final_DataCenter_emis[k] != final_DataCenter_emis[f'{k}_nei']\n",
    "        for k in pollutant_cols\n",
    "    ], axis=1).any(axis=1)\n",
    "\n",
    "    final_DataCenter_emis = final_DataCenter_emis[mask]\n",
    "\n",
    "    # Compute group sums for each pollutant by EIS_ID\n",
    "    group_sums = final_DataCenter_emis.groupby('EIS_ID')[[k for k in pollutant_cols]].sum().reset_index()\n",
    "\n",
    "    group_sums.head()\n",
    "\n",
    "    print (\"processing \", DataCenter_name)\n",
    "    original_DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "    original_DataCenter = pd.read_csv(original_DataCenter_file) \n",
    "    original_DataCenter = reformat_DataCenter(original_DataCenter)\n",
    "\n",
    "\n",
    "    # Merge for comparison\n",
    "    comparison_df = group_sums.merge(original_DataCenter, on='EIS_ID')\n",
    "    comparison_df.head()\n",
    "\n",
    "    # Scatter plots\n",
    "    for nei, egrid in pollutant_map.items():\n",
    "        x = comparison_df[nei]\n",
    "        y = comparison_df[egrid] - comparison_df[nei]\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(x, y, alpha=0.6, edgecolors='k')\n",
    "        plt.xlabel(f'{nei} NEI')\n",
    "        plt.ylabel(f'{nei} base - nei')\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.title(f'{nei} Comparison - {DataCenter_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Plot the total emissions (either final or diff_final) by Species for all egrid scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Dictionary to store total emissions\n",
    "DataCenter_sum = {}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_diff_map = {\n",
    "    'NOx': 'NOx_tons_dif_final',\n",
    "    'PM2_5': 'PM2.5_tons_dif_final',\n",
    "    'VOC': 'VOC_tons_dif_final',\n",
    "    'NH3': 'NH3_tons_dif_final',\n",
    "    'SOx': 'SO2_tons_dif_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_final_map = {\n",
    "    'NOx': 'NOx_tons_final',\n",
    "    'PM2_5': 'PM2.5_tons_final',\n",
    "    'VOC': 'VOC_tons_final',\n",
    "    'NH3': 'NH3_tons_final',\n",
    "    'SOx': 'SO2_tons_final'\n",
    "}\n",
    "\n",
    "pollutant_map = pollutant_diff_map\n",
    "\n",
    "egrids_list = [\"current_2020\"]\n",
    "\n",
    "for DataCenter_name in egrids_list: \n",
    "    print (\"processing \", DataCenter_name)\n",
    "    original_DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "    original_DataCenter = pd.read_csv(original_DataCenter_file) \n",
    "\n",
    "    # Store totals per pollutant\n",
    "    for nei, egrid in pollutant_map.items():\n",
    "        DataCenter_sum.setdefault(nei, {})[DataCenter_name] = original_DataCenter[egrid].sum()\n",
    "\n",
    "    print(DataCenter_sum)\n",
    "\n",
    "output_dir = f'/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/{egrids_list[0]}/'    \n",
    "os.makedirs(output_dir, exist_ok =True)\n",
    "\n",
    "# Convert to DataFrame: rows = pollutant, columns = egrid cases\n",
    "emissions_df = pd.DataFrame(DataCenter_sum).T  # Transpose so pollutants are rows\n",
    "\n",
    "# Plotting\n",
    "ax = emissions_df.plot(kind='bar', figsize=(10, 6))\n",
    "ax.set_ylabel(\"Total Emissions\")\n",
    "ax.set_title(\"Total Difference Emissions by Pollutant and eGRID Case\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Data Center Scenario\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, f'Total_Difference.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Emissions changes from current_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_diff_map = {\n",
    "    'NOx': 'NOx_tons_dif_final',\n",
    "    'PM2_5': 'PM2.5_tons_dif_final',\n",
    "    'VOC': 'VOC_tons_dif_final',\n",
    "    'NH3': 'NH3_tons_dif_final',\n",
    "    'SOx': 'SO2_tons_dif_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_final_map = {\n",
    "    'NOx': 'NOx_tons_final',\n",
    "    'PM2_5': 'PM2.5_tons_final',\n",
    "    'VOC': 'VOC_tons_final',\n",
    "    'NH3': 'NH3_tons_final',\n",
    "    'SOx': 'SO2_tons_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_base_map = {\n",
    "    'NOx': 'NOx_tons_base',\n",
    "    'PM2_5': 'PM2.5_tons_base',\n",
    "    'VOC': 'VOC_tons_base',\n",
    "    'NH3': 'NH3_tons_base',\n",
    "    'SOx': 'SO2_tons_base'\n",
    "}\n",
    "\n",
    "egrids_list = [\"current_2020\"] # ,\"decarb95_2050\",\"highREcost_2050\"]\n",
    "\n",
    "def convert_DataCenter_csv_to_df(egrids_list, pollutant_map):\n",
    "    # Dictionary to store total emissions\n",
    "    DataCenter_sum = {}\n",
    "\n",
    "\n",
    "    for DataCenter_name in egrids_list: \n",
    "        print (\"processing \", DataCenter_name)\n",
    "        original_DataCenter_file = os.path.join(DataCenter_dir_path, f'300MW_national_{DataCenter_name}.csv')\n",
    "        original_DataCenter = pd.read_csv(original_DataCenter_file) \n",
    "\n",
    "        # Store totals per pollutant\n",
    "        for nei, egrid in pollutant_map.items():\n",
    "            DataCenter_sum.setdefault(nei, {})[DataCenter_name] = original_DataCenter[egrid].sum()\n",
    "\n",
    "        print(DataCenter_sum)\n",
    "\n",
    "    output_dir = f'/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/{egrids_list[0]}/'    \n",
    "    os.makedirs(output_dir, exist_ok =True)\n",
    "\n",
    "    # Convert to DataFrame: rows = pollutant, columns = egrid cases\n",
    "    emissions_df = pd.DataFrame(DataCenter_sum).T  # Transpose so pollutants are rows\n",
    "    return emissions_df\n",
    "\n",
    "emissions_diff = convert_DataCenter_csv_to_df(egrids_list, pollutant_diff_map)\n",
    "\n",
    "# Plotting\n",
    "ax = emissions_diff.plot(kind='bar', figsize=(10, 6))\n",
    "ax.set_ylabel(\"Total Emissions\")\n",
    "ax.set_title(\"Total Difference Emissions by Pollutant and eGRID Case\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"eGRID Case\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, f'Total_Difference.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emissions_base = convert_DataCenter_csv_to_df(egrids_list, pollutant_base_map)\n",
    "\n",
    "emissions_base['decarb95_2050 - current_2020'] = emissions_base['decarb95_2050'] - emissions_base['current_2020']\n",
    "emissions_base['highREcost_2050 - current_2020'] = emissions_base['highREcost_2050'] - emissions_base['current_2020']\n",
    "\n",
    "emissions_base.drop(columns=egrids_list, inplace=True)\n",
    "emissions_base\n",
    "\n",
    "output_dir = f'/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/{egrids_list[1]}_base/' \n",
    "os.makedirs(output_dir, exist_ok =True)\n",
    "\n",
    "# Plotting\n",
    "ax = emissions_base.plot(kind='bar', figsize=(10, 6))\n",
    "ax.set_ylabel(\"Total Emissions\")\n",
    "ax.set_title(\"Total Difference Emissions: future eGRID scenario - current_2020\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"eGRID scenarios\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, f'Total_Difference_based_on_current_2020.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Plot the current_2020_base emissions based on GEA regions\n",
    "\n",
    "Note that the current_2020_base has different emissions than NEI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "run_name = 'current_2020_base'\n",
    "DataCenter_emis_file = f'/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/{run_name}.shp'\n",
    "final_output_dir = f'/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/{run_name}/'\n",
    "\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "# read emission scenario\n",
    "gdf_emis = gpd.read_file(DataCenter_emis_file) \n",
    "# Reset index to ensure proper comparison\n",
    "gdf_emis.reset_index(drop=True, inplace=True)\n",
    "gdf_emis.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "# emissions column names\n",
    "cs_pollutants = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "nei_pollutants = [f'{poll}_nei' for poll in cs_pollutants]\n",
    "\n",
    "# Loop through each pollutant\n",
    "for cs_col, nei_col in zip(cs_pollutants, nei_pollutants):\n",
    "    # grouping and summing emissions by each cambium_gea regions\n",
    "    grouped_sum = gdf_emis.groupby('cambium_ge')[[cs_col, nei_col]].sum().reset_index()\n",
    "\n",
    "    print(grouped_sum)\n",
    "\n",
    "    regions = grouped_sum['cambium_ge'].tolist()\n",
    "    x = np.arange(len(regions))\n",
    "    width = 0.35 # bar width\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (10, 6))\n",
    "    bars1 = ax.bar(x-width/2, grouped_sum[cs_col], width, label =f'{cs_col}_ccs')\n",
    "    bars2 = ax.bar(x+width/2, grouped_sum[nei_col], width, label = f'{nei_col}')\n",
    "\n",
    "    # Add value labels above bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:,.0f}',\n",
    "                        xy = (bar.get_x() + bar.get_width()/2, height/2),\n",
    "                        xytext = (0, 3),\n",
    "                        textcoords = 'offset points',\n",
    "                        ha = 'center',\n",
    "                        va = 'bottom', fontsize=9, rotation = 90,\n",
    "                        )\n",
    "    \n",
    "    ax.set_xlabel('Cambium_gea region')\n",
    "    ax.set_ylabel('Total Emissions [tons/yr]')\n",
    "    ax.set_title(f'Total {cs_col} Emissions by Cambium_gea Region')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(grouped_sum['cambium_ge'], rotation =45, ha='right')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(final_output_dir + f'Total_Difference_{cs_col}_by_regions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "def plot_diff_DC_emis(gdf_emis, output_dir, emis_region):\n",
    "        \n",
    "    # Container to collect filtered grouped_sum results\n",
    "    grouped_combined = pd.DataFrame()\n",
    "\n",
    "    # emissions column names\n",
    "    pollutants = ['PM2_5', 'NOx','SOx', 'VOC','NH3']\n",
    "    base_pollutants = [f'{poll}_base' for poll in pollutants]\n",
    "    sens_pollutants = [f'{poll}_sens' for poll in pollutants]\n",
    "\n",
    "    multi_region_indices = []\n",
    "\n",
    "    # Loop through each pollutant\n",
    "    for i, (base_col, sens_col) in enumerate(zip(base_pollutants, sens_pollutants)):\n",
    "        # grouping and summing emissions by each cambium_gea regions\n",
    "        grouped_sum = gdf_emis.groupby('cambium_ge_sens')[[base_col, sens_col]].sum().reset_index()\n",
    "        print(grouped_sum)\n",
    "\n",
    "        if len(grouped_sum) > 1:\n",
    "            multi_region_indices.append(i)\n",
    "\n",
    "        else:\n",
    "            # First time: initialize with region column\n",
    "            if grouped_combined.empty:\n",
    "                grouped_combined['cambium_ge_sens'] = grouped_sum['cambium_ge_sens']\n",
    "            \n",
    "            grouped_combined[base_col] = grouped_sum[base_col].values\n",
    "            grouped_combined[sens_col] = grouped_sum[sens_col].values\n",
    "\n",
    "\n",
    "    if multi_region_indices:\n",
    "        n = len(multi_region_indices)\n",
    "        fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(10, 6*n))\n",
    "\n",
    "        for idx, ax in zip(multi_region_indices, axes):\n",
    "            base_col = base_pollutants[idx]\n",
    "            sens_col = sens_pollutants[idx]\n",
    "            grouped_sum = gdf_emis.groupby('cambium_ge_sens')[[base_col, sens_col]].sum().reset_index()\n",
    "            grouped_sum.sort_values(by = base_col, ascending=False, inplace=True)\n",
    "            x = np.arange(len(grouped_sum))\n",
    "            width = 0.35\n",
    "\n",
    "            bars1 = ax.bar(x-width/2, grouped_sum[base_col], width, label =f'{base_col}')\n",
    "            bars2 = ax.bar(x+width/2, grouped_sum[sens_col], width, label = f'{sens_col}')\n",
    "\n",
    "            # Add value labels above bars\n",
    "            for bars in [bars1, bars2]:\n",
    "                for bar in bars:\n",
    "                    height = bar.get_height()\n",
    "                    ax.annotate(f'{height:,.0f}',\n",
    "                                xy = (bar.get_x() + bar.get_width()/2, height/2),\n",
    "                                xytext = (0, 3),\n",
    "                                textcoords = 'offset points',\n",
    "                                ha = 'center',\n",
    "                                va = 'bottom', fontsize=9, rotation = 90,\n",
    "                                )\n",
    "            \n",
    "            ax.set_xlabel('Cambium_gea region')\n",
    "            ax.set_ylabel('Total Emissions [tons/yr]')\n",
    "            substring_to_remove = \"_base\"\n",
    "            col = base_col.replace(substring_to_remove, \"\")\n",
    "            ax.set_title(f'Total {col} Emissions by Cambium Regions')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(grouped_sum['cambium_ge_sens'], rotation =45, ha='right')\n",
    "            ax.legend()\n",
    "\n",
    "    if not grouped_combined.empty:\n",
    "        \n",
    "        base_vals = [grouped_combined[f'{p}_base'].iloc[0] for p in pollutants]\n",
    "        sens_vals = [grouped_combined[f'{p}_sens'].iloc[0] for p in pollutants]\n",
    "\n",
    "        x = np.arange(len(pollutants))\n",
    "        width = 0.35 # bar width\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        bars1 = ax.bar(x-width/2,  base_vals, width, label ='Base')\n",
    "        bars2 = ax.bar(x+width/2, sens_vals, width, label = 'Sens')\n",
    "\n",
    "        # Add value labels above bars\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.annotate(f'{height:,.0f}',\n",
    "                            xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                            xytext=(0, 3),\n",
    "                            textcoords='offset points',\n",
    "                            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        ax.set_xlabel('Pollutant')\n",
    "        ax.set_ylabel('Total Emissions [tons/yr]')\n",
    "        ax.set_title(f'Total Emissions by Pollutant (Base vs Sens) at {emis_region}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(pollutants, rotation =45, ha='right')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'Total_Difference.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Plot the base vs each_region emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCenter_emis_dir = '/Users/yunhalee/Documents/LOCAETA/DataCenter_emissions/' \n",
    "emis_output_dir = '/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/'\n",
    "base_emis = 'current_2020_base.shp'\n",
    "emis_regions = ['All','CAISO','PJM_East','MISO_Central','MISO_South','NorthernGrid_West','SPP_North'] # \n",
    "\n",
    "\n",
    "# read emission scenario\n",
    "gdf_base_emis = gpd.read_file(DataCenter_emis_dir + base_emis) \n",
    "gdf_base_emis.reset_index(drop=True, inplace=True)\n",
    "print(gdf_base_emis.head())\n",
    "\n",
    "\n",
    "for emis_region in emis_regions: \n",
    "    if emis_region == 'All':\n",
    "        gdf_sens_emis = gpd.read_file(f'{DataCenter_emis_dir}current_2020.shp') \n",
    "        final_output_dir = f'{emis_output_dir}current_2020'\n",
    "    else:\n",
    "        gdf_sens_emis = gpd.read_file(f'{DataCenter_emis_dir}current_2020_{emis_region}.shp') \n",
    "        final_output_dir = f'{emis_output_dir}current_2020_{emis_region}'\n",
    "    gdf_sens_emis.reset_index(drop=True, inplace=True)\n",
    "    print(gdf_sens_emis.head())\n",
    "\n",
    "    # Make the output dir if not exits\n",
    "    os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "    # Columns to merge on\n",
    "    key_cols = ['EIS_ID', 'SCC', 'rel_point_', 'source_fil', 'was_mapped']\n",
    "\n",
    "    # Columns to compare (everything except keys + geometry)\n",
    "    compare_cols = gdf_sens_emis.columns.difference(key_cols + ['geometry'])\n",
    "\n",
    "    # Merge on the key columns\n",
    "    merged = gdf_base_emis[key_cols + list(compare_cols)].merge(\n",
    "        gdf_sens_emis[key_cols + list(compare_cols)],\n",
    "        on=key_cols,\n",
    "        suffixes=('_base', '_sens')\n",
    "    )\n",
    "\n",
    "    # Create mask where any compare column differs\n",
    "    diff_mask = (merged[[f\"{col}_base\" for col in compare_cols]].values !=\n",
    "                merged[[f\"{col}_sens\" for col in compare_cols]].values).any(axis=1)\n",
    "\n",
    "    # Get only differing rows\n",
    "    diff_combined = merged[diff_mask]\n",
    "\n",
    "    # Optional: bring back geometry from base emissions\n",
    "    diff_combined = diff_combined.merge(\n",
    "        gdf_base_emis[key_cols + ['geometry']],\n",
    "        on=key_cols,\n",
    "        how='left'\n",
    "    )\n",
    "    # Make it a GeoDataFrame again\n",
    "    diff_combined = gpd.GeoDataFrame(diff_combined, geometry='geometry')\n",
    "\n",
    "    plot_diff_DC_emis(diff_combined, final_output_dir, emis_region)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micromamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
