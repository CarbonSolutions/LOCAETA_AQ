{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BenMAP data analysis\n",
    "\n",
    "1. spatial distribution of actual values\n",
    "2. spatial distribution of normalized values (by race/ethinicity population by county)\n",
    "3. national sum of each endpoint by race-ethinicity\n",
    "4. LA state sum of each endpoint by race-ethinictiy \n",
    "5. LA state sum of each endpoint (normalized by the corresponding population ) by race-ethinictiy\n",
    "\n",
    "\n",
    "#### Note that only \"Emergency Room Visits  Asthma\" has ethinicity separate (for White population), so I will merge them into just \"White\". \n",
    "## Population data is from INMAP (2013 population with race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Function to subset data for a specific state or use national data\n",
    "def subset_data(final_df, state_fips=None):\n",
    "    if state_fips:\n",
    "        # Ensure state_fips is a list\n",
    "        if not isinstance(state_fips, list):\n",
    "            state_fips = [state_fips]\n",
    "        \n",
    "        # Collect all states of interest, including neighbors\n",
    "        all_fips = set(state_fips)\n",
    "\n",
    "        # Filter the DataFrame\n",
    "        return final_df[final_df['STATE_FIPS'].isin(all_fips)]\n",
    "    return final_df\n",
    "\n",
    "def plot_bar(df, x_column, y_column, title, output_dir, benmap_output, group_col=None):\n",
    "    \"\"\"\n",
    "    Create an improved bar plot with better formatting and cleaner appearance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The data to plot\n",
    "    x_column : str\n",
    "        Column name to use for the x-axis values\n",
    "    y_column : str\n",
    "        Column name to use for the y-axis values\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    output_dir : str\n",
    "        Directory to save the output plot\n",
    "    benmap_output: str\n",
    "        type of benmap output(incidence or valuation)\n",
    "    group_col : str, optional\n",
    "        Column name to use for grouping/coloring. If None, no grouping is applied.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    # Set the style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # change title\n",
    "    if benmap_output == \"incidence\":\n",
    "        plot_label = 'Health Benefits'\n",
    "        if y_column == 'Mean_per_Pop':\n",
    "            plot_label = plot_label + '\\n (per million people) '\n",
    "    elif benmap_output == 'valuation':\n",
    "        if y_column == 'Mean': \n",
    "            plot_label = 'Monetized Health Benefits (in Million $)'\n",
    "    \n",
    "    # for valuation, make the output in million\n",
    "    if benmap_output == 'valuation':\n",
    "        df[y_column] = df[y_column]/1e6\n",
    "\n",
    "    # Create the barplot with or without grouping\n",
    "    if group_col:\n",
    "        ax = sns.barplot(\n",
    "            data=df, \n",
    "            x=x_column,\n",
    "            y=y_column,\n",
    "            hue=group_col,\n",
    "            errorbar=None,  # Remove error bars\n",
    "            palette='turbo'\n",
    "        )\n",
    "        add_font_size = 0 \n",
    "    else:\n",
    "        ax = sns.barplot(\n",
    "            data=df, \n",
    "            x=x_column,\n",
    "            y=y_column,\n",
    "            errorbar=None,  # Remove error bars\n",
    "            width=0.3,\n",
    "            palette=['#3498db']  # Single color when no grouping\n",
    "        )\n",
    "        add_font_size= 10 # make the font size larger for none group_col case\n",
    "    \n",
    "    # Set title and labels with better formatting\n",
    "    ax.set_title(title, fontsize=14 + add_font_size, fontweight='bold', pad=20)\n",
    "    ax.set_ylabel(plot_label, fontsize=12 + add_font_size)\n",
    "    ax.set_xlabel(x_column if x_column != 'Endpoint' else '', fontsize=12 + add_font_size / 2)\n",
    "\n",
    "    # Use symlog scale for y-axis if values vary widely\n",
    "    ax.set_yscale('symlog')\n",
    "\n",
    "    # Customize legend only if group_col is provided\n",
    "    if group_col:\n",
    "        ax.legend(title=group_col, title_fontsize=12, fontsize=10, frameon=True)\n",
    "\n",
    "    \n",
    "    # Improve the value annotations on the bars\n",
    "    for p in ax.patches:\n",
    "        value = p.get_height()\n",
    "        \n",
    "        # Skip tiny values or exactly zero values\n",
    "        if abs(value) < 0.0001:\n",
    "            continue\n",
    "        \n",
    "        # Format numbers nicely based on their magnitude\n",
    "        if abs(value) < 0.01:\n",
    "            annotation = f'{value:.2f}'\n",
    "        elif abs(value) < 1:\n",
    "            annotation = f'{value:.1f}'\n",
    "        else:\n",
    "            annotation = f'{int(value)}'\n",
    "        \n",
    "        # Position the text better\n",
    "        text_y_pos = value + (0.05 * np.sign(value))\n",
    "        ax.annotate(\n",
    "            annotation, \n",
    "            (p.get_x() + p.get_width() / 2., text_y_pos),\n",
    "            ha='center', \n",
    "            va='bottom' if value >= 0 else 'top',\n",
    "            fontsize=9 + add_font_size,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    # Remove top and right spines for cleaner look\n",
    "    sns.despine()\n",
    "    \n",
    "    # Rotate the x-tick labels for better readability\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=10 + add_font_size)\n",
    "\n",
    "    if group_col:\n",
    "        # Add subtle vertical gridlines between categories\n",
    "        for i in range(len(ax.get_xticks())):\n",
    "            ax.axvline(x=i - 0.5, color='grey', linestyle='--', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Add a more prominent horizontal line at y=0\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=1.2)\n",
    "    \n",
    "    # Adjust layout to make everything fit\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create filename with or without group_col info\n",
    "    filename = f'{title.replace(\" \", \"_\")}'\n",
    "    if group_col:\n",
    "        filename += f'_by_{group_col}'\n",
    "    \n",
    "    # Save with higher quality\n",
    "    fig.savefig(\n",
    "        os.path.join(output_dir, f'{filename}.png'),\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    \n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "# Creating a pretty table with plotly\n",
    "def pretty_create_table(df, columns, title):\n",
    "    formatted_values = [format_values(df[col]) for col in columns]\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=columns,\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'),\n",
    "        cells=dict(values=formatted_values,\n",
    "                   fill_color='lavender',\n",
    "                   align='left'))\n",
    "    ])\n",
    "    fig.update_layout(title_text=title)\n",
    "    file_path = os.path.join(output_dir, f'{TARGET_GRID_LEVEL}_{title}.png')\n",
    "    fig.write_image(file_path, format='png', scale=2, engine='kaleido')\n",
    "    #fig.show()\n",
    "\n",
    "# Example function to format values\n",
    "def format_values(value):\n",
    "    return f'{value:.4g}' if isinstance(value, (int, float)) else value\n",
    "\n",
    "# Function to create and save a table as CSV\n",
    "def create_csv(df, columns, title, output_dir):\n",
    "    # Format the values in the DataFrame\n",
    "    formatted_df = df[columns].applymap(format_values)\n",
    "    \n",
    "    # Create the file path\n",
    "    file_path = os.path.join(output_dir, f'{TARGET_GRID_LEVEL}_{title}.csv')\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    formatted_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"CSV file saved at {file_path}\")\n",
    "\n",
    "\n",
    "# Function to plot spatial distribution for each race on the same page using the same color scale\n",
    "def plot_spatial_distribution_combined(gdf, title, y_axis, output_dir):\n",
    "    unique_races = gdf['Race'].unique()\n",
    "    n_races = len(unique_races)\n",
    "    fig, axes = plt.subplots(1, n_races, figsize=(5 * n_races, 8), sharex=True, sharey=True)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    vmin, vmax = gdf[y_axis].min(), gdf[y_axis].max()\n",
    "    max_abs = max(abs(vmin), abs(vmax)) * 0.5\n",
    "\n",
    "    if vmax > 0 :\n",
    "        vmin = 0 \n",
    "        vmax = max_abs\n",
    "    else:\n",
    "        vmin = -max_abs\n",
    "        vmax = 0\n",
    "\n",
    "    if n_races == 1:\n",
    "        axes = [axes]  # Make axes iterable if there is only one subplot\n",
    "\n",
    "    for ax, race in zip(axes, unique_races):\n",
    "        race_gdf = gdf[gdf['Race'] == race]\n",
    "        race_gdf.plot(column=y_axis, cmap='viridis', vmin=vmin, vmax=vmax, legend=False, ax=ax)\n",
    "        ax.set_title(f'{race}')\n",
    "\n",
    "        # Calculate the sum of the values in the domain\n",
    "        if y_axis == \"Mean_per_Pop\":\n",
    "            total_sum = race_gdf[\"Mean\"].sum() / race_gdf[\"Population\"].sum() * 1000000\n",
    "            # Add the sum as text within the subplot\n",
    "            ax.text(0.5, -0.1, f'Sum: {total_sum:.2f}', ha='center', va='center', transform=ax.transAxes, fontsize=12, color='black')\n",
    "        else:  \n",
    "            total_sum = race_gdf[y_axis].sum()\n",
    "            # Add the sum as text within the subplot\n",
    "            ax.text(0.5, -0.1, f'Sum: {total_sum:.2f}', ha='center', va='center', transform=ax.transAxes, fontsize=12, color='black')\n",
    "\n",
    "    # Add a single color bar\n",
    "    cbar_ax = fig.add_axes([0.95, 0.15, 0.01, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm._A = []\n",
    "    fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, f'{y_axis}_{TARGET_GRID_LEVEL}_{title}.png'), dpi=300, bbox_inches='tight')\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "# Define the preferred race order\n",
    "race_order = [\"ALL\", \"BLACK\", \"WHITE\", \"ASIAN\", \"NATAMER\", \"HISPANIC\"]\n",
    "\n",
    "def plot_spatial_distribution_benmap_with_basemap(gdf, field, output_dir, region_name):\n",
    "\n",
    "\n",
    "    # Ensure the GeoDataFrame is in the correct CRS for basemaps (Web Mercator)\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Group by Endpoint to ensure each endpoint is plotted separately\n",
    "    grouped_endpoints = gdf.groupby(\"Endpoint\")\n",
    "\n",
    "    for endpoint, gdf_endpoint in grouped_endpoints:\n",
    "        \n",
    "        # Extract available race categories within this endpoint group\n",
    "        available_races = [race for race in race_order if race in gdf_endpoint[\"Race\"].unique()]\n",
    "\n",
    "        # change colorbar label\n",
    "        if \"Mortality\" in endpoint:\n",
    "            colorbar_label = 'Annual Avoided number of deaths'\n",
    "        elif \"Asthma\" in endpoint:\n",
    "            colorbar_label = 'Annual Avoided number of people with the symptom'\n",
    "        elif \"Work\" in endpoint:\n",
    "            colorbar_label = 'Annual Avoided work day loss'\n",
    "\n",
    "        if field == 'Mean_per_Pop':\n",
    "            colorbar_label = colorbar_label + '\\n (per million) '\n",
    "    \n",
    "        for race in available_races:\n",
    "\n",
    "            # Filter data for the current race within the endpoint group\n",
    "            gdf_race = gdf_endpoint[gdf_endpoint[\"Race\"] == race]\n",
    "\n",
    "            # Ensure the field exists\n",
    "            if field not in gdf_race.columns:\n",
    "                #print(f'Field {field} does not exist in the data for endpoint {endpoint}, race {race}. Skipping...')\n",
    "                continue\n",
    "\n",
    "            # **Check if the field contains only zeros**\n",
    "            if gdf_race[field].sum() == 0:\n",
    "                #print(f'All values are zero for {endpoint}, {race}. Skipping...')\n",
    "                continue  # Skip this plot\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            # Plot the spatial distribution\n",
    "            vmin, vmax = gdf[field].min(), gdf[field].max()\n",
    "            max_abs = max(abs(vmin), abs(vmax)) * 0.5\n",
    "\n",
    "            if vmin >= 0 :\n",
    "                vmin = 0 \n",
    "                vmax = max_abs\n",
    "\n",
    "                # Define the number of discrete bins\n",
    "                num_bins = 9  # Adjust this number for more or fewer steps\n",
    "                color_map = plt.cm.Reds  # Choose a sequential colormap\n",
    "                bounds = np.linspace(vmin, vmax, num_bins + 1)  # Define color step boundaries\n",
    "                norm = mcolors.BoundaryNorm(bounds, color_map.N)  # Create a discrete colormap\n",
    "                \n",
    "            else:\n",
    "                vmin = -max_abs\n",
    "                vmax = max_abs\n",
    "                # Define the number of discrete bins\n",
    "                num_bins = 18  # Adjust this number for more or fewer steps\n",
    "                color_map = plt.cm.get_cmap(\"bwr\")  # Reverse coolwarm to get cool (blue) for negative and warm (red) for positive\n",
    "                bounds = np.linspace(vmin, vmax, num_bins + 1)  # Define color step boundaries\n",
    "                norm = mcolors.BoundaryNorm(bounds, color_map.N)  # Create a discrete colormap\n",
    "\n",
    "\n",
    "            if region_name == 'Nation':\n",
    "                set_edgecolor= 'none'\n",
    "            else: \n",
    "                set_edgecolor='black'\n",
    "            gdf_race.plot(column=field, cmap=color_map, norm = norm, vmin=vmin, vmax=vmax, \n",
    "                          legend=False, edgecolor=set_edgecolor, ax=ax, markersize=30, alpha=0.8)\n",
    "\n",
    "            # Add a basemap\n",
    "            ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, zoom=6)\n",
    "\n",
    "            ax.set_title(f'{endpoint} - {race} ({region_name})')\n",
    "\n",
    "            # Calculate summary statistics for this race\n",
    "            total_target = gdf_race[field].sum()\n",
    "            max_target = gdf_race[field].max()\n",
    "            min_target = gdf_race[field].min()\n",
    "\n",
    "            # Display summary stats on the plot\n",
    "            ax.text(0.5, -0.15, f'Total: {total_target:.3f}\\nMax: {max_target:.3f}\\nMin: {min_target:.3f}', \n",
    "                    ha='center', va='center', transform=ax.transAxes, fontsize=12, color='black')\n",
    "\n",
    "\n",
    "            # Add a color bar\n",
    "            sm = plt.cm.ScalarMappable(cmap=color_map, norm=norm)\n",
    "            sm._A = []\n",
    "            cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  \n",
    "            fig.colorbar(sm, cax=cbar_ax).set_label(f'{colorbar_label}')\n",
    "\n",
    "            # Adjust layout\n",
    "            plt.subplots_adjust(left=0.1, right=0.85, top=0.9, bottom=0.2)\n",
    "\n",
    "            # Save the figure for each endpoint-race combination\n",
    "            output_filename = f'{field}_{endpoint}_{race}_{region_name}_with_basemap.png'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            print(f'Saved: {output_path}')\n",
    "\n",
    "\n",
    "\n",
    "def modify_geojson(geojson_data, column):\n",
    "\n",
    "    geojson_dict = json.loads(geojson_data)\n",
    "    \n",
    "    # Display properties to add to each feature\n",
    "    display_properties = {\n",
    "        'color': '#808080',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.75,\n",
    "        'type': 'polygon',\n",
    "        'iconFile': 'none',\n",
    "        'layerOrder': 5,\n",
    "        'onLoad': False,\n",
    "        'legendEntry': 'none'\n",
    "    }\n",
    "\n",
    "    # Add displayProperties at the top level\n",
    "    geojson_dict['displayProperties'] = display_properties\n",
    "\n",
    "    if column == 'TotalPopD':\n",
    "        geojson_dict['QuantityDescriptor'] = \"Changes in total premature deaths\"\n",
    "    elif column == 'TotalPM25':\n",
    "        geojson_dict['QuantityDescriptor'] = \"Changes in surface PM2.5 concentrations\"\n",
    "    \n",
    "    # Rename the quantity column to \"Quantity\"\n",
    "    for feature in geojson_dict['features']:\n",
    "        feature['properties']['Quantity'] = feature['properties'].pop(column)\n",
    "    \n",
    "    modified_geojson = json.dumps(geojson_dict, indent=2)\n",
    "    modified_geojson = f\"var INMAP_{column} = {modified_geojson};\"\n",
    "    \n",
    "    return modified_geojson\n",
    "\n",
    "\n",
    "def save_benmap_json(gdf_diff, columns_to_save, webdata_path):\n",
    "\n",
    "    for column in columns_to_save:\n",
    "\n",
    "        gdf_column = gdf_diff[['geometry', column]].copy()\n",
    "        \n",
    "        if column == 'TotalPopD':\n",
    "            threshold = 0\n",
    "        elif column == 'TotalPM25':\n",
    "            threshold = 0.0000001\n",
    "\n",
    "        gdf_filtered = gdf_column[(gdf_column[column].abs() > threshold)]\n",
    "\n",
    "        geojson_data = gdf_filtered.to_json()\n",
    "        modified_geojson = modify_geojson(geojson_data, column)\n",
    "\n",
    "        filename = os.path.join(webdata_path, f'INMAP_{column}.json')\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(modified_geojson)\n",
    "\n",
    "        print(f\"GeoJSON data for column '{column}' has been saved to '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "##############\n",
    "# STEP 1: get BenMAP grid shapefile\n",
    "##############\n",
    "\n",
    "# Define file paths\n",
    "\n",
    "TARGET_GRID_LEVEL = 'county'\n",
    "\n",
    "if TARGET_GRID_LEVEL == 'county':\n",
    "\n",
    "    grid_shapefile_path = '/Users/yunhalee/Documents/LOCAETA/RCM/BenMAP/grids/County/County.shp'\n",
    "    grid_gdf = gpd.read_file(grid_shapefile_path)\n",
    "\n",
    "elif TARGET_GRID_LEVEL == 'tracts':\n",
    "\n",
    "    ## TODO - add BenMAP tract shapefile to get Row and Col information\n",
    "\n",
    "    grid_shapefile_path = '/Users/yunhalee/Documents/LOCAETA/RCM/BenMAP/grids/US Census Tracts/US Census Tracts.shp'\n",
    "    grid_gdf = gpd.read_file(grid_shapefile_path)\n",
    "\n",
    "# Rename Columns\n",
    "grid_gdf.rename( columns={\"COL\":\"Col\", \"ROW\": \"Row\"}, inplace=True)\n",
    "# Define file paths\n",
    "\n",
    "benmap_output_type =['incidence' , 'valuation'] # \n",
    "runnames = [ 'TN_DataCenter_NOx_2ppm','TN_DataCenter_NOx_25ppm'] #['LA_CCS', 'LA_CCS_noNH3']  # [ 'CO_CCS', 'CO_CCS_wo_NH3_VOC', 'CO_Cherokee_CCS_wo_NH3_VOC','CO_Suncor_CCS_wo_NH3_VOC', 'NEI_no_Landfill_2001411'] # # \n",
    "regions = {'TN': '47', \"Nation\": None} #{\"CO\": '08', \"Nation\": None}\n",
    "\n",
    "for runname in runnames: \n",
    "    for benmap_output in benmap_output_type:\n",
    "\n",
    "        benmap_output_file = f'/Users/yunhalee/Documents/LOCAETA/RCM/BenMAP/batchmode/APVR/control_{runname}_{TARGET_GRID_LEVEL}_inmap_2020_pm25-{benmap_output}.csv'\n",
    "        #benmap_output_file = f\"/Users/yunhalee/Documents/LOCAETA/RCM/BenMAP/batchmode/APVR/GUI_{benmap_output}_08052024.csv\"\n",
    "        output_dir = f\"/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/BenMAP/{TARGET_GRID_LEVEL}/{benmap_output}_results/{runname}\"\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "        ##############\n",
    "        # STEP 2: Create final dataframe containing geometry and BenMAP key results ,and compute the normalized Mean with the population (x1e6) for incidence\n",
    "        ##############\n",
    "\n",
    "        benmap = pd.read_csv(benmap_output_file)\n",
    "\n",
    "        # this is only for GUI case\n",
    "        benmap.rename( columns={\"Column\":\"Col\"}, inplace=True)\n",
    "        benmap['Pollutant'] = 'PM2.5'\n",
    "    \n",
    "        # Merge incidence with pop_gdf on the 'Row' and 'Col' columns\n",
    "        merged_df = benmap.merge(grid_gdf, on=['Row', 'Col'], how='left')\n",
    "\n",
    "        if benmap_output == 'incidence':\n",
    "            columns_to_keep = ['Endpoint', 'Pollutant', 'Author', 'Race', 'Ethnicity', 'STATE_FIPS','CNTY_FIPS', 'Row', 'Col', 'Mean', 'Population','geometry']\n",
    "        else:\n",
    "            columns_to_keep = ['Endpoint', 'Pollutant', 'Author', 'Race','Ethnicity', 'STATE_FIPS','CNTY_FIPS', 'Row', 'Col', 'Mean','geometry']\n",
    "\n",
    "        final_df = merged_df[columns_to_keep]\n",
    "\n",
    "        # change mortality endpoint\n",
    "        # Modify Endpoint based on Author\n",
    "        final_df.loc[final_df['Author'].str.contains('Pope', na=False), 'Endpoint'] = 'Mortality All Cause : Method 2'\n",
    "        final_df.loc[final_df['Author'].str.contains('Di', na=False), 'Endpoint'] = 'Mortality All Cause : Method 1'\n",
    "\n",
    "        # \"HISPANIC\" is ethinicty, not race, but I am going to reassign it as RACE for model analysis\n",
    "        final_df.loc[final_df['Ethnicity'].str.contains('HISPANIC', na=False), 'Race'] = 'HISPANIC'\n",
    "        final_df.drop(columns=['Ethnicity'])\n",
    "        \n",
    "        final_df.head()\n",
    "\n",
    "        ##############\n",
    "        # STEP 3: Create plots of total health impact (sum the health impact for each endpoint) either for State or Nation and Create map plots of local health impact\n",
    "        ##############\n",
    "\n",
    "        \n",
    "        #regions = {\"LA\": ['22','05', \"28\", \"48\"] , \"Nation\": None}\n",
    "        #regions = {\"Nation\": None}\n",
    "\n",
    "        final_df = gpd.GeoDataFrame(final_df, geometry= \"geometry\")\n",
    "        type(final_df)\n",
    "\n",
    "        # Loop over the regions dictionary and process the data\n",
    "        for region_name, state_fips in regions.items():\n",
    "            \n",
    "            # Subset the final DataFrame based on the chosen state or national\n",
    "            final_df_subset = subset_data(final_df, state_fips)\n",
    "\n",
    "            print(f\"Processing data for: {region_name}\")\n",
    "\n",
    "            if benmap_output == 'incidence':\n",
    "                # Group by ['Endpoint', 'Pollutant', 'Author', 'Race'] and calculate the sum of 'Mean'\n",
    "                race_grouped_sum = final_df_subset.groupby(['Endpoint', 'Pollutant', 'Author', 'Race']).agg({'Mean': 'sum', \"Population\":\"sum\"}).reset_index()\n",
    "\n",
    "                race_grouped_sum['Mean_per_Pop'] = race_grouped_sum['Mean'] / race_grouped_sum['Population'] * 1000000  # Scale by 1,000,000 for readability\n",
    "\n",
    "                # Plotting \"SUM\" barplots by 'Race' for each {endpoint}, {pollutant}, {author}\n",
    "                plot_bar(race_grouped_sum, \"Endpoint\", \"Mean\", f'Health benefits by Race in {region_name}', output_dir, benmap_output, 'Race')\n",
    "                \n",
    "                plot_bar(race_grouped_sum, \"Endpoint\", \"Mean_per_Pop\", f'Normalized Health benefits by Race in {region_name}', output_dir, benmap_output,'Race')\n",
    "\n",
    "                # Creating a table to show the values in the barplots\n",
    "                table_columns = ['Endpoint', 'Race', 'Mean', 'Mean_per_Pop']\n",
    "\n",
    "            else:\n",
    "                # Group by ['Endpoint', 'Pollutant', 'Author', 'Race'] and calculate the sum of 'Mean'\n",
    "                race_grouped_sum = final_df_subset.groupby(['Endpoint', 'Pollutant', 'Author', 'Race']).agg({'Mean': 'sum'}).reset_index()\n",
    "\n",
    "                # Plotting \"SUM\" barplots by 'Race' for each {endpoint}, {pollutant}, {author}\n",
    "                plot_bar(race_grouped_sum, \"Endpoint\", \"Mean\", f'Monetized Health benefits by Race in {region_name}', output_dir, benmap_output, 'Race')\n",
    "\n",
    "                # Creating a table to show the values in the barplots\n",
    "                table_columns = ['Endpoint', 'Race', 'Mean']\n",
    "\n",
    "            create_csv(race_grouped_sum, table_columns, f'Summary Table Health Benefits by Race in {region_name}', output_dir)\n",
    "\n",
    "            #result = final_df_subset[final_df_subset['Race'] == 'ALL'].groupby('Endpoint')['Mean'].sum().reset_index()\n",
    "            #plot_bar(result, \"Endpoint\", \"Mean\", f'Sum of Mean by Endpoint in {region_name}', output_dir)\n",
    "\n",
    "            # Group by ['Endpoint', 'Pollutant', 'Author']\n",
    "            grouped = final_df_subset.groupby(['Endpoint', 'Pollutant', 'Author'])\n",
    "\n",
    "            for (endpoint, pollutant, author), group in grouped:\n",
    "\n",
    "                #if region_name != 'Nation': # skip Nation mapping \n",
    "                    ## print(f\"checking group df {group.head()}\")\n",
    "                    \n",
    "                print(\"basemap mapping started\")\n",
    "                ## title_mean = f'Spatial Distribution for {endpoint} ({pollutant}, {author}) in {region_name}'\n",
    "                ## plot_spatial_distribution_combined(group, title_mean, \"Mean\", output_dir)\n",
    "                plot_spatial_distribution_benmap_with_basemap(group, \"Mean\", output_dir, region_name)\n",
    "            \n",
    "                if benmap_output == 'incidence':\n",
    "                    group['Mean_per_Pop'] = group.apply(lambda row: row['Mean'] / row['Population'] * 1000000, axis=1)  # Scale by 1,000,000 for readability\n",
    "\n",
    "                    plot_spatial_distribution_benmap_with_basemap(group, \"Mean_per_Pop\", output_dir, region_name)\n",
    "                    ## title_mean_per_pop = f'Spatial Distribution for {endpoint} ({pollutant}, {author}) per Population in {region_name}'\n",
    "                    ## plot_spatial_distribution_combined(group, title_mean_per_pop, \"Mean_per_Pop\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make health impact bar plot for all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def plot_health_benefit(combined_df, y_label, output_dir): \n",
    "    # Convert DataFrame from wide to long format\n",
    "    df_long = combined_df.melt(id_vars=[\"Endpoint\", \"Race\"], var_name=\"Run\", value_name=\"Value\")\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    df_long = df_long.dropna(subset=[\"Value\"])\n",
    "\n",
    "    # Skip zero values if desired\n",
    "    df_long = df_long[df_long[\"Value\"] != 0]\n",
    "\n",
    "    # Set Seaborn style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Determine the number of unique \"Endpoint\" categories with data\n",
    "    endpoints_with_data = df_long[\"Endpoint\"].unique()\n",
    "    num_endpoints = len(endpoints_with_data)\n",
    "\n",
    "    # Adjust figure size dynamically\n",
    "    fig_width = min(25, num_endpoints * 8)\n",
    "    fig_height = 10\n",
    "\n",
    "    # Create figure and axes manually for better control\n",
    "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    if num_endpoints <= 3:\n",
    "        ncols = num_endpoints\n",
    "    else:\n",
    "        ncols = 3\n",
    "        \n",
    "    nrows = (num_endpoints + ncols - 1) // ncols\n",
    "    gridspec = fig.add_gridspec(nrows=nrows, ncols=ncols, hspace=0.4, wspace=0.3)\n",
    "\n",
    "    # Create each subplot individually\n",
    "    for i, endpoint in enumerate(endpoints_with_data):\n",
    "        row, col = i // ncols, i % ncols\n",
    "        ax = fig.add_subplot(gridspec[row, col])\n",
    "        \n",
    "        # Filter data for this endpoint\n",
    "        endpoint_data = df_long[df_long[\"Endpoint\"] == endpoint]\n",
    "        \n",
    "        # Skip if no data\n",
    "        if len(endpoint_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Create the bar plot\n",
    "        ax = sns.barplot(data=endpoint_data, x=\"Race\", y=\"Value\", hue=\"Run\", ax=ax)\n",
    "\n",
    "        # Set title and labels\n",
    "        ax.set_title(f\"Endpoint: {endpoint}\", fontsize=13, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Race-Ethnicity\", fontsize=15)\n",
    "        ax.set_ylabel(y_label, fontsize=15)\n",
    "\n",
    "        # Regular rotation for other endpoints\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "        \n",
    "        # Remove legend from individual subplots (we'll add a common one later)\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "        # Add vertical gridlines between categories\n",
    "        # Get the actual x tick positions\n",
    "        tick_positions = ax.get_xticks()\n",
    "        for j in range(len(tick_positions)):\n",
    "            if j > 0:  # Skip the first one to avoid a line at the left edge\n",
    "                # Place gridline between tick positions\n",
    "                midpoint = (tick_positions[j-1] + tick_positions[j]) / 2\n",
    "                ax.axvline(x=midpoint, color='grey', linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "        \n",
    "        # Remove legend from individual subplots (we'll add a common one later)\n",
    "        if hasattr(ax, 'get_legend') and ax.get_legend() is not None:\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "    # Add a single legend for the entire figure\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, title=\"Run\", loc=\"upper center\", \n",
    "            bbox_to_anchor=(0.5, 0.05), ncol=min(5, len(labels)), frameon=True)\n",
    "\n",
    "    if '\\n' in y_label:\n",
    "        filename_label = y_label.replace('\\n', '')\n",
    "    else:\n",
    "        filename_label = y_label\n",
    "\n",
    "    # Improve overall layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)  # Make room for the legend\n",
    "    plt.savefig(os.path.join(output_dir, f'{filename_label}.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_only_mortality(combined_df, y_label, output_dir): \n",
    "\n",
    "    # Filter the dataframe to get only rows containing \"Mortality All Cause\" in the Endpoint\n",
    "    mortality_df = combined_df[combined_df['Endpoint'].str.contains('Mortality All Cause') & combined_df['Race'].str.contains('ALL') ].copy()\n",
    "\n",
    "    mortality_df.drop(columns='Race', inplace=True)\n",
    "    mortality_df.set_index('Endpoint', inplace=True)\n",
    "    print(mortality_df.head())\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Create the bar plot\n",
    "    ax = mortality_df.plot(kind='bar')\n",
    "\n",
    "    # Customize the plot (optional)\n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xticks(fontsize=12, rotation=0) # Rotate x-axis labels for better readability\n",
    "    # Add grid lines for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add vertical gridlines between categories\n",
    "    # Get the actual x tick positions\n",
    "    tick_positions = ax.get_xticks()\n",
    "    for j in range(len(tick_positions)):\n",
    "        if j > 0:  # Skip the first one to avoid a line at the left edge\n",
    "            # Place gridline between tick positions\n",
    "            midpoint = (tick_positions[j-1] + tick_positions[j]) / 2\n",
    "            ax.axvline(x=midpoint, color='grey', linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "\n",
    "    # Add a single legend for the entire figure\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, title=\"Run\", loc=\"upper center\", \n",
    "            bbox_to_anchor=(0.5, -0.1), ncol=min(2, len(labels)), frameon=True)\n",
    "        \n",
    "    # Add value labels on top of bars\n",
    "    for container in ax.containers:\n",
    "        if 'Monetized' in y_label:\n",
    "            ax.bar_label(container, fmt='{:,.0f}')\n",
    "        else:\n",
    "            ax.bar_label(container, fmt='{:,.1f}')\n",
    "\n",
    "    if '\\n' in y_label:\n",
    "        filename_label = y_label.replace('\\n', '')\n",
    "    else:\n",
    "        filename_label = y_label\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{filename_label} only mortality.png'), dpi=300, bbox_inches='tight')\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "benmap_output_type =['incidence' , 'valuation']\n",
    "TARGET_GRID_LEVEL = 'county'\n",
    "runnames = [ 'TN_DataCenter_NOx_2ppm','TN_DataCenter_NOx_25ppm'] # runnames =  ['CO_CCS', 'CO_CCS_wo_NH3_VOC', 'CO_Cherokee_CCS_wo_NH3_VOC','CO_Suncor_CCS_wo_NH3_VOC' ,  'NEI_no_Landfill_2001411'] #['LA_CCS', 'LA_CCS_noNH3'] #  # ['ZERO'] # \n",
    "target_file = 'County_Summary Table Health Benefits by Race in Nation.csv'\n",
    "\n",
    "\n",
    "for benmap_output in benmap_output_type:\n",
    "    combined_df = None\n",
    "    combined_df_normalized = None\n",
    "\n",
    "    # Iterate over each run name\n",
    "    for runname in runnames:\n",
    "        # Construct the file path\n",
    "        output_dir = f\"/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/BenMAP/{TARGET_GRID_LEVEL}/{benmap_output}_results/\"\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(output_dir + runname + \"/\" + target_file)\n",
    "        \n",
    "        # Filter out rows where 'Mean' is zero\n",
    "        df = df[df['Mean'] != 0]\n",
    "        \n",
    "        if benmap_output == 'incidence' :\n",
    "            # Rename columns to include the run name\n",
    "            df.rename(columns={'Mean': runname, 'Mean_per_Pop': f'{runname}_(per_million)'}, inplace=True)\n",
    "        else:\n",
    "            df.rename(columns={'Mean': runname}, inplace=True)\n",
    "        \n",
    "        # Select only the necessary columns for merging\n",
    "        df_mean = df[['Endpoint', 'Race', runname]]\n",
    "\n",
    "        # Merge DataFrames on 'Endpoint' and 'Race'\n",
    "        if combined_df is None:\n",
    "            combined_df = df_mean  # Initialize with the first DataFrame\n",
    "        else:\n",
    "            combined_df = pd.merge(combined_df, df_mean, on=['Endpoint', 'Race'], how='outer')\n",
    "\n",
    "        if benmap_output == 'incidence' :\n",
    "            df_normalized = df[['Endpoint', 'Race', f'{runname}_(per_million)']]\n",
    "        \n",
    "            if combined_df_normalized is None:\n",
    "                combined_df_normalized = df_normalized  # Initialize with the first DataFrame\n",
    "            else:\n",
    "                combined_df_normalized = pd.merge(combined_df_normalized, df_normalized, on=['Endpoint', 'Race'], how='outer')\n",
    "\n",
    "    # Print or process the combined DataFrames\n",
    "    print(f\"Combined DataFrame for {benmap_output}:\\n\", combined_df)\n",
    "    print(f\"Combined Normalized DataFrame for {benmap_output}:\\n\", combined_df_normalized)\n",
    "\n",
    "    if benmap_output == 'incidence' :\n",
    "        plot_health_benefit(combined_df, 'Health Benefits', output_dir+runnames[0])\n",
    "        plot_health_benefit(combined_df_normalized, \"Normalized Health Benefits \\n (per million)\", output_dir+runnames[0])\n",
    "        plot_only_mortality(combined_df, 'Health Benefits', output_dir+runnames[0])\n",
    "\n",
    "        combined_df.to_csv(output_dir+runnames[0]+ \"/Health_Benefit.csv\", index=False)\n",
    "        combined_df_normalized.to_csv(output_dir+runnames[0]+ \"/Normalized_Health_Benefit_all_runs.csv\", index=False)\n",
    "\n",
    "    elif benmap_output == 'valuation':\n",
    "        plot_health_benefit(combined_df, 'Monetized Health Benefits \\n (per million $)', output_dir+runnames[0])\n",
    "        plot_only_mortality(combined_df, 'Monetized Health Benefits \\n (per million $)', output_dir+runnames[0])\n",
    "        combined_df.to_csv(output_dir+runnames[0] + \"/Monetized_Health_Benefit.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from great_tables import GT\n",
    "\n",
    "\n",
    "GT(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make JSON files from BenMAP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "\n",
    "def convert_group_to_geojson(group):\n",
    "    \"\"\"Convert a GeoDataFrame group to a list of GeoJSON features.\"\"\"\n",
    "    features = json.loads(group.to_json())['features']\n",
    "    for feature in features:\n",
    "        # Rename 'Mean' to 'Quantity'\n",
    "        feature['properties']['Quantity'] = feature['properties'].pop('Mean')\n",
    "    return features\n",
    "\n",
    "def save_grouped_benmap_json(gdf, webdata_output_dir, output_file, quantity_descriptor, threshold=1e-2):\n",
    "    \"\"\"Save a single JSON file with Endpoint and Race groupings.\"\"\"\n",
    "    \n",
    "    display_properties = {\n",
    "        'color': '#808080',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.75,\n",
    "        'type': 'polygon',\n",
    "        'iconFile': 'none',\n",
    "        'layerOrder': 5,\n",
    "        'onLoad': False,\n",
    "        'legendEntry': 'none'\n",
    "    }\n",
    "\n",
    "    if 'Monetized' in quantity_descriptor:\n",
    "        # Filter out small values\n",
    "        gdf_filtered = gdf[gdf['Mean'].abs() >= 1e3]\n",
    "    else: \n",
    "        # Filter out small values\n",
    "        gdf_filtered = gdf[gdf['Mean'].abs() >= threshold]\n",
    "\n",
    "    grouped = gdf_filtered.groupby(['Endpoint', 'Race'])\n",
    "\n",
    "    print(\"grouped \", grouped.head())\n",
    "\n",
    "    data_dict = {}\n",
    "    for (endpoint, race), group in grouped:\n",
    "        features = convert_group_to_geojson(group[['geometry', 'Mean']].copy())\n",
    "        if endpoint not in data_dict:\n",
    "            data_dict[endpoint] = {}\n",
    "\n",
    "        data_dict[endpoint][race] = {\n",
    "            \"QuantityDescriptor\": f\"{quantity_descriptor} for {endpoint} in {race} group\",\n",
    "            \"features\": features\n",
    "        }\n",
    "\n",
    "    final_json = {\n",
    "        \"displayProperties\": display_properties,\n",
    "        \"data\": data_dict\n",
    "    }\n",
    "\n",
    "    filename = os.path.join(webdata_output_dir, f'BenMAP_{output_file}.json')\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(final_json, f, indent=2)\n",
    "\n",
    "    print(f\"Grouped GeoJSON saved to '{filename}'.\")\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "# STEP 1: get BenMAP grid shapefile\n",
    "##############\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "\n",
    "\n",
    "webdata_path = '/Users/yunhalee/Documents/LOCAETA/github/LOCAETA/WebTool/Data/'\n",
    "\n",
    "\n",
    "TARGET_GRID_LEVEL = 'county'\n",
    "\n",
    "if TARGET_GRID_LEVEL == 'county':\n",
    "\n",
    "    grid_shapefile_path = '/Users/yunhalee/Documents/LOCAETA/RCM/BenMAP/grids/County/County.shp'\n",
    "    grid_gdf = gpd.read_file(grid_shapefile_path)\n",
    "\n",
    "elif TARGET_GRID_LEVEL == 'tracts':\n",
    "\n",
    "    ## TODO - add BenMAP tract shapefile to get Row and Col information\n",
    "\n",
    "    grid_shapefile_path = '/Users/yunhalee/Documents/LOCAETA/RCM/BenMAP/grids/US Census Tracts/US Census Tracts.shp'\n",
    "    grid_gdf = gpd.read_file(grid_shapefile_path)\n",
    "\n",
    "# Rename Columns\n",
    "grid_gdf.rename( columns={\"COL\":\"Col\", \"ROW\": \"Row\"}, inplace=True)\n",
    "# Define file paths\n",
    "\n",
    "benmap_output_type =['incidence', 'valuation'] # \n",
    "runnames = [ 'TN_DataCenter_NOx_2ppm','TN_DataCenter_NOx_25ppm'] #['LA_CCS', 'LA_CCS_noNH3']  # [ 'CO_CCS', 'CO_CCS_wo_NH3_VOC', 'CO_Cherokee_CCS_wo_NH3_VOC','CO_Suncor_CCS_wo_NH3_VOC', 'NEI_no_Landfill_2001411'] # # \n",
    "regions = {'TN': '47', \"Nation\": None} #{\"CO\": '08', \"Nation\": None}\n",
    "\n",
    "for runname in runnames: \n",
    "    for benmap_output in benmap_output_type:\n",
    "\n",
    "\n",
    "        webdata_output_dir = f\"{webdata_path}/{runname}\"\n",
    "        benmap_output_file = f'/Users/yunhalee/Documents/LOCAETA/RCM/BenMAP/batchmode/APVR/control_{runname}_{TARGET_GRID_LEVEL}_inmap_2020_pm25-{benmap_output}.csv'\n",
    "        #benmap_output_file = f\"/Users/yunhalee/Documents/LOCAETA/RCM/BenMAP/batchmode/APVR/GUI_{benmap_output}_08052024.csv\"\n",
    "        output_dir = f\"/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/BenMAP/{TARGET_GRID_LEVEL}/{benmap_output}_results/{runname}\"\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        if not os.path.exists(webdata_output_dir):\n",
    "            os.makedirs(webdata_output_dir)\n",
    "\n",
    "\n",
    "        ##############\n",
    "        # STEP 2: Create final dataframe containing geometry and BenMAP key results ,and compute the normalized Mean with the population (x1e6) for incidence\n",
    "        ##############\n",
    "\n",
    "        benmap = pd.read_csv(benmap_output_file)\n",
    "\n",
    "        # this is only for GUI case\n",
    "        benmap.rename( columns={\"Column\":\"Col\"}, inplace=True)\n",
    "        benmap['Pollutant'] = 'PM2.5'\n",
    "    \n",
    "        # Merge incidence with pop_gdf on the 'Row' and 'Col' columns\n",
    "        merged_df = benmap.merge(grid_gdf, on=['Row', 'Col'], how='left')\n",
    "\n",
    "        if benmap_output == 'incidence':\n",
    "            columns_to_keep = ['Endpoint', 'Race', 'Author', 'Ethnicity', 'Row', 'Col', 'Mean', 'geometry']\n",
    "            quantity_descriptor = 'Health Benefits of'\n",
    "        else:\n",
    "            columns_to_keep = ['Endpoint', 'Race','Author','Ethnicity', 'Row', 'Col', 'Mean','geometry']\n",
    "            quantity_descriptor = 'Monetized Health Benefits of'\n",
    "\n",
    "        final_df = merged_df[columns_to_keep]\n",
    "\n",
    "        # change mortality endpoint\n",
    "        # Modify Endpoint based on Author\n",
    "        final_df.loc[final_df['Author'].str.contains('Pope', na=False), 'Endpoint'] = 'Mortality All Cause : Method 2'\n",
    "        final_df.loc[final_df['Author'].str.contains('Di', na=False), 'Endpoint'] = 'Mortality All Cause : Method 1'\n",
    "        final_df.drop(columns=['Author'], inplace=True)\n",
    "\n",
    "        # \"HISPANIC\" is ethinicty, not race, but I am going to reassign it as RACE for model analysis\n",
    "        final_df.loc[final_df['Ethnicity'].str.contains('HISPANIC', na=False), 'Race'] = 'HISPANIC'\n",
    "        final_df.drop(columns=['Ethnicity'], inplace=True)\n",
    "        \n",
    "        print(\"final df\", final_df.head())\n",
    "\n",
    "        ##############\n",
    "        # STEP 3: Create plots of total health impact (sum the health impact for each endpoint) either for State or Nation and Create map plots of local health impact\n",
    "        ##############\n",
    "\n",
    "        \n",
    "        #regions = {\"LA\": ['22','05', \"28\", \"48\"] , \"Nation\": None}\n",
    "        #regions = {\"Nation\": None}\n",
    "\n",
    "        final_gdf = gpd.GeoDataFrame(final_df, geometry= \"geometry\")\n",
    "        type(final_gdf)\n",
    "\n",
    "        save_grouped_benmap_json(final_gdf, webdata_output_dir, benmap_output, quantity_descriptor )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
