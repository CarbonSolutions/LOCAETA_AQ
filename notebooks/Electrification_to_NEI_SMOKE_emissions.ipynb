{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preparing Electrification emissions into NEI-SMOKE format\n",
    "\n",
    "Caleb prepared two sets of emission files (e.g., facility_easyhard (with EIS_ID and SCC) and powerplant_easyhard (with only EIS_ID)). \n",
    "\n",
    " - For \"facility\" data, it will be similar to the CCS emission processing (matching EIS_ID and SCC).\n",
    "\n",
    " - For the powerplant data, I need to process the facility-level emisisons using 2020 NEI emissions (per EIS ID and per SCC) by splitting into each SCC by the NEI 2020 emissions weight. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Read raw emissions and NEI-SMOKE all point source shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "import os, sys\n",
    "\n",
    "# Add the path to the main package directory\n",
    "package_path = os.path.abspath('/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/LOCAETA_AQ')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "import emission_processing\n",
    "\n",
    "# Read the point source emissions\n",
    "original_emis ='/Users/yunhalee/Documents/LOCAETA/RCM/INMAP/evaldata_v1.6.1/2020_nei_emissions/combined_NEI2020_pt_oilgas_ptegu_ptnonipm_w_sectors.shp'\n",
    "nei_all_pt = gpd.read_file(original_emis)\n",
    "\n",
    "# Reset index to ensure proper comparison\n",
    "nei_all_pt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# rename the nei emissions\n",
    "pollutant_cols = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "\n",
    "col_dict = {}\n",
    "for poll in pollutant_cols:\n",
    "    col_dict[poll] = f'{poll}_nei'\n",
    "\n",
    "nei_all_pt.rename(columns = col_dict, inplace=True)\n",
    "\n",
    "nei_all_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def reformat_powerplant(df):\n",
    "\n",
    "    # columns I need\n",
    "    pollutant_cols = [col for col in df.columns if '_tons_final' in col]\n",
    "    base_cols = [col for col in df.columns if '_tons_base' in col]\n",
    "    Facilities_col_names = pollutant_cols + base_cols + ['eis','cambium_gea','DOE/EIA ORIS plant or facility code']\n",
    "\n",
    "    # subset the dataframe \n",
    "    df = df[Facilities_col_names] \n",
    "\n",
    "    # Total before grouping\n",
    "    total_before = df[pollutant_cols + base_cols].sum()\n",
    "\n",
    "    # drop the rows if eis is missing\n",
    "    df = df.dropna(subset=['eis'])\n",
    "\n",
    "    # Total after grouping\n",
    "    total_after = df[pollutant_cols + base_cols].sum()\n",
    "\n",
    "    # Define columns as integers\n",
    "    df = df.astype({'eis': 'int64', 'DOE/EIA ORIS plant or facility code': 'int64'})\n",
    "\n",
    "    # rename columns\n",
    "    df.rename(columns={'eis': 'EIS_ID', 'DOE/EIA ORIS plant or facility code': \"oris_ID\"}, inplace=True)\n",
    "\n",
    "    # Group by EIS_ID and aggregate emissions and cambium_gea\n",
    "    df_grouped = df.groupby('EIS_ID').agg({\n",
    "        **{col: 'sum' for col in pollutant_cols + base_cols},\n",
    "        'cambium_gea': 'first' \n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    # Check if Totals are preserved\n",
    "    print('before :', total_before, 'after: ', total_after) \n",
    "    #print(\"Reformatted df with cambium_gea:\", df_grouped.head())\n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def find_minimal_unique_identifier_columns(df, max_combination_size=30):\n",
    "    \"\"\"\n",
    "    Finds the minimal set of columns that uniquely identify rows in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: pandas.DataFrame\n",
    "        max_combination_size: int, maximum number of columns to consider in combinations (avoid long runtime)\n",
    "\n",
    "    Returns:\n",
    "        List of column names or None\n",
    "    \"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    for r in range(1, min(len(cols), max_combination_size) + 1):\n",
    "        for combo in combinations(cols, r):\n",
    "            if not df.duplicated(subset=combo).any():\n",
    "                return list(combo)\n",
    "    return None\n",
    "\n",
    "def mapping_powerplant_to_nei(nei_with_powerplant, nei_all_pt, unique_identifier_columns, is_base):\n",
    "\n",
    "    if is_base: \n",
    "        # Column mapping between NEI and eGRID\n",
    "        pollutant_map = {\n",
    "            'NOx': 'NOx_tons_base',\n",
    "            'PM2_5': 'PM2.5_tons_base',\n",
    "            'VOC': 'VOC_tons_base',\n",
    "            'NH3': 'NH3_tons_base',\n",
    "            'SOx': 'SO2_tons_base'\n",
    "        }\n",
    "    else:\n",
    "        pollutant_map = {\n",
    "            'NOx': 'NOx_tons_final',\n",
    "            'PM2_5': 'PM2.5_tons_final',\n",
    "            'VOC': 'VOC_tons_final',\n",
    "            'NH3': 'NH3_tons_final',\n",
    "            'SOx': 'SO2_tons_final'\n",
    "        }\n",
    "\n",
    "    # add Boolean to track back the data center data later\n",
    "    nei_with_powerplant['was_mapped'] = True  # add flag\n",
    "\n",
    "\n",
    "    # Compute and apply split factors per pollutant\n",
    "    for nei_col, Facilities_col in pollutant_map.items():\n",
    "\n",
    "        print (nei_col, Facilities_col)\n",
    "        # Group sum for each pollutant by EIS_ID\n",
    "        total_by_eis = nei_with_powerplant.groupby('EIS_ID')[f'{nei_col}_nei'].transform('sum')\n",
    "        nei_with_powerplant[f'{nei_col}_total_by_eis'] = total_by_eis\n",
    "\n",
    "        # Default: compute split factor using NEI emissions\n",
    "        split_col = f'{nei_col}_split'\n",
    "\n",
    "        nei_with_powerplant[split_col] = np.where(\n",
    "            total_by_eis == 0, \n",
    "            np.nan, \n",
    "            nei_with_powerplant[f'{nei_col}_nei'] / total_by_eis\n",
    "        )\n",
    "\n",
    "        # Find EIS_IDs where total_by_eis is zero but Facilities_col is non-zero\n",
    "        mask_zero_total = (total_by_eis == 0) & nei_with_powerplant[Facilities_col].notna() & (nei_with_powerplant[Facilities_col] != 0)\n",
    "\n",
    "        print(f\"{nei_col}: # fallback allocations due to zero NEI = {mask_zero_total.sum()}\")\n",
    "\n",
    "        # For these EIS_IDs, assign equal split factor across matching rows\n",
    "        for eid in nei_with_powerplant.loc[mask_zero_total, 'EIS_ID'].unique():\n",
    "            match_rows = nei_with_powerplant['EIS_ID'] == eid\n",
    "            n_rows = match_rows.sum()\n",
    "            nei_with_powerplant.loc[match_rows, split_col] = 1.0 / n_rows\n",
    "\n",
    "        # Now compute eGRID-scaled emissions and save as nei original name\n",
    "        nei_with_powerplant[f'{nei_col}'] = nei_with_powerplant[split_col] * nei_with_powerplant[Facilities_col]\n",
    "\n",
    "        print(f\"after {Facilities_col} splitting : \", nei_with_powerplant[f'{nei_col}'].sum())\n",
    "    # OPTIONAL: Drop intermediate split columns\n",
    "    #nei_with_powerplant.drop(columns=[f'{k}_split' for k in pollutant_map], inplace=True)\n",
    "    # Merge results back into the full NEI dataset\n",
    "\n",
    "    # Merge results back into the full NEI dataset\n",
    "    nei_all_pt_final = nei_all_pt.merge(\n",
    "        nei_with_powerplant[ \n",
    "            unique_identifier_columns + [\"was_mapped\",'cambium_gea'] + [f'{k}' for k in pollutant_map]\n",
    "        ],\n",
    "        on=unique_identifier_columns,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    gdf_subset = nei_all_pt_final[nei_all_pt_final['was_mapped'] == True]\n",
    "    print(\"base dataframe size \", gdf_subset.shape, nei_with_powerplant.shape)\n",
    "    print(\"before filling; subset nei sum \", gdf_subset[['PM2_5_nei', 'NH3_nei', 'VOC_nei', 'NOx_nei', 'SOx_nei']].sum())\n",
    "    print(\"before filling; subset base sum \", gdf_subset[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum())\n",
    "    print(\"before filling; nei_all_pt_final base sum \", nei_all_pt_final[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum()) \n",
    "\n",
    "    # fill the empty rows with NEI dataset\n",
    "    for k in pollutant_map:\n",
    "        nei_all_pt_final[f'{k}'] = nei_all_pt_final[f'{k}'].fillna(nei_all_pt_final[f'{k}_nei'])\n",
    "        nei_all_pt_final[f'{k}_diff'] = nei_all_pt_final[f'{k}'] - nei_all_pt_final[f'{k}_nei']\n",
    "\n",
    "    # Define difference columns\n",
    "    diff_cols = ['VOC_diff', 'NH3_diff', 'NOx_diff', 'SOx_diff', 'PM2_5_diff']\n",
    "\n",
    "    # Mask for rows that were mapped\n",
    "    mapped_mask = nei_all_pt_final['was_mapped'] == True\n",
    "\n",
    "    # Mask for rows with no difference in any pollutant\n",
    "    no_change_mask = (nei_all_pt_final[diff_cols] == 0).all(axis=1)\n",
    "\n",
    "    # Combine masks\n",
    "    mapped_but_unchanged = nei_all_pt_final[mapped_mask & no_change_mask]\n",
    "\n",
    "    # Show result\n",
    "    print(\"Number of rows where emissions were mapped but did not change:\", mapped_but_unchanged.shape[0])\n",
    "\n",
    "    # Remove rows where all values in specified columns are zero\n",
    "    gdf_subset = nei_all_pt_final[nei_all_pt_final['was_mapped'] == True]\n",
    "    print(\"subset dataframe size \", gdf_subset.shape, nei_with_powerplant.shape)\n",
    "    print(\"subset nei sum \", gdf_subset[['PM2_5_nei', 'NH3_nei', 'VOC_nei', 'NOx_nei', 'SOx_nei']].sum())\n",
    "    print(\"subset base sum \", gdf_subset[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum())\n",
    "\n",
    "    # drop the unnecessary columns\n",
    "    nei_all_pt_final.drop(columns=[f'{k}_diff' for k in pollutant_map], inplace=True)\n",
    "    #nei_all_pt_final.drop(columns=[f'{k}_nei' for k in pollutant_map], inplace=True)\n",
    "\n",
    "    return nei_all_pt_final\n",
    "\n",
    "# Diagnostic to find why eGRID sums don't match after merging with NEI\n",
    "\n",
    "def diagnose_egrid_nei_mismatch(egrid, nei_all_pt, nei_with_powerplant):\n",
    "    \"\"\"\n",
    "    Diagnose why eGRID sums don't match after filtering/merging with NEI data\n",
    "    \"\"\"\n",
    "    print(\"=== eGRID-NEI EIS_ID MISMATCH DIAGNOSTIC ===\")\n",
    "    \n",
    "    pollutant_cols = ['PM2.5_tons_base', 'NH3_tons_base', 'VOC_tons_base', 'NOx_tons_base', 'SO2_tons_base']\n",
    "    \n",
    "    # Get unique EIS_IDs from each dataset\n",
    "    egrid_eids = set(egrid['EIS_ID'].unique())\n",
    "    nei_eids = set(nei_all_pt['EIS_ID'].unique())\n",
    "    merged_eids = set(nei_with_powerplant['EIS_ID'].unique())\n",
    "    \n",
    "    print(f\"Unique EIS_IDs in eGRID: {len(egrid_eids)}\")\n",
    "    print(f\"Unique EIS_IDs in NEI: {len(nei_eids)}\")\n",
    "    print(f\"Unique EIS_IDs after merge: {len(merged_eids)}\")\n",
    "    \n",
    "    # Find missing EIS_IDs\n",
    "    egrid_not_in_nei = egrid_eids - nei_eids\n",
    "    nei_not_in_egrid = nei_eids - egrid_eids\n",
    "    egrid_lost_in_merge = egrid_eids - merged_eids\n",
    "    \n",
    "    print(f\"\\nEIS_IDs in eGRID but not in NEI: {len(egrid_not_in_nei)}\")\n",
    "    print(f\"EIS_IDs in NEI but not in eGRID: {len(nei_not_in_egrid)}\")\n",
    "    print(f\"EIS_IDs from eGRID lost after merge: {len(egrid_lost_in_merge)}\")\n",
    "    \n",
    "    if len(egrid_not_in_nei) > 0:\n",
    "        print(f\"\\n❌ FOUND THE PROBLEM: {len(egrid_not_in_nei)} eGRID facilities have no NEI data\")\n",
    "        \n",
    "        # Calculate emissions lost due to missing EIS_IDs\n",
    "        missing_egrid = egrid[egrid['EIS_ID'].isin(egrid_not_in_nei)]\n",
    "\n",
    "        pollutant_cols = ['PM2.5_tons_final', 'NH3_tons_final', 'VOC_tons_final', 'NOx_tons_final', 'SO2_tons_final']\n",
    "        \n",
    "        print(\"\\nFinal Emissions lost from missing EIS_IDs:\")\n",
    "        lost_emissions = missing_egrid[pollutant_cols].sum()\n",
    "        for col, value in lost_emissions.items():\n",
    "            print(f\"  {col}: {value}\") \n",
    "\n",
    "\n",
    "        \n",
    "        print(\"\\nBase Emissions lost from missing EIS_IDs:\")\n",
    "        lost_emissions = missing_egrid[pollutant_cols].sum()\n",
    "        for col, value in lost_emissions.items():\n",
    "            print(f\"  {col}: {value}\")\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"\\nAll missing EIS_IDs: {list(egrid_not_in_nei)}\")\n",
    "        \n",
    "        # Show some details about the missing facilities\n",
    "        print(f\"\\nDetails of first few missing facilities:\")\n",
    "        sample_missing = missing_egrid.head()[['EIS_ID'] + pollutant_cols]\n",
    "        for idx, row in sample_missing.iterrows():\n",
    "            print(f\"  EIS_ID {row['EIS_ID']}: NOx={row['NOx_tons_base']}, PM2.5={row['PM2.5_tons_base']}\")\n",
    "    \n",
    "    if len(egrid_lost_in_merge) > len(egrid_not_in_nei):\n",
    "        print(f\"\\n❌ ADDITIONAL PROBLEM: More EIS_IDs lost in merge than expected\")\n",
    "        extra_lost = egrid_lost_in_merge - egrid_not_in_nei\n",
    "        print(f\"Extra lost EIS_IDs: {len(extra_lost)}\")\n",
    "        print(f\"Sample extra lost: {list(extra_lost)[:5]}\")\n",
    "    \n",
    "    # Verify the math\n",
    "    original_sum = egrid[pollutant_cols].sum()\n",
    "    kept_egrid = egrid[egrid['EIS_ID'].isin(merged_eids)]\n",
    "    kept_sum = kept_egrid[pollutant_cols].sum()\n",
    "    \n",
    "    print(f\"\\n=== EMISSION ACCOUNTING ===\")\n",
    "    print(\"Original eGRID sums:\")\n",
    "    for col, val in original_sum.items():\n",
    "        print(f\"  {col}: {val}\")\n",
    "    \n",
    "    print(\"\\nSums for EIS_IDs that made it through merge:\")\n",
    "    for col, val in kept_sum.items():\n",
    "        print(f\"  {col}: {val}\")\n",
    "    \n",
    "    print(\"\\nDifference (lost emissions):\")\n",
    "    diff = original_sum - kept_sum\n",
    "    for col, val in diff.items():\n",
    "        print(f\"  {col}: {val}\")\n",
    "    \n",
    "    return {\n",
    "        'egrid_not_in_nei': egrid_not_in_nei,\n",
    "        'lost_emissions': missing_egrid[pollutant_cols].sum() if len(egrid_not_in_nei) > 0 else None,\n",
    "        'kept_egrid': kept_egrid\n",
    "    }\n",
    "\n",
    "# Quick function to check EIS_ID formats\n",
    "def check_eis_id_formats(egrid, nei_all_pt):\n",
    "    \"\"\"Check if EIS_ID formats might be causing mismatch\"\"\"\n",
    "    print(\"=== EIS_ID FORMAT CHECK ===\")\n",
    "    \n",
    "    egrid_sample = egrid['EIS_ID'].head(10).tolist()\n",
    "    nei_sample = nei_all_pt['EIS_ID'].head(10).tolist()\n",
    "    \n",
    "    print(\"Sample eGRID EIS_IDs:\", egrid_sample)\n",
    "    print(\"Sample NEI EIS_IDs:\", nei_sample)\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\neGRID EIS_ID dtype: {egrid['EIS_ID'].dtype}\")\n",
    "    print(f\"NEI EIS_ID dtype: {nei_all_pt['EIS_ID'].dtype}\")\n",
    "    \n",
    "    # Check for leading/trailing spaces\n",
    "    egrid_spaces = egrid['EIS_ID'].astype(str).str.contains('^ | $', regex=True).any()\n",
    "    nei_spaces = nei_all_pt['EIS_ID'].astype(str).str.contains('^ | $', regex=True).any()\n",
    "    \n",
    "    if egrid_spaces or nei_spaces:\n",
    "        print(\"❌ Found leading/trailing spaces in EIS_IDs\")\n",
    "        print(f\"  eGRID has spaces: {egrid_spaces}\")\n",
    "        print(f\"  NEI has spaces: {nei_spaces}\")\n",
    "    else:\n",
    "        print(\"✅ No leading/trailing spaces found\")\n",
    "    \n",
    "    # Check lengths\n",
    "    egrid_lengths = egrid['EIS_ID'].astype(str).str.len().unique()\n",
    "    nei_lengths = nei_all_pt['EIS_ID'].astype(str).str.len().unique()\n",
    "    \n",
    "    print(f\"\\neGRID EIS_ID lengths: {sorted(egrid_lengths)}\")\n",
    "    print(f\"NEI EIS_ID lengths: {sorted(nei_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "overall_scenario = 'Food_Agr'\n",
    "Scenario_dir_path = f'/Users/yunhalee/Documents/LOCAETA/Electrification/{overall_scenario}/'\n",
    "Scenario_list = [\"current_easyhard\", \"2050_easyhard_noIRA_111D\", \"2050_easyhard_decarb95\"] \n",
    "\n",
    "if overall_scenario != 'Full_USA':\n",
    "    scen_emis_list = { scen: f'{scen}_{overall_scenario}' for scen in Scenario_list} # {overall_scenario: emis_name}\n",
    "else:\n",
    "    scen_emis_list = { scen: f'{scen}' for scen in Scenario_list}\n",
    "\n",
    "for Scenario_name, Emis_name in scen_emis_list.items():\n",
    "\n",
    "    print (\"processing \", Scenario_name)\n",
    "    #Facilities_file = os.path.join(Scenario_dir_path, f'300MW_national_{Scenario_name}.csv')\n",
    "    Facilities_file = os.path.join(Scenario_dir_path, f'pp_{Scenario_name}.csv')\n",
    "\n",
    "    egrid = pd.read_csv(Facilities_file)\n",
    "\n",
    "    print(\"original data\", egrid[['PM2.5_tons_base', 'NH3_tons_base', 'VOC_tons_base', 'NOx_tons_base', 'SO2_tons_base']].sum())\n",
    "    egrid = reformat_powerplant(egrid)\n",
    "    print(\"after grouping\", egrid[['PM2.5_tons_base', 'NH3_tons_base', 'VOC_tons_base', 'NOx_tons_base', 'SO2_tons_base']].sum())\n",
    "\n",
    "\n",
    "\n",
    "    # Filter NEI rows to only those that exist in eGRID\n",
    "    nei_with_powerplant = nei_all_pt[nei_all_pt['EIS_ID'].isin(egrid['EIS_ID'])].copy()\n",
    "\n",
    "    # Subset only for necessary columns\n",
    "    nei_with_powerplant.drop(columns=['height', 'diam',\n",
    "        'temp', 'velocity'], inplace=True)\n",
    "\n",
    "    unique_identifier_columns = find_minimal_unique_identifier_columns(nei_with_powerplant)\n",
    "\n",
    "    if unique_identifier_columns:\n",
    "        print(\"Columns that uniquely identify rows:\", unique_identifier_columns)\n",
    "    else:\n",
    "        print(\"No combination of columns uniquely identifies rows.\")\n",
    "\n",
    "    print(\"filtering\", nei_with_powerplant.shape)\n",
    "\n",
    "    # Merge eGRID emissions\n",
    "    nei_with_powerplant = nei_with_powerplant.merge(egrid, on='EIS_ID', how='left')\n",
    "    true_sum = nei_with_powerplant.groupby('EIS_ID')[['PM2.5_tons_base', 'NH3_tons_base', 'VOC_tons_base', 'NOx_tons_base', 'SO2_tons_base']].first().sum()\n",
    "    print(\"intial nei_with_powerplant sum\", true_sum)\n",
    "\n",
    "    print(\"Merging egrid\", nei_with_powerplant.shape)\n",
    "\n",
    "    # DEBUGGING\n",
    "    check_eis_id_formats(egrid, nei_all_pt)\n",
    "    diagnostic_results = diagnose_egrid_nei_mismatch(egrid, nei_all_pt, nei_with_powerplant)\n",
    "    print(diagnostic_results)\n",
    "\n",
    "    for is_base_emission in [True, False]: \n",
    "    #for is_base_emission in [False]: \n",
    "        nei_all_pt_final = mapping_powerplant_to_nei(nei_with_powerplant, nei_all_pt, unique_identifier_columns, is_base = is_base_emission)\n",
    "\n",
    "        # Split into two GeoDataFrames based on was_mapped\n",
    "        mapped_df = nei_all_pt_final[nei_all_pt_final['was_mapped'] == True].copy()\n",
    "        unmapped_df = nei_all_pt_final[nei_all_pt_final['was_mapped'] != True].copy()\n",
    "\n",
    "        print(\"after filling; mapped_df sum \", mapped_df[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum()) \n",
    "\n",
    "        print(\"final size \", mapped_df.shape, unmapped_df.shape)\n",
    "\n",
    "        # Save outputs with region suffix\n",
    "        if is_base_emission:\n",
    "\n",
    "            # Save the mapped data with runname in the filename\n",
    "            if not mapped_df.empty:\n",
    "                mapped_filename = os.path.join(Scenario_dir_path, f\"{Emis_name}_pp_base.shp\")\n",
    "                mapped_df.to_file(mapped_filename, driver='ESRI Shapefile')\n",
    "                print(f\"Saved mapped data to {mapped_filename}\")\n",
    "        else:\n",
    "            # Save the mapped data with runname in the filename\n",
    "            if not mapped_df.empty:\n",
    "                mapped_filename = os.path.join(Scenario_dir_path, f\"{Emis_name}_pp.shp\")\n",
    "                mapped_df.to_file(mapped_filename, driver='ESRI Shapefile')\n",
    "                print(f\"Saved mapped data to {mapped_filename}\")\n",
    "\n",
    "        print(mapped_df['source_fil'].unique())\n",
    "        print(unmapped_df['source_fil'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Read facility data using CCS script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CCS_raw_file = f'/Users/yunhalee/Documents/LOCAETA/Electrification/{overall_scenario}/facility_easyhard.csv'\n",
    "Scenario_name, Emis_name = next(iter(scen_emis_list.items())) # get the first item-key which is current_easyhard\n",
    "\n",
    "if Scenario_name == 'current_easyhard':\n",
    "    print(\"Good to go\")\n",
    "else:\n",
    "    raise Exception(\"Wrong scenario!!\")\n",
    "\n",
    "# process CCS emissions to merge it with NEI emissions\n",
    "cs_emis = pd.read_csv(CCS_raw_file)\n",
    "\n",
    "# ensure scc column to be integer\n",
    "cs_emis['scc'] = cs_emis['scc'].astype(int)\n",
    "\n",
    "cs_emis.rename(columns={'eis': 'EIS_ID', 'scc': 'SCC'}, inplace = True)\n",
    "# Find all duplicate rows, including the first occurrence\n",
    "all_duplicates = cs_emis[cs_emis.duplicated(keep=False)]\n",
    "if all_duplicates.empty:\n",
    "    print(\"Great, no duplicate\")\n",
    "else:\n",
    "    print(\"Arg..there are duplicates\")\n",
    "    print(\"All duplicate rows:\")\n",
    "    print(all_duplicates)\n",
    "    \n",
    "print(\"cs_emis:\", cs_emis.head())\n",
    "\n",
    "# Check if EIS ID in cs_emis also exist in eGRID\n",
    "common_ids = cs_emis['EIS_ID'][cs_emis['EIS_ID'].isin(egrid['EIS_ID'])].unique()\n",
    "\n",
    "if not common_ids:\n",
    "    print(\"Great, no duplicated EIS_ID between cs_emis and egrid\")\n",
    "    # use the unmapped nei dataframe for cs_emis emission processing \n",
    "    nei_df = unmapped_df.copy()\n",
    "else:\n",
    "    print(\"Wait, there is duplicated EIS_ID between cs_emis and egrid\")\n",
    "    print(common_ids)\n",
    "    nei_df = nei_all_pt.copy()\n",
    "\n",
    "print(\"nei df:\", nei_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_non_powerplant_to_nei(nei_with_non_powerplant, nei_df, unique_identifier_columns, is_base):\n",
    "\n",
    "    if is_base: \n",
    "        pollutant_map = {\n",
    "            'NOx': 'NOx_tons_base',\n",
    "            'PM2_5': 'PM2.5_tons_base',\n",
    "            'VOC': 'VOC_tons_base',\n",
    "            'NH3': 'NH3_tons_base',\n",
    "            'SOx': 'SO2_tons_base'\n",
    "        }\n",
    "    else:\n",
    "        pollutant_map = {\n",
    "            'NOx': 'NOx_tons_final',\n",
    "            'PM2_5': 'PM2.5_tons_final',\n",
    "            'VOC': 'VOC_tons_final',\n",
    "            'NH3': 'NH3_tons_final',\n",
    "            'SOx': 'SO2_tons_final'\n",
    "        }\n",
    "\n",
    "    # add Boolean to track back the powerplant data later\n",
    "    nei_with_non_powerplant['was_mapped'] = True  \n",
    "\n",
    "    # Compute and apply split factors per pollutant\n",
    "    for nei_col, powerplant_col in pollutant_map.items():\n",
    "        print(nei_col, powerplant_col)\n",
    "\n",
    "        # Group sum by (EIS_ID, SCC) now\n",
    "        total_by_group = nei_with_non_powerplant.groupby(['EIS_ID', 'SCC'])[f'{nei_col}_nei'].transform('sum')\n",
    "\n",
    "        # Default split factor within (EIS_ID, SCC)\n",
    "        split_col = f'{nei_col}_split'\n",
    "        nei_with_non_powerplant[split_col] = (\n",
    "            nei_with_non_powerplant[f'{nei_col}_nei'] / total_by_group.replace(0, pd.NA)\n",
    "        )\n",
    "\n",
    "        # Handle zero-NEI cases where powerplant emissions exist\n",
    "        mask_zero_total = (\n",
    "            (total_by_group == 0) &\n",
    "            nei_with_non_powerplant[powerplant_col].notna() &\n",
    "            (nei_with_non_powerplant[powerplant_col] != 0)\n",
    "        )\n",
    "\n",
    "        print(f\"{nei_col}: # fallback allocations due to zero NEI = {mask_zero_total.sum()}\")\n",
    "\n",
    "        for (eid, scc) in (\n",
    "            nei_with_non_powerplant.loc[mask_zero_total, ['EIS_ID', 'SCC']]\n",
    "            .drop_duplicates()\n",
    "            .itertuples(index=False)\n",
    "        ):\n",
    "            match_rows = (nei_with_non_powerplant['EIS_ID'] == eid) & (nei_with_non_powerplant['SCC'] == scc)\n",
    "            n_rows = match_rows.sum()\n",
    "            nei_with_non_powerplant.loc[match_rows, split_col] = 1.0 / n_rows\n",
    "\n",
    "        # Scale emissions and overwrite pollutant column\n",
    "        nei_with_non_powerplant[nei_col] = (\n",
    "            nei_with_non_powerplant[split_col] * nei_with_non_powerplant[powerplant_col]\n",
    "        )\n",
    "\n",
    "    # Merge results back into the full NEI dataset\n",
    "    nei_all_pt_final = nei_df.merge(\n",
    "        nei_with_non_powerplant[ \n",
    "            unique_identifier_columns + [\"was_mapped\", 'cambium_gea'] + [f'{k}' for k in pollutant_map]\n",
    "        ],\n",
    "        on=unique_identifier_columns,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    gdf_subset = nei_all_pt_final[nei_all_pt_final['was_mapped'] == True]\n",
    "    print(\"base dataframe size \", gdf_subset.shape, nei_with_non_powerplant.shape)\n",
    "    print(\"before filling; subset nei sum \", gdf_subset[['PM2_5_nei', 'NH3_nei', 'VOC_nei', 'NOx_nei', 'SOx_nei']].sum())\n",
    "    print(\"before filling; subset base sum \", gdf_subset[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum())\n",
    "    print(\"before filling; nei_all_pt_final base sum \", nei_all_pt_final[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum()) \n",
    "\n",
    "    # Fill missing rows with NEI dataset values\n",
    "    for k in pollutant_map:\n",
    "        unmapped_mask = nei_all_pt_final['was_mapped'] != True\n",
    "        nei_all_pt_final.loc[unmapped_mask, k] = nei_all_pt_final.loc[unmapped_mask, k].fillna(nei_all_pt_final.loc[unmapped_mask, f'{k}_nei'])\n",
    "\n",
    "    return nei_all_pt_final \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#filter \n",
    "nei_with_non_powerplant = nei_df.merge(\n",
    "    cs_emis[['EIS_ID','SCC']], on=['EIS_ID','SCC'], how='inner'\n",
    ")\n",
    "\n",
    "\n",
    "# Subset only for necessary columns\n",
    "nei_with_non_powerplant.drop(columns=['height', 'diam',\n",
    "    'temp', 'velocity',  'cambium_gea'], inplace=True)\n",
    "\n",
    "# unique_identifier_columns = find_minimal_unique_identifier_columns(nei_with_non_powerplant)\n",
    "\n",
    "# if unique_identifier_columns:\n",
    "#     print(\"Columns that uniquely identify rows:\", unique_identifier_columns)\n",
    "# else:\n",
    "#     print(\"No combination of columns uniquely identifies rows.\")\n",
    "print(\"cs_emis original base sum \", cs_emis.filter(like='tons_base', axis=1).sum())\n",
    "print(\"cs_emis original final sum \", cs_emis.filter(like = 'tons_final', axis=1).sum()) \n",
    "\n",
    "print(\"filtering\", nei_df.shape, nei_with_non_powerplant.shape, cs_emis.shape)\n",
    "\n",
    "# Merge eGRID emissions\n",
    "nei_with_non_powerplant = nei_with_non_powerplant.merge(cs_emis, on=['EIS_ID', 'SCC'], how='left')\n",
    "\n",
    "true_sum = nei_with_non_powerplant.groupby(['EIS_ID','SCC'])[['PM2.5_tons_base', 'NH3_tons_base', 'VOC_tons_base', 'NOx_tons_base', 'SO2_tons_base']].first().sum()\n",
    "print(\"intial nei_with_non_powerplant sum\", true_sum)\n",
    "\n",
    "print(\"Merging egrid\", nei_with_non_powerplant.shape)\n",
    "\n",
    "for is_base_emission in [False, True ]: \n",
    "#for is_base_emission in [False]: \n",
    "    nei_all_pt_final = mapping_non_powerplant_to_nei(nei_with_non_powerplant, nei_all_pt, unique_identifier_columns, is_base = is_base_emission)\n",
    "\n",
    "    # nei_all_pt_final can't have NULL\n",
    "    nei_all_pt_final = nei_all_pt_final.fillna(0)\n",
    "\n",
    "    # Split into two GeoDataFrames based on was_mapped\n",
    "    mapped_df = nei_all_pt_final[nei_all_pt_final['was_mapped'] == True].copy()\n",
    "    unmapped_df = nei_all_pt_final[nei_all_pt_final['was_mapped'] != True].copy()\n",
    "\n",
    "    print(\"after filling; mapped_df sum \", mapped_df[['PM2_5', 'NH3', 'VOC', 'NOx', 'SOx']].sum()) \n",
    "\n",
    "    print(\"final size \", mapped_df.shape, unmapped_df.shape)\n",
    "\n",
    "    # Save outputs with region suffix\n",
    "    if is_base_emission:\n",
    "\n",
    "        # Save the mapped data with runname in the filename\n",
    "        if not mapped_df.empty:\n",
    "            mapped_filename = os.path.join(Scenario_dir_path, f\"{Emis_name}_base.shp\")\n",
    "            mapped_df.to_file(mapped_filename, driver='ESRI Shapefile')\n",
    "            print(f\"Saved mapped data to {mapped_filename}\")\n",
    "\n",
    "        # # Save the rest (unmapped data) as rest_NEI\n",
    "        # if not unmapped_df.empty:\n",
    "        #     rest_filename = os.path.join(Scenario_dir_path, f\"{Scenario_name}_base_rest_NEI.shp\")\n",
    "        #     unmapped_df.to_file(rest_filename, driver='ESRI Shapefile')\n",
    "        #     print(f\"Saved unmapped NEI data to {rest_filename}\")\n",
    "    else:\n",
    "        # Save the mapped data with runname in the filename\n",
    "        if not mapped_df.empty:\n",
    "            mapped_filename = os.path.join(Scenario_dir_path, f\"{Emis_name}.shp\")\n",
    "            mapped_df.to_file(mapped_filename, driver='ESRI Shapefile')\n",
    "            print(f\"Saved mapped data to {mapped_filename}\")\n",
    "\n",
    "        # Save the rest (unmapped data) as rest_NEI\n",
    "        if not unmapped_df.empty:\n",
    "            rest_filename = os.path.join(Scenario_dir_path, f\"{Emis_name}_rest_NEI.shp\")\n",
    "            unmapped_df.to_file(rest_filename, driver='ESRI Shapefile')\n",
    "            print(f\"Saved unmapped NEI data to {rest_filename}\")\n",
    "    print(mapped_df['source_fil'].unique())\n",
    "    print(unmapped_df['source_fil'].unique())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_sum = cs_emis[['PM2.5_tons_base','NH3_tons_base','VOC_tons_base','NOx_tons_base','SO2_tons_base']].sum()\n",
    "\n",
    "merged_sum = (\n",
    "    nei_with_non_powerplant\n",
    "    .groupby(['EIS_ID','SCC'])[['PM2.5_tons_base','NH3_tons_base','VOC_tons_base','NOx_tons_base','SO2_tons_base']]\n",
    "    .first()   # ensure one row per pair\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "print(\"cs_emis sum:\\n\", cs_sum)\n",
    "print(\"merged sum:\\n\", merged_sum)\n",
    "print(\"difference:\\n\", merged_sum - cs_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Evaluate new emissions formatted for NEI-SMOKE style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#overall_scenario = 'Food&Agr'\n",
    "emis_dir_path = f'/Users/yunhalee/Documents/LOCAETA/Electrification/{overall_scenario}/'\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_final_map = {\n",
    "    'NOx': 'NOx_tons_final',\n",
    "    'PM2_5': 'PM2.5_tons_final',\n",
    "    'VOC': 'VOC_tons_final',\n",
    "    'NH3': 'NH3_tons_final',\n",
    "    'SOx': 'SO2_tons_final'\n",
    "}\n",
    "\n",
    "# Column mapping between NEI and eGRID\n",
    "pollutant_base_map = {\n",
    "    'NOx': 'NOx_tons_base',\n",
    "    'PM2_5': 'PM2.5_tons_base',\n",
    "    'VOC': 'VOC_tons_base',\n",
    "    'NH3': 'NH3_tons_base',\n",
    "    'SOx': 'SO2_tons_base'\n",
    "}\n",
    "\n",
    "is_base_emission = True\n",
    "\n",
    "# nei emissions column names\n",
    "pollutant_cols = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "\n",
    "for scen_name, emis_name in scen_emis_list.items():\n",
    "\n",
    "    if is_base_emission: \n",
    "        pollutant_map = pollutant_base_map\n",
    "        file_path1 = os.path.join(emis_dir_path, f'{emis_name}_pp_base.shp')\n",
    "        file_path2 = os.path.join(emis_dir_path, f'current_easyhard_base.shp') \n",
    "        if overall_scenario != 'Full_USA': \n",
    "            file_path2 = os.path.join(emis_dir_path, f'current_easyhard_{overall_scenario}_base.shp')  \n",
    "\n",
    "    else:\n",
    "        pollutant_map = pollutant_final_map\n",
    "        file_path1 = os.path.join(emis_dir_path, f'{emis_name}_pp.shp')\n",
    "        file_path2 = os.path.join(emis_dir_path, f'current_easyhard.shp') \n",
    "        if overall_scenario != 'Full_USA':  \n",
    "            file_path2 = os.path.join(emis_dir_path, f'current_easyhard_{overall_scenario}.shp')   \n",
    "\n",
    "    # read emission scenario\n",
    "    final_emis1 = gpd.read_file(file_path1) \n",
    "    final_emis2 = gpd.read_file(file_path2) \n",
    "\n",
    "    # Reset index to ensure proper comparison\n",
    "    final_emis1.reset_index(drop=True, inplace=True)\n",
    "    final_emis2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print (\"processing \", emis_name)\n",
    "    original_file1 = os.path.join(emis_dir_path, f'pp_{scen_name}.csv')\n",
    "    original_emis1 = pd.read_csv(original_file1) \n",
    "    original_emis1 = reformat_powerplant(original_emis1)\n",
    "    original_file2 = os.path.join(emis_dir_path, f'facility_easyhard.csv')\n",
    "    original_emis2 = pd.read_csv(original_file2) \n",
    "\n",
    "    # ensure scc column to be integer\n",
    "    original_emis2['scc'] = original_emis2['scc'].astype(int)\n",
    "    original_emis2.rename(columns={'eis_id': 'EIS_ID', 'scc': 'SCC'}, inplace = True)\n",
    "\n",
    "    # --- sum by pollutant for each dataset ---\n",
    "    # final_emis already has pollutant_cols\n",
    "    sum_final1 = final_emis1[pollutant_cols].sum()\n",
    "    sum_final2 = final_emis2[pollutant_cols].sum()\n",
    "\n",
    "    # original_emis needs mapped column names\n",
    "    sum_orig1 = original_emis1[list(pollutant_map.values())].sum()\n",
    "    sum_orig2 = original_emis2[list(pollutant_map.values())].sum()\n",
    "\n",
    "    # Rename original_emis sums to match pollutant_cols\n",
    "    sum_orig1.index = pollutant_cols\n",
    "    sum_orig2.index = pollutant_cols\n",
    "\n",
    "    # --- create one DataFrame for plotting ---\n",
    "    df_compare = pd.DataFrame({\n",
    "        'pp_base': sum_final1,\n",
    "        'original_pp': sum_orig1,\n",
    "        'non_pp_base': sum_final2,\n",
    "        'original_non_pp': sum_orig2\n",
    "    })\n",
    "\n",
    "    print(df_compare)\n",
    "\n",
    "    # --- plotting ---\n",
    "    ax = df_compare.plot(\n",
    "        kind='bar',\n",
    "        figsize=(10,6),\n",
    "        width=0.8\n",
    "    )\n",
    "\n",
    "    # Add values on top of bars\n",
    "    for p in ax.patches:\n",
    "        value = p.get_height()\n",
    "        if not pd.isna(value):\n",
    "            ax.annotate(\n",
    "                f\"{value:,.0f}\",               # format with commas, no decimals\n",
    "                (p.get_x() + p.get_width() / 2, value),\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=8, rotation=90, xytext=(0, 2), textcoords=\"offset points\"\n",
    "            )\n",
    "\n",
    "    plt.title(f\"Pollutant Sum Comparison for {emis_name}\")\n",
    "    plt.ylabel(\"Emissions (tons)\")\n",
    "    plt.xlabel(\"Pollutants\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title=\"Dataset\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Compare base and final for powerplants and non-powerplant emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "emis_dir_path = f'/Users/yunhalee/Documents/LOCAETA/Electrification/{overall_scenario}/'\n",
    "\n",
    "pollutant_cols = ['NOx','PM2_5','VOC','NH3','SOx']\n",
    "\n",
    "# storage for results\n",
    "compare_all = []\n",
    "\n",
    "# --- case 1: powerplants (loop over emis_list) ---\n",
    "for emis_name in scen_emis_list.values():\n",
    "    file_path1 = os.path.join(emis_dir_path, f'{emis_name}_pp_base.shp')\n",
    "    file_path2 = os.path.join(emis_dir_path, f'{emis_name}_pp.shp')  \n",
    "    df_name = f'powerplants: {emis_name}'\n",
    "\n",
    "    # read emission scenario\n",
    "    final_emis1 = gpd.read_file(file_path1) \n",
    "    final_emis2 = gpd.read_file(file_path2) \n",
    "\n",
    "    # Reset index\n",
    "    final_emis1.reset_index(drop=True, inplace=True)\n",
    "    final_emis2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # --- sum by pollutant ---\n",
    "    sum_final1 = final_emis1[pollutant_cols].sum()\n",
    "    sum_final2 = final_emis2[pollutant_cols].sum()\n",
    "\n",
    "    # --- store results ---\n",
    "    df_compare = pd.DataFrame({\n",
    "        f'{df_name}_base': sum_final1,\n",
    "        f'{df_name}_final': sum_final2,\n",
    "        'final-base': sum_final2 - sum_final1\n",
    "    })\n",
    "    df_compare['source'] = df_name\n",
    "    compare_all.append(df_compare)\n",
    "\n",
    "    # # --- plotting ---\n",
    "    # ax = df_compare.plot(\n",
    "    #     kind='bar',\n",
    "    #     figsize=(14, 6),\n",
    "    #     width=0.8\n",
    "    # )\n",
    "\n",
    "    # # Annotate values\n",
    "    # for p in ax.patches:\n",
    "    #     value = p.get_height()\n",
    "    #     if not pd.isna(value):\n",
    "    #         ax.annotate(\n",
    "    #             f\"{value:,.0f}\", \n",
    "    #             (p.get_x() + p.get_width() / 2, value),\n",
    "    #             ha='center', va='bottom',\n",
    "    #             fontsize=8, rotation=90, xytext=(0, 2), textcoords=\"offset points\"\n",
    "    #         )\n",
    "\n",
    "    # plt.title(f\"Emissions Comparison for {df_name} ({emis_name})\")\n",
    "    # plt.ylabel(\"Emissions (tons)\")\n",
    "    # plt.xlabel(\"Columns\")\n",
    "    # plt.xticks(rotation=90)\n",
    "    # plt.legend(title=\"Dataset\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --- case 2: non-powerplants (only once, outside emis_list loop) ---\n",
    "file_path1 = os.path.join(emis_dir_path, 'current_easyhard_base.shp') \n",
    "file_path2 = os.path.join(emis_dir_path, 'current_easyhard.shp')\n",
    "if overall_scenario != 'Full_USA':\n",
    "    file_path1 = os.path.join(emis_dir_path, f'current_easyhard_{overall_scenario}_base.shp') \n",
    "    file_path2 = os.path.join(emis_dir_path, f'current_easyhard_{overall_scenario}.shp')\n",
    "df_name = 'others_facilities: same in all runs'\n",
    "\n",
    "# read emission scenario\n",
    "final_emis1 = gpd.read_file(file_path1) \n",
    "final_emis2 = gpd.read_file(file_path2) \n",
    "\n",
    "# Reset index\n",
    "final_emis1.reset_index(drop=True, inplace=True)\n",
    "final_emis2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# --- sum by pollutant ---\n",
    "sum_final1 = final_emis1[pollutant_cols].sum()\n",
    "sum_final2 = final_emis2[pollutant_cols].sum()\n",
    "\n",
    "# --- store results ---\n",
    "df_compare = pd.DataFrame({\n",
    "    f'{df_name}_base': sum_final1,\n",
    "    f'{df_name}_final': sum_final2,\n",
    "    'final-base': sum_final2 - sum_final1\n",
    "})\n",
    "df_compare['source'] = df_name\n",
    "compare_all.append(df_compare)\n",
    "\n",
    "# --- combine into one DataFrame ---\n",
    "df_all = pd.concat(compare_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/'+list(scen_emis_list.values())[0]\n",
    "\n",
    "try:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Directory {output_dir} is created or already exists\")\n",
    "except OSError as e:\n",
    "    print(f\"Problem in creating directory: {e}\")\n",
    "\n",
    "\n",
    "final_diff_emis = df_all.pivot_table(index=df_all.index, columns=\"source\", values=\"final-base\")\n",
    "\n",
    "# Re-order the columns for the plotting\n",
    "new_cols = [ f'powerplants: {emis_name}' for emis_name in scen_emis_list.values()]\n",
    "new_cols.append('others_facilities: same in all runs')\n",
    "\n",
    "print(f\"checking plot bar legend names: {new_cols}\")\n",
    "\n",
    "final_diff_emis = final_diff_emis[new_cols]\n",
    "\n",
    "# --- plotting ---\n",
    "ax = final_diff_emis.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(10,6),\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# add values on top of bars\n",
    "for p in ax.patches:\n",
    "    value = p.get_height()\n",
    "    if not pd.isna(value):\n",
    "        ax.annotate(\n",
    "            f\"{value:,.0f}\",\n",
    "            (p.get_x() + p.get_width() / 2, value/2),\n",
    "            ha='center', va='bottom',\n",
    "            fontsize=10, rotation=90, xytext=(0, 2), textcoords=\"offset points\"\n",
    "        )\n",
    "\n",
    "plt.title(f\"Pollutant Emissions Change (Final - Base) for {overall_scenario}\")\n",
    "plt.ylabel(\"Emissions change (tons)\")\n",
    "plt.xlabel(\"Pollutants\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir+\"/Total_Difference.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## DON'T USE YET - Plot the current_2020_base emissions based on GEA regions\n",
    "\n",
    "Note that the current_2020_base has different emissions than NEI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "run_name = 'current_easyhard'\n",
    "Facilities_emis_file = f'{Scenario_dir_path}/{run_name}.shp'\n",
    "Facilities_emis_file_pp = f'{Scenario_dir_path}/{run_name}_pp.shp'\n",
    "\n",
    "final_output_dir = f'/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/{run_name}/'\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "# read both emission scenarios\n",
    "gdf_emis = gpd.read_file(Facilities_emis_file).reset_index(drop=True)\n",
    "gdf_emis_pp = gpd.read_file(Facilities_emis_file_pp).reset_index(drop=True)\n",
    "\n",
    "# combine\n",
    "gdf_emis_all = pd.concat([gdf_emis, gdf_emis_pp], ignore_index=True)\n",
    "gdf_emis_all = gpd.GeoDataFrame(gdf_emis_all, crs=gdf_emis.crs)\n",
    "\n",
    "print(\"Combined shape:\", gdf_emis_all.shape)\n",
    "gdf_emis_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "# emissions column names\n",
    "cs_pollutants = ['NOx','PM2_5', 'VOC','NH3', 'SOx']\n",
    "nei_pollutants = [f'{poll}_nei' for poll in cs_pollutants]\n",
    "\n",
    "# Loop through each pollutant\n",
    "for cs_col, nei_col in zip(cs_pollutants, nei_pollutants):\n",
    "    # grouping and summing emissions by each cambium_gea regions\n",
    "    grouped_sum = gdf_emis.groupby('cambium_ge')[[cs_col, nei_col]].sum().reset_index()\n",
    "\n",
    "    print(grouped_sum)\n",
    "\n",
    "    regions = grouped_sum['cambium_ge'].tolist()\n",
    "    x = np.arange(len(regions))\n",
    "    width = 0.35 # bar width\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (10, 6))\n",
    "    bars1 = ax.bar(x-width/2, grouped_sum[cs_col], width, label =f'{cs_col}_electrification')\n",
    "    bars2 = ax.bar(x+width/2, grouped_sum[nei_col], width, label = f'{nei_col}')\n",
    "\n",
    "    # Add value labels above bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:,.0f}',\n",
    "                        xy = (bar.get_x() + bar.get_width()/2, height/2),\n",
    "                        xytext = (0, 3),\n",
    "                        textcoords = 'offset points',\n",
    "                        ha = 'center',\n",
    "                        va = 'bottom', fontsize=9, rotation = 90,\n",
    "                        )\n",
    "    \n",
    "    ax.set_xlabel('Cambium_gea region')\n",
    "    ax.set_ylabel('Total Emissions [tons/yr]')\n",
    "    ax.set_title(f'Total {cs_col} Emissions by Cambium_gea Region')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(grouped_sum['cambium_ge'], rotation =45, ha='right')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(final_output_dir + f'Total_Difference_{cs_col}_by_regions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micromamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
