{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INMAP source-receptor matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import s3fs\n",
    "import zarr\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def rect(i, w, s, e, n):\n",
    "    x = [w[i], e[i], e[i], w[i], w[i]]\n",
    "    y = [s[i], s[i], n[i], n[i], s[i]]\n",
    "    return x, y\n",
    "\n",
    "def poly(sr, num_cells):\n",
    "    ret = []\n",
    "    w = sr[\"W\"][:]\n",
    "    s = sr[\"S\"][:]\n",
    "    e = sr[\"E\"][:]\n",
    "    n = sr[\"N\"][:]\n",
    "    for i in range(num_cells):\n",
    "        x, y = rect(i, w, s, e, n)\n",
    "        ret.append(Polygon([[x[0], y[0]], [x[1], y[1]], [x[2], y[2]],\n",
    "                            [x[3], y[3]], [x[4], y[4]]]))\n",
    "    return ret\n",
    "\n",
    "# Define the run_sr function\n",
    "def run_sr(emis, model, emis_units=\"tons/year\"):\n",
    "    start = time.time()\n",
    "    sr = zarr.open(\"/Users/yunhalee/Documents/LOCAETA/RCM/INMAP/source_receptor_model/isrm_v1.2.1.zarr\", mode=\"r\")\n",
    "\n",
    "    # Number of sources/receptors is fixed at 52411\n",
    "    num_cells = 52411\n",
    "\n",
    "    print(\"num_cells\", num_cells)\n",
    "\n",
    "    # Build the geometry\n",
    "    p = poly(sr, num_cells)\n",
    "    print(\"Making polygons as geometry.\")\n",
    "\n",
    "    # Create a GeoDataFrame for the grid\n",
    "    df = pd.DataFrame({'Location': range(num_cells)})\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=p)\n",
    "    gdf.crs = \"+proj=lcc +lat_1=33.000000 +lat_2=45.000000 +lat_0=40.000000 +lon_0=-97.000000 +x_0=0 +y_0=0 +a=6370997.000000 +b=6370997.000000 +to_meter=1\"\n",
    "    emis = emis.to_crs(gdf.crs)\n",
    "    print(\"emis after crs\", emis.head())\n",
    "\n",
    "    gdf = gdf.to_crs(gdf.crs)\n",
    "    print(gdf.head())\n",
    "\n",
    "\n",
    "    join_right_df = gdf.sjoin(emis, how=\"right\")\n",
    "    print(\"Finished joining the dataframes.\")\n",
    "    print(join_right_df.head())\n",
    "\n",
    "    print(\"join_right_df lenght:\", len(join_right_df))\n",
    "\n",
    "    # Filter out rows with NaNs in the Location column\n",
    "    join_right_df = join_right_df.dropna(subset=['Location'])\n",
    "    join_right_df['Location'] = join_right_df['Location'].astype(int)\n",
    "\n",
    "    index = join_right_df.Location.tolist()\n",
    "    ppl = np.unique(join_right_df.Location.tolist())\n",
    "    num = range(0, len(ppl))\n",
    "    dictionary = dict(zip(ppl, num))\n",
    "\n",
    "    print(\"index:\", index)\n",
    "    print(\"ppl:\", ppl)\n",
    "    print(\"num:\", num)\n",
    "\n",
    "    SOA = sr['SOA'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"SOA data is allocated.\")\n",
    "    pNO3 = sr['pNO3'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"pNO3 data is allocated.\")\n",
    "    pNH4 = sr['pNH4'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"pNH4 data is allocated.\")\n",
    "    pSO4 = sr['pSO4'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"pSO4 data is allocated.\")\n",
    "    PM25 = sr['PrimaryPM25'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"PrimaryPM25 data is allocated.\")\n",
    "\n",
    "    SOA_data, pNO3_data, pNH4_data, pSO4_data, PM25_data = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    for i in range(len(index)):\n",
    "        if index[i] in dictionary and i < num_emis_rows:\n",
    "            SOA_data += SOA[0, dictionary[index[i]], :] * emis.VOC.iloc[i]\n",
    "            pNO3_data += pNO3[0, dictionary[index[i]], :] * emis.NOx.iloc[i]\n",
    "            pNH4_data += pNH4[0, dictionary[index[i]], :] * emis.NH3.iloc[i]\n",
    "            pSO4_data += pSO4[0, dictionary[index[i]], :] * emis.SOx.iloc[i]\n",
    "            PM25_data += PM25[0, dictionary[index[i]], :] * emis.PM2_5.iloc[i]\n",
    "    data = SOA_data + pNO3_data + pNH4_data + pSO4_data + PM25_data\n",
    "\n",
    "    print(\"Accessing the data.\")\n",
    "    if emis_units == \"tons/year\":\n",
    "        fact = 28766.639\n",
    "\n",
    "    TotalPM25 = fact * data\n",
    "    TotalPop = sr['TotalPop'][0:num_cells]\n",
    "    MortalityRate = sr['MortalityRate'][0:num_cells]\n",
    "    deathsK = (np.exp(np.log(1.06)/10 * TotalPM25) - 1) * TotalPop * 1.0465819687408728 * MortalityRate / 100000 * 1.025229357798165\n",
    "    deathsL = (np.exp(np.log(1.14)/10 * TotalPM25) - 1) * TotalPop * 1.0465819687408728 * MortalityRate / 100000 * 1.025229357798165\n",
    "\n",
    "    ret = gpd.GeoDataFrame(pd.DataFrame({'SOA': fact * SOA_data,\n",
    "                                         'pNO3': fact * pNO3_data,\n",
    "                                         'pNH4': fact * pNH4_data,\n",
    "                                         'pSO4': fact * pSO4_data,\n",
    "                                         'PrimPM25': fact * PM25_data,\n",
    "                                         'TotalPM25': TotalPM25,\n",
    "                                         'deathsK': deathsK,\n",
    "                                         'deathsL': deathsL}), geometry=p[:num_cells])\n",
    "\n",
    "    print(\"Finished (%.0f seconds)\" % (time.time() - start))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read emission file\n",
    "file = '/Users/yunhalee/Documents/LOCAETA/CS_emissions/LA_point_CSS.shp'\n",
    "emis = gpd.read_file(file)\n",
    "print(emis[[\"VOC\", \"NOx\", \"NH3\", \"SOx\", \"PM2_5\"]].sum())\n",
    "\n",
    "num_emis_rows = emis.shape[0]\n",
    "print(f\"Number of rows in emission file: {num_emis_rows}\")\n",
    "print(emis.head())\n",
    "\n",
    "# This step might take a while. (20-25 mins in the author's computer)\n",
    "# If this step is interrupted due to insufficient memory, you can try virtual memory.\n",
    "resultsISRM = run_sr(emis, model=\"isrm\", emis_units=\"tons/year\")\n",
    "\n",
    "resultsISRM.head()\n",
    "\n",
    "deaths = pd.DataFrame.from_dict({\n",
    "    \"Model\": [\"ISRM\"],\n",
    "    \"Krewski Deaths\": [resultsISRM.deathsK.sum()],\n",
    "    \"LePeule Deaths\": [resultsISRM.deathsL.sum()],\n",
    "})\n",
    "\n",
    "print(deaths)\n",
    "\n",
    "\n",
    "resultsISRM.to_file(\"/Users/yunhalee/Documents/LOCAETA/RCM/INMAP/source_receptor_model/outputs/ISRM_LA_CSS.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read emission file\n",
    "file = '/Users/yunhalee/Documents/LOCAETA/CS_emissions/LA_point_CSS_reduced_emis.shp'\n",
    "emis2 = gpd.read_file(file)\n",
    "gdf_crs = \"+proj=lcc +lat_1=33.000000 +lat_2=45.000000 +lat_0=40.000000 +lon_0=-97.000000 +x_0=0 +y_0=0 +a=6370997.000000 +b=6370997.000000 +to_meter=1\"\n",
    "emis = emis.to_crs(gdf_crs)\n",
    "print(emis2[[\"VOC\", \"NOx\", \"NH3\", \"SOx\", \"PM2_5\"]].sum())\n",
    "\n",
    "num_emis_rows = emis2.shape[0]\n",
    "print(f\"Number of rows in emission file: {num_emis_rows}\")\n",
    "print(emis2.head())\n",
    "\n",
    "# This step might take a while. (20-25 mins in the author's computer)\n",
    "# If this step is interrupted due to insufficient memory, you can try virtual memory.\n",
    "resultsISRM_red = run_sr(emis2, model=\"isrm\", emis_units=\"tons/year\")\n",
    "\n",
    "resultsISRM_red.head()\n",
    "\n",
    "deaths_red = pd.DataFrame.from_dict({\n",
    "    \"Model\": [\"apsca_q0\"],\n",
    "    \"Krewski Deaths\": [resultsISRM_red.deathsK.sum()],\n",
    "    \"LePeule Deaths\": [resultsISRM_red.deathsL.sum()],\n",
    "})\n",
    "\n",
    "deaths_red\n",
    "\n",
    "resultsISRM_red.to_file(\"/Users/yunhalee/Documents/LOCAETA/RCM/INMAP/source_receptor_model/outputs/ISRM_LA_CSS_reduced_emis.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = resultsISRM_red.TotalPM25.quantile(0.985)\n",
    "resultsISRM_red.plot(column=resultsISRM_red.TotalPM25, cmap=\"GnBu\", legend=True, \n",
    "                 vmin=0, vmax=cut, figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl = 9.0e6\n",
    "\n",
    "pd.DataFrame.from_dict({\n",
    "    \"Model\": [\"ISRM\"],\n",
    "    \"Krewski Damages\": deaths[\"Krewski Deaths\"] * vsl,\n",
    "    \"LePeule Damages\": deaths[\"LePeule Deaths\"] * vsl,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = resultsISRM.TotalPM25.quantile(0.985)\n",
    "resultsISRM.plot(column=resultsISRM.TotalPM25, cmap=\"GnBu\", legend=True, \n",
    "                 vmin=0, vmax=cut, figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = resultsISRM_red.TotalPM25.quantile(0.985)\n",
    "resultsISRM_red.plot(column=resultsISRM_red.TotalPM25, cmap=\"GnBu\", legend=True, \n",
    "                 vmin=0, vmax=cut, figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsISRM.to_file(\"ISRM_LA_CSS.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
