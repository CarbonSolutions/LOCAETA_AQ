{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Debugging NEI emissions (for duplicates)\n",
    "\n",
    "Date : May 6, 2025 \n",
    "\n",
    "Whole USA scenario, EIS_ID and SCC no longer a unquie identifier, so there are some duplicates, resulting in incorrect final NEI-CCS emissions. The script below is checking what causes the duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# LA_CCS and CO_CCS don't have the duplicates case for EIS_ID and SCC\n",
    "\n",
    "# read base and sens emission scenarios\n",
    "gdf_debug = gpd.read_file(\n",
    "#  '/Users/yunhalee/Documents/LOCAETA/RCM/INMAP/evaldata_v1.6.1/2020_nei_emissions/combined_NEI2020_pt_oilgas_ptegu_ptnonipm.shp')\n",
    " '/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA_point_CCS.shp')\n",
    "\n",
    "debug_output_dir = '/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/emissions/USA_CCS/'\n",
    "\n",
    "# Reset index to ensure proper comparison\n",
    "gdf_debug.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#print(gdf_debug[(gdf_debug['EIS_ID'].astype(int) == 715711) & (gdf_debug['SCC'].astype(int) == 10100601)]) \n",
    "#print(gdf_debug[(gdf_debug['EIS_ID'].astype(int) == 12611) & (gdf_debug['SCC'].astype(int) == 10200602)])  \n",
    "print(gdf_debug[(gdf_debug['EIS_ID'].astype(int) == 15662811)])\n",
    "\n",
    "print(gdf_debug[(gdf_debug['EIS_ID'].astype(int) == 3982711)])    \n",
    "\n",
    "# ptnonipm_2 has PM2_5 0.99218\n",
    "# ptegu_1 has PM2_5 9.899564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates\n",
    "duplicate_keys = (\n",
    "    gdf_debug.groupby(['EIS_ID', 'SCC'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .query('count > 1')[['EIS_ID', 'SCC']]\n",
    ")\n",
    "duplicates = gdf_debug.merge(duplicate_keys, on=['EIS_ID', 'SCC'], how='inner')\n",
    "duplicates['row_key'] = duplicates.index  # Track original index\n",
    "\n",
    "# Case 1: Same EIS_ID, SCC, ghgrp_faci but different PM2_5\n",
    "case1_keys = (\n",
    "    duplicates.groupby(['EIS_ID', 'SCC', 'ghgrp_faci'])['PM2_5']\n",
    "    .nunique()\n",
    "    .reset_index(name='pm25_variety')\n",
    "    .query('pm25_variety > 1')[['EIS_ID', 'SCC', 'ghgrp_faci']]\n",
    ")\n",
    "case1 = duplicates.merge(case1_keys, on=['EIS_ID', 'SCC', 'ghgrp_faci'])\n",
    "case1_row_keys = set(case1['row_key'])\n",
    "\n",
    "# Exclude Case 1 rows before doing Case 2\n",
    "case_others = duplicates[~duplicates['row_key'].isin(case1_row_keys)]\n",
    "\n",
    "# Case 2: Multiple ghgrp_faci for the same NEI (EIS_ID + SCC)\n",
    "case2_keys = (\n",
    "    case_others.groupby(['EIS_ID', 'SCC'])['ghgrp_faci']\n",
    "    .nunique()\n",
    "    .reset_index(name='ghgrp_faci_count')\n",
    "    .query('ghgrp_faci_count > 1')[['EIS_ID', 'SCC']]\n",
    ")\n",
    "case2 = case_others.merge(case2_keys, on=['EIS_ID', 'SCC'])\n",
    "case2_row_keys = set(case2['row_key'])\n",
    "\n",
    "# Case 3: Remaining (All IDs are same but two subparts, C and D, results in different NH3/VOC increase)\n",
    "remaining_row_keys = set(duplicates['row_key']) - case1_row_keys - case2_row_keys\n",
    "case3 = duplicates[duplicates['row_key'].isin(remaining_row_keys)]\n",
    "\n",
    "# Output\n",
    "print(f\"Total Duplicates: {len(duplicates)} rows\")\n",
    "print(f\"Case 1: {len(case1)} rows\")\n",
    "print(f\"Case 2: {len(case2)} rows\")\n",
    "print(f\"Case 3: {len(case3)} rows\")\n",
    "print(f\"Sum of all cases: {len(case1) + len(case2) + len(case3)} rows\")\n",
    "\n",
    "#sort data based on EIS_ID and SCC\n",
    "case1 = case1.sort_values(by=['EIS_ID', 'SCC'], ascending=[True, True])\n",
    "case2 = case2.sort_values(by=['EIS_ID', 'SCC'], ascending=[True, True])\n",
    "case3 = case3.sort_values(by=['EIS_ID', 'SCC'], ascending=[True, True])\n",
    "\n",
    "case1.to_csv(debug_output_dir + 'Case1_one_ghgrp_fac_multiple_NEI_duplicates.csv', index=False)\n",
    "case2.to_csv(debug_output_dir + 'Case2_more_than_one_ghgrp_faci_per_NEI_duplicates.csv', index=False)\n",
    "case3.to_csv(debug_output_dir + 'Case3_other_duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def solve_case1_duplicates(case1_df):\n",
    "    \"\"\"\n",
    "    Case 1 Solution: Split CCS emissions proportionally based on NEI emissions weights\n",
    "\n",
    "    For the same EIS_ID, SCC, and ghgrp_faci, NEI-SMOKE processed emissions are split into multiple sources.\n",
    "    Each species uses its corresponding original emission for weighting.\n",
    "    \"\"\"\n",
    "    print(\"Solving Case 1 duplicates...\")\n",
    "    \n",
    "    # Define original emissions columns (NEI-SMOKE processed)\n",
    "    original_emission_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "    \n",
    "    # Define new emissions columns that need to be split\n",
    "    new_emission_cols = ['VOC_out_su', 'NOX_out_su', 'NH3_out_su', 'SO2_out_su', 'PM25_out_s']\n",
    "    \n",
    "    # Mapping between original and new emission columns\n",
    "    emission_mapping = {\n",
    "        'VOC_out_su': 'VOC',\n",
    "        'NOX_out_su': 'NOx',\n",
    "        'NH3_out_su': 'NH3',\n",
    "        'SO2_out_su': 'SOx',\n",
    "        'PM25_out_s': 'PM2_5'\n",
    "    }\n",
    "    \n",
    "    solved_rows = []\n",
    "    \n",
    "    # Group by EIS_ID, SCC, ghgrp_faci\n",
    "    for group_key, group_df in case1_df.groupby(['EIS_ID', 'SCC', 'ghgrp_faci']):\n",
    "        if len(group_df) > 1:\n",
    "            for idx, row in group_df.iterrows():\n",
    "                # Create new row\n",
    "                new_row = row.copy()\n",
    "                \n",
    "                # Apply species-specific weighting to new emissions columns\n",
    "                for new_col in new_emission_cols:\n",
    "                    if new_col in new_row.index and pd.notna(new_row[new_col]):\n",
    "                        # Get corresponding original emission column\n",
    "                        orig_col = emission_mapping[new_col]\n",
    "                        \n",
    "                        # Calculate weight based on this species' original emission\n",
    "                        total_orig_emission = group_df[orig_col].sum()\n",
    "                        if total_orig_emission > 0:\n",
    "                            weight = row[orig_col] / total_orig_emission\n",
    "                        else:\n",
    "                            weight = 1 / len(group_df)  # Equal split if no original emission\n",
    "                        \n",
    "                        # Apply weighting\n",
    "                        new_row[new_col] = new_row[new_col] * weight\n",
    "                \n",
    "                # Keep original emissions unchanged\n",
    "                solved_rows.append(new_row)\n",
    "        else:\n",
    "            # Single row, no splitting needed\n",
    "            solved_rows.append(group_df.iloc[0])\n",
    "    \n",
    "    return pd.DataFrame(solved_rows)\n",
    "\n",
    "\n",
    "def solve_case2_duplicates(case2_df):\n",
    "    \"\"\"\n",
    "    Case 2 Solution: Sum NH3 and VOC increase emissions across multiple ghgrp facilities\n",
    "    \n",
    "    For the same EIS_ID and SCC, multiple ghgrp facilities may be linked.\n",
    "    Verify that non-summed new emissions match their original counterparts.\n",
    "    \"\"\"\n",
    "    print(\"Solving Case 2 duplicates...\")\n",
    "    \n",
    "    # Define original emissions columns\n",
    "    original_emission_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "    \n",
    "    # Define new emissions columns\n",
    "    new_emission_cols = ['VOC_out_su', 'NOX_out_su', 'NH3_out_su', 'SO2_out_su', 'PM25_out_s']\n",
    "    \n",
    "    # Define which columns to sum (NH3 and VOC increase emissions)\n",
    "    sum_cols = ['NH3_out_su', 'VOC_out_su']\n",
    "    \n",
    "    # Define columns that shouldn't be summed (other species)\n",
    "    no_sum_cols = ['NOX_out_su', 'SO2_out_su', 'PM25_out_s']\n",
    "    \n",
    "    # Mapping for verification\n",
    "    verification_mapping = {\n",
    "        'NOX_out_su': 'NOx',\n",
    "        'SO2_out_su': 'SOx',\n",
    "        'PM25_out_s': 'PM2_5'\n",
    "    }\n",
    "    \n",
    "    solved_rows = []\n",
    "    \n",
    "    # Group by EIS_ID and SCC\n",
    "    for group_key, group_df in case2_df.groupby(['EIS_ID', 'SCC']):\n",
    "        if len(group_df) > 1:\n",
    "            # Verify that no_sum_cols values are consistent with original emissions\n",
    "            for new_col in no_sum_cols:\n",
    "                if new_col in group_df.columns and new_col in verification_mapping:\n",
    "                    orig_col = verification_mapping[new_col]\n",
    "                    if orig_col in group_df.columns:\n",
    "                        # Check if all values in the group are the same for both columns\n",
    "                        new_values = group_df[new_col].dropna().unique()\n",
    "                        orig_values = group_df[orig_col].dropna().unique()\n",
    "                        \n",
    "                        if len(new_values) > 1 or len(orig_values) > 1:\n",
    "                            print(f\"Warning: Inconsistent values in group {group_key} for {new_col}/{orig_col}\")\n",
    "                            print(f\"  {new_col} values: {new_values}\")\n",
    "                            print(f\"  {orig_col} values: {orig_values}\")\n",
    "            \n",
    "            # Create a consolidated row\n",
    "            consolidated_row = group_df.iloc[0].copy()  # Start with first row\n",
    "            \n",
    "            # Sum NH3 and VOC increase emissions\n",
    "            for col in sum_cols:\n",
    "                if col in group_df.columns:\n",
    "                    consolidated_row[col] = group_df[col].sum()\n",
    "            \n",
    "            # For other species, keep the first value (shouldn't be summed)\n",
    "            for col in no_sum_cols:\n",
    "                if col in group_df.columns:\n",
    "                    consolidated_row[col] = group_df[col].iloc[0]\n",
    "            \n",
    "            # Concatenate ghgrp_faci IDs to show which facilities were combined\n",
    "            consolidated_row['ghgrp_faci'] = ';'.join(group_df['ghgrp_faci'].astype(str).unique())\n",
    "            \n",
    "            solved_rows.append(consolidated_row)\n",
    "        else:\n",
    "            # Single row, no consolidation needed\n",
    "            solved_rows.append(group_df.iloc[0])\n",
    "    \n",
    "    return pd.DataFrame(solved_rows)\n",
    "\n",
    "\n",
    "def solve_case3_duplicates(case3_df):\n",
    "    \"\"\"\n",
    "    Case 3 Solution: Sum NH3 and VOC increase emissions from both subparts\n",
    "    Similar to Case 1, this duplicate comes from having two subparts (C and D), \n",
    "    which result in different NH3 and VOC emissions increase.\n",
    "    Verify that non-summed new emissions match their original counterparts.\n",
    "    \"\"\"\n",
    "    print(\"Solving Case 3 duplicates...\")\n",
    "    \n",
    "    # Define original emissions columns\n",
    "    original_emission_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "    \n",
    "    # Define new emissions columns\n",
    "    new_emission_cols = ['VOC_out_su', 'NOX_out_su', 'NH3_out_su', 'SO2_out_su', 'PM25_out_s']\n",
    "    \n",
    "    # Define which columns to sum (NH3 and VOC increase emissions)\n",
    "    sum_cols = ['NH3_out_su', 'VOC_out_su']\n",
    "    \n",
    "    # Define columns that shouldn't be summed (other species)\n",
    "    no_sum_cols = ['NOX_out_su', 'SO2_out_su', 'PM25_out_s']\n",
    "    \n",
    "    # Mapping for verification\n",
    "    verification_mapping = {\n",
    "        'NOX_out_su': 'NOx',\n",
    "        'SO2_out_su': 'SOx',\n",
    "        'PM25_out_s': 'PM2_5'\n",
    "    }\n",
    "    \n",
    "    solved_rows = []\n",
    "    \n",
    "    # Group by EIS_ID and SCC (assuming subparts are differentiated within these groups)\n",
    "    for group_key, group_df in case3_df.groupby(['EIS_ID', 'SCC']):\n",
    "        if len(group_df) > 1:\n",
    "            # Verify that no_sum_cols values are consistent with original emissions\n",
    "            for new_col in no_sum_cols:\n",
    "                if new_col in group_df.columns and new_col in verification_mapping:\n",
    "                    orig_col = verification_mapping[new_col]\n",
    "                    if orig_col in group_df.columns:\n",
    "                        # Check if all values in the group are the same for both columns\n",
    "                        new_values = group_df[new_col].dropna().unique()\n",
    "                        orig_values = group_df[orig_col].dropna().unique()\n",
    "                        \n",
    "                        if len(new_values) > 1 or len(orig_values) > 1:\n",
    "                            print(f\"Warning: Inconsistent values in group {group_key} for {new_col}/{orig_col}\")\n",
    "                            print(f\"  {new_col} values: {new_values}\")\n",
    "                            print(f\"  {orig_col} values: {orig_values}\")\n",
    "            \n",
    "            # Create a consolidated row\n",
    "            consolidated_row = group_df.iloc[0].copy()  # Start with first row\n",
    "            \n",
    "            # Sum NH3 and VOC increase emissions\n",
    "            for col in sum_cols:\n",
    "                if col in group_df.columns:\n",
    "                    consolidated_row[col] = group_df[col].sum()\n",
    "            \n",
    "            # For other species, keep the first value (shouldn't be summed)\n",
    "            for col in no_sum_cols:\n",
    "                if col in group_df.columns:\n",
    "                    consolidated_row[col] = group_df[col].iloc[0]\n",
    "            \n",
    "            # Add a note about subparts being combined if subpart column exists\n",
    "            if 'subpart' in group_df.columns:\n",
    "                consolidated_row['subpart'] = ';'.join(group_df['subpart'].astype(str).unique())\n",
    "            \n",
    "            solved_rows.append(consolidated_row)\n",
    "        else:\n",
    "            # Single row, no consolidation needed\n",
    "            solved_rows.append(group_df.iloc[0])\n",
    "    \n",
    "    return pd.DataFrame(solved_rows)\n",
    "\n",
    "\n",
    "def apply_duplicate_solutions(gdf_debug, debug_output_dir):\n",
    "    \"\"\"\n",
    "    Main function to apply all duplicate solutions and update the original dataframe\n",
    "    \"\"\"\n",
    "    # Get the row keys that need to be removed from original dataframe\n",
    "    all_duplicate_row_keys = case1_row_keys.union(case2_row_keys).union(remaining_row_keys)\n",
    "    \n",
    "    # Remove duplicates from original dataframe\n",
    "    gdf_clean = gdf_debug[~gdf_debug.index.isin(all_duplicate_row_keys)].copy()\n",
    "    \n",
    "    # Solve each case\n",
    "    case1_solved = solve_case1_duplicates(case1)\n",
    "    case2_solved = solve_case2_duplicates(case2)\n",
    "    case3_solved = solve_case3_duplicates(case3)\n",
    "    \n",
    "    # Remove row_key column if it exists (used for tracking)\n",
    "    for df in [case1_solved, case2_solved, case3_solved]:\n",
    "        if 'row_key' in df.columns:\n",
    "            df.drop('row_key', axis=1, inplace=True)\n",
    "    \n",
    "    # Combine solved cases back to clean dataframe\n",
    "    gdf_final = pd.concat([gdf_clean, case1_solved, case2_solved, case3_solved], \n",
    "                         ignore_index=True)\n",
    "    \n",
    "    # Save solved cases for verification\n",
    "    case1_solved.to_csv(debug_output_dir + 'Case1_solved.csv', index=False)\n",
    "    case2_solved.to_csv(debug_output_dir + 'Case2_solved.csv', index=False)\n",
    "    case3_solved.to_csv(debug_output_dir + 'Case3_solved.csv', index=False)\n",
    "    \n",
    "    print(f\"Original dataframe: {len(gdf_debug)} rows\")\n",
    "    print(f\"After removing duplicates: {len(gdf_clean)} rows\")\n",
    "    print(f\"Case 1 solved: {len(case1_solved)} rows\")\n",
    "    print(f\"Case 2 solved: {len(case2_solved)} rows\")\n",
    "    print(f\"Case 3 solved: {len(case3_solved)} rows\")\n",
    "    print(f\"Final dataframe: {len(gdf_final)} rows\")\n",
    "    \n",
    "    return gdf_final\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the solutions\n",
    "gdf_final = apply_duplicate_solutions(gdf_debug, debug_output_dir)\n",
    "\n",
    "# Verify no duplicates remain\n",
    "final_duplicates = (\n",
    "    gdf_final.groupby(['EIS_ID', 'SCC'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .query('count > 1')\n",
    ")\n",
    "\n",
    "print(f\"Remaining duplicates after solution: {len(final_duplicates)} groups\")\n",
    "\n",
    "# Save final cleaned dataframe\n",
    "gdf_final.to_csv(debug_output_dir + 'gdf_final_no_duplicates.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
