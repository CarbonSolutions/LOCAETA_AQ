{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Re-process USA CCS data from Kelly \n",
    "\n",
    "Date: July 17, 2025 (Completed: Aug 1, 2025)\n",
    "\n",
    "Author: Yunha Lee\n",
    "\n",
    "\n",
    "### Remove the following rows:\n",
    " - epa_subpart = -1 : this is the case for a facility is NOT in CO2NCORD but IS in GHGRP. \n",
    " - scc = NaN: this is the case for subparts that are not estimated to be stationary combustion \n",
    "\n",
    "### This script will sume NH3 and VOC emissions for multiple ghgrp facilities and/or two subparts C & D. With the file created from this script, the case 2 and 3 duplicates should be removed.\n",
    "\n",
    "==========\n",
    "\n",
    "Kelly's USA CCS data has three types of duplicates when sorting by EIS_ID and SCC:\n",
    "\n",
    "Case 1. multiple sources (most commonly two sources; for example, ptnonipm_2 and ptegu_1)\n",
    "==>  I plan to split the final CCS emissions into two or more parts, weight by the corresponding NEI emissions.\n",
    " \n",
    "Case 2. multiple ghgrp facilities are linked (usually 2), which has different NH3 and VOC increase emissions; all other species has identical values.\n",
    "==>  I plan to sum the NH3 and VOC increase emissions across the two (or more) ghgrp facilities.\n",
    " \n",
    "Case 3. Two subparts (C and D), which result in different NH3 and VOC emissions increase (the other emissions are identical).  \n",
    "==>  I plan to sum the NH3 and VOC increase emissions from both subparts.\n",
    "\n",
    "==========\n",
    "\n",
    "Kelly's USA CCS data contains 111 rows that doesn't match back to the NEI base 2020 emissions. For that case, I only put back the CCS's VOC and NH3 emission increase only and set all other emissions, including NEI emissions, to be zero. This way, the NEI emissions in the new CCS emission file matches back to the original NEI emissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CCS_raw_file = '/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA-compiled-cobenefits-emissions-withSCC-v4.csv'\n",
    "\n",
    "# This function processes Amy's old CS emission data file.\n",
    "cs_emis = pd.read_csv(CCS_raw_file, index_col=False)\n",
    "\n",
    "cs_emis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the rows with missing SCC or epa_subpart = -1\n",
    "\n",
    "print(\"cs_emis original shape: \", cs_emis.shape)\n",
    "indices_to_drop = cs_emis[cs_emis['epa_subpart'] == '-1'].index\n",
    "cs_emis.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "print(\"after dropping epa_subpart = -1, cs_emis shape: \", cs_emis.shape)\n",
    "\n",
    "indices_to_drop = cs_emis[cs_emis['scc'].isna()].index\n",
    "cs_emis.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "print(\"after scc with NaN, cs_emis shape: \", cs_emis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all duplicate rows, including the first occurrence\n",
    "all_duplicates = cs_emis[cs_emis.duplicated(keep=False)]\n",
    "print(\"All duplicate rows:\")\n",
    "print(all_duplicates)\n",
    "\n",
    "# Identify duplicates\n",
    "duplicate_keys = (\n",
    "    cs_emis.groupby(['eis_id', 'scc'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .query('count > 1')[['eis_id', 'scc']]\n",
    ")\n",
    "duplicates = cs_emis.merge(duplicate_keys, on=['eis_id', 'scc'], how='inner')\n",
    "duplicates['row_key'] = duplicates.index  # Track original index\n",
    "\n",
    "print(duplicates.shape, cs_emis.shape)\n",
    "\n",
    "# Case 1: Multiple ghgrp_faci for the same NEI (EIS_ID + SCC)\n",
    "case1_keys = (\n",
    "    duplicates.groupby(['eis_id', 'scc'])['ghgrp_facility_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='ghgrp_faci_count')\n",
    "    .query('ghgrp_faci_count > 1')[['eis_id', 'scc']]\n",
    ")\n",
    "case1 = duplicates.merge(case1_keys, on=['eis_id', 'scc'])\n",
    "case1_row_keys = set(case1['row_key'])\n",
    "\n",
    "\n",
    "# Exclude Case 1 rows before doing Case 2\n",
    "case_others = duplicates[~duplicates['row_key'].isin(case1_row_keys)]\n",
    "\n",
    "# Case 2: Multiple 'epa_subpart' for the same NEI (EIS_ID + SCC)\n",
    "case2_keys = (\n",
    "    case_others.groupby(['eis_id', 'scc'])['epa_subpart']\n",
    "    .nunique()\n",
    "    .reset_index(name='epa_subpart_count')\n",
    "    .query('epa_subpart_count > 1')[['eis_id', 'scc']]\n",
    ")\n",
    "case2 = case_others.merge(case2_keys, on=['eis_id', 'scc'])\n",
    "case2_row_keys = set(case2['row_key'])\n",
    "\n",
    "print(case1.shape, case2.shape)\n",
    "duplicates.head()\n",
    "\n",
    "combined_case_row_keys = case1_row_keys.union(case2_row_keys)\n",
    "print(\"Total unique rows in Case 1 or Case 2:\", len(combined_case_row_keys))\n",
    "\n",
    "unexplained = duplicates[~duplicates['row_key'].isin(combined_case_row_keys)]\n",
    "print(\"Unexplained duplicates:\", unexplained.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define species names\n",
    "species_to_sum = ['VOC', 'NH3']\n",
    "all_species = ['VOC', 'NH3', 'NOX', 'SO2', 'PM25']\n",
    "\n",
    "# Manually define sum_cols (from image)\n",
    "sum_cols = [ 'NH3_increase_SCC_tons','VOC_increase_SCC_tons']\n",
    "\n",
    "# Dynamically identify all species-related columns, excluding those in sum_cols\n",
    "CCS_cols = [\n",
    "    col for col in cs_emis.columns\n",
    "    if any(sp in col for sp in all_species) and col not in sum_cols\n",
    "]\n",
    "\n",
    "# Step 1: Identify duplicate (eis_id, scc) pairs\n",
    "dup_keys = (\n",
    "    cs_emis.groupby(['eis_id', 'scc'])\n",
    "    .size().reset_index(name='count')\n",
    "    .query('count > 1')[['eis_id', 'scc']]\n",
    ")\n",
    "\n",
    "# Step 2: Split duplicated and non-duplicated rows\n",
    "cs_emis['row_key'] = cs_emis.index\n",
    "duplicated_rows = cs_emis.merge(dup_keys, on=['eis_id', 'scc'], how='inner')\n",
    "non_duplicated_rows = cs_emis[~cs_emis['row_key'].isin(duplicated_rows['row_key'])]\n",
    "\n",
    "# Step 3: Deduplicate only needed rows\n",
    "grouped = duplicated_rows.groupby(['eis_id', 'scc'])\n",
    "\n",
    "import numpy as np\n",
    "def values_consistent(group, col):\n",
    "    vals = group[col].dropna().unique()\n",
    "    return len(vals) <= 1\n",
    "\n",
    "exclude_consistency_check = sum_cols + ['NH3_increase_subpart_tons', 'VOC_increase_subpart_tons', 'frac_of_VOC_subpart']\n",
    "\n",
    "# Compare only the columns where values differ between the two rows\n",
    "def get_diff_columns(group):\n",
    "    if group.shape[0] != 2:\n",
    "        raise ValueError(\"Expected group with exactly 2 rows\")\n",
    "    \n",
    "    # Compare the two rows\n",
    "    diffs = (group.iloc[0] != group.iloc[1]) & ~(group.iloc[0].isna() & group.iloc[1].isna())\n",
    "    \n",
    "    # Return only the differing columns\n",
    "    return group.loc[:, diffs]\n",
    "\n",
    "dedup_list = []\n",
    "for key, group in grouped:\n",
    "    ref = group.iloc[0]\n",
    "    consistent = all(values_consistent(group, col) for col in CCS_cols if col not in exclude_consistency_check)\n",
    "    if not consistent:\n",
    "        print(f\"❌ Inconsistent values for group: {key}\")\n",
    "        print(get_diff_columns(group[CCS_cols]))\n",
    "        raise ValueError(f\"Inconsistent species values in group: {key}\")\n",
    "    \n",
    "    summed_row = ref.copy()\n",
    "    for col in sum_cols:\n",
    "        summed_row[col] = group[col].sum()\n",
    "    dedup_list.append(summed_row)\n",
    "\n",
    "dedup_df = pd.DataFrame(dedup_list)\n",
    "\n",
    "# Step 4: Final dataframe\n",
    "cs_emis_final = pd.concat([non_duplicated_rows, dedup_df], ignore_index=True)\n",
    "cs_emis_final.drop(columns='row_key', inplace=True)\n",
    "\n",
    "# Step 5: Sanity check\n",
    "print(\"\\n==== 🔍 CCS_cols Sum Check ====\")\n",
    "before = cs_emis[CCS_cols].sum()\n",
    "after = cs_emis_final[CCS_cols].sum()\n",
    "diff = after - before\n",
    "\n",
    "check_df = pd.DataFrame({\n",
    "    'Before': before,\n",
    "    'After': after,\n",
    "    'Diff': diff\n",
    "})\n",
    "print(check_df)\n",
    "\n",
    "# Check row counts\n",
    "print(f\"\\nOriginal rows: {cs_emis.shape[0]}, Final rows: {cs_emis_final.shape[0]}\")\n",
    "print(f\"Duplicates handled: {duplicated_rows.shape[0] - dedup_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cs_emis.shape, cs_emis_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To verify the sum of NOx, SO2 and PM2.5, the sum from the \"non_duplicated_rows\" and the sum from the first of \"duplicated_rows\" should be compared to the sum to the cs_emis_final. \n",
    "\n",
    "# Step 1: Sum from non-duplicated rows\n",
    "non_dup_sum = non_duplicated_rows[CCS_cols].sum()\n",
    "\n",
    "# Step 2: For duplicated groups, get the first row only\n",
    "first_of_duplicates = duplicated_rows.groupby(['eis_id', 'scc'], as_index=False).first()\n",
    "first_dup_sum = first_of_duplicates[CCS_cols].sum()\n",
    "\n",
    "# Step 3: Compare to full final deduplicated dataframe\n",
    "final_sum = cs_emis_final[CCS_cols].sum()\n",
    "\n",
    "# Step 4: Combine and compare\n",
    "verify_df = pd.DataFrame({\n",
    "    'Non-Duplicated': non_dup_sum,\n",
    "    'First of Duplicates': first_dup_sum,\n",
    "    'Reconstructed (NonDup + FirstDup)': non_dup_sum + first_dup_sum,\n",
    "    'Final Deduplicated': final_sum,\n",
    "    'Diff': final_sum - (non_dup_sum + first_dup_sum)\n",
    "})\n",
    "\n",
    "print(\"\\n=== 🔍 Emission Verification ===\")\n",
    "print(verify_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCS_missing_species = ['VOC', 'NH3']\n",
    "\n",
    "for missing in CCS_missing_species:\n",
    "    # sometimes, kelley's output missing these species output\n",
    "    missing_name = missing + \"_out_subpart_tons\"\n",
    "    if missing_name not in cs_emis.columns:\n",
    "        print(\"computing missing species output :\", missing_name)\n",
    "       \n",
    "        cs_emis[missing_name] = cs_emis[missing + '_subpart_tons'].fillna(0)  + cs_emis[missing + '_increase_SCC_tons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns for my emission processing\n",
    "columns_to_drop = [col for col in cs_emis_final.columns if \"CO2\" in col]\n",
    "columns_to_drop = columns_to_drop + [col for col in cs_emis_final.columns if \"cost_\" in col]\n",
    "\n",
    "cs_emis_final.drop(columns=columns_to_drop, inplace=True)\n",
    "print(cs_emis_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_emis_final.to_csv(\"/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA-compiled-cobenefits-emissions-withSCC-v4_ghgrp_subpart_sum.csv\", index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some rows missing scc\n",
    "missing_scc = cs_emis_final[cs_emis_final['epa_subpart'] == \"-1\"]\n",
    "\n",
    "missing_scc_sum = missing_scc[CCS_cols].sum()\n",
    "print(missing_scc_sum)\n",
    "\n",
    "print(cs_emis_final[CCS_cols].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Apply CCS emission to NEI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 1: Read NEI and CCS original data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the path to the main package directory\n",
    "package_path = os.path.abspath('/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/LOCAETA_AQ')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "import emission_processing\n",
    "\n",
    "# output file path for processed emissions \n",
    "NEI_CCS_emis_file = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA_CCS_combined_NEI_point_oilgas_ptegu_ptnonimps.shp\"\n",
    "State_emis_file = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA_point_CCS.shp\"\n",
    "State_CCS_emis_file = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA_point_CCS_reduced_emis.shp\"\n",
    "output_plots_dir ='/Users/yunhalee/Documents/LOCAETA/LOCAETA_AQ/outputs/'\n",
    "\n",
    "# CCS and NEI raw data directory\n",
    "nei_pt_path = '/Users/yunhalee/Documents/LOCAETA/RCM/INMAP/evaldata_v1.6.1/2020_nei_emissions/'\n",
    "combined_NEI_emis_path = nei_pt_path +'combined_NEI2020_pt_oilgas_ptegu_ptnonipm_w_sectors.shp'\n",
    "\n",
    "target_crs = \"+proj=lcc +lat_1=33.000000 +lat_2=45.000000 +lat_0=40.000000 +lon_0=-97.000000 +x_0=0 +y_0=0 +a=6370997.000000 +b=6370997.000000 +to_meter=1\"\n",
    "\n",
    "gdf = gpd.read_file(combined_NEI_emis_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCS_raw_file = '/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA-compiled-cobenefits-emissions-withSCC-v4_ghgrp_subpart_sum.csv' #'/Users/yunhalee/Documents/LOCAETA/CS_emissions/final_output_1_manual_update_noLandfill.csv'\n",
    "\n",
    "# process CCS emissions to merge it with NEI emissions\n",
    "cs_emis = pd.read_csv(CCS_raw_file)\n",
    "\n",
    "# ensure scc column to be integer\n",
    "cs_emis['scc'] = cs_emis['scc'].astype(int)\n",
    "\n",
    "cs_emis.rename(columns={'eis_id': 'EIS_ID', 'scc': 'SCC'}, inplace = True)\n",
    "\n",
    "cs_emis.head()\n",
    "\n",
    "if 'PM25_reduction_subpart_tons' not in cs_emis.columns:\n",
    "    print('PM25_reduction_subpart_tons is missing, so now computed')\n",
    "    cs_emis['PM25_reduction_subpart_tons'] = cs_emis['PM25CON_reduction_subpart_tons'] + cs_emis['PM25FIL_reduction_subpart_tons']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Pre-step 2: Check if the subpart_tons matches NEI emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: Define key columns and make a copy\n",
    "NEI_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "CCS_subpart_cols = ['VOC_subpart_tons', 'NOX_subpart_tons', 'NH3_subpart_tons','SO2_subpart_tons', 'PM25_subpart_tons']\n",
    "CCS_changes_cols = ['VOC_increase_SCC_tons', 'NOX_reduction_subpart_tons', 'NH3_increase_SCC_tons','SO2_reduction_subpart_tons', 'PM25_reduction_subpart_tons'] \n",
    "\n",
    "key_cols = ['EIS_ID', 'SCC']\n",
    "gdf_copy = gdf.copy()\n",
    "\n",
    "# STEP 1: Aggregate NEI values by (EIS_ID, SCC)\n",
    "gdf_agg = gdf_copy.groupby(key_cols)[NEI_cols].sum().reset_index()\n",
    "\n",
    "# STEP 2: Merge with cs_emis for comparison\n",
    "compare_df = pd.merge(\n",
    "    gdf_agg,\n",
    "    cs_emis[key_cols + CCS_subpart_cols + CCS_changes_cols],\n",
    "    on=key_cols,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# STEP 3: Compare each NEI vs CCS column\n",
    "for nei_col, ccs_col in zip(NEI_cols, CCS_subpart_cols):\n",
    "    nei_vals = compare_df[nei_col]\n",
    "    ccs_vals = compare_df[ccs_col]\n",
    "    \n",
    "    match_mask = (\n",
    "        np.isclose(nei_vals, ccs_vals, equal_nan=True) |\n",
    "        ((nei_vals.isna() & (ccs_vals == 0)) | ((nei_vals == 0) & ccs_vals.isna()))\n",
    "    )\n",
    "\n",
    "    mismatch_mask = ~match_mask\n",
    "\n",
    "    if mismatch_mask.any():\n",
    "        print(f\"\\n❌ Mismatched values for {nei_col} vs {ccs_col}:\")\n",
    "        print(compare_df.loc[mismatch_mask, key_cols + [nei_col, ccs_col]])\n",
    "        print(compare_df.loc[mismatch_mask, [nei_col, ccs_col]].sum())\n",
    "\n",
    "# Compare the net difference (between NEI and CCS subpart) to the changes of CCS emissions. \n",
    "for chg_col, ccs_col in zip(CCS_changes_cols, CCS_subpart_cols):\n",
    "    print(compare_df.loc[mismatch_mask, [chg_col, ccs_col]].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Important note: There is very small difference in CCS subpart emissions and NEI emissions, likely due to the difference between NEI raw data and the NEI SMOKE formatted data. This might be a bug in NEI SMOKE, but I can't fix this directly. So I am going to apply the changes of CCS emissions into my NEI emissions (from NEI SMOKE formatted data) to avoid this inconsistency issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Step 2: Find the matching group of EIS_ID and SCC between cs_emis and NEI and allocate the cs_emis into the NEI emissions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: Define key columns and make a copy\n",
    "NEI_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "CCS_cols = ['VOC_increase_SCC_tons', 'NOX_reduction_subpart_tons',  'NH3_increase_SCC_tons', 'SO2_reduction_subpart_tons', 'PM25_reduction_subpart_tons'] \n",
    "\n",
    "#CCS_cols = ['VOC_out_subpart_tons', 'NH3_out_subpart_tons', 'NOX_out_subpart_tons','SO2_out_subpart_tons', 'PM25_out_subpart_tons']\n",
    "\n",
    "key_cols = ['EIS_ID', 'SCC']\n",
    "gdf_copy = gdf.copy()\n",
    "\n",
    "# STEP 1: Identify matched (EIS_ID, SCC) pairs\n",
    "matched_keys = pd.merge(\n",
    "    gdf_copy[key_cols].drop_duplicates(),\n",
    "    cs_emis[key_cols].drop_duplicates(),\n",
    "    on=key_cols\n",
    ")\n",
    "\n",
    "# STEP 2: Split gdf into matched and unmatched\n",
    "gdf_matched = gdf_copy.merge(matched_keys, on=key_cols, how='inner')\n",
    "gdf_unmatched = gdf_copy.merge(matched_keys, on=key_cols, how='outer', indicator=True).query('_merge == \"left_only\"').drop(columns='_merge')\n",
    "\n",
    "# STEP 3: Merge CCS values into matched gdf\n",
    "gdf_matched = gdf_matched.merge(cs_emis[key_cols + CCS_cols], on=key_cols, how='left', suffixes=('', '_ccs'))\n",
    "\n",
    "# STEP 4: Allocation only for matched rows\n",
    "def allocate(group):\n",
    "    n = len(group)\n",
    "    for nei_col, ccs_col in zip(NEI_cols, CCS_cols):\n",
    "        ccs_val = group[f\"{ccs_col}\"].iloc[0]\n",
    "\n",
    "        if pd.isna(ccs_val):\n",
    "            group[ccs_col] = np.nan\n",
    "            continue\n",
    "\n",
    "        nei_vals = group[nei_col]\n",
    "        total_nei = nei_vals.sum()\n",
    "\n",
    "        if pd.isna(total_nei):\n",
    "            group[ccs_col] = np.nan\n",
    "        elif total_nei > 0:\n",
    "            group[ccs_col] = (nei_vals / total_nei) * ccs_val\n",
    "        else:\n",
    "            group[ccs_col] = ccs_val / n\n",
    "    return group\n",
    "\n",
    "print(\"=== Allocating CCS only for matched groups ===\")\n",
    "gdf_matched_allocated = gdf_matched.groupby(key_cols, group_keys=False).apply(allocate)\n",
    "\n",
    "# Drop helper columns (e.g., *_ccs) if needed\n",
    "gdf_matched_allocated = gdf_matched_allocated.drop(columns=[f\"{c}_ccs\" for c in CCS_cols], errors='ignore')\n",
    "\n",
    "# STEP 5: Combine with unmatched (with CCS columns as NaN)\n",
    "for c in CCS_cols:\n",
    "    if c not in gdf_unmatched.columns:\n",
    "        gdf_unmatched[c] = np.nan\n",
    "\n",
    "final = pd.concat([gdf_matched_allocated, gdf_unmatched], ignore_index=True)\n",
    "\n",
    "# STEP 6: conservation check for the allocated vs matched original)\n",
    "print(\"\\n=== Conservation Check ===\")\n",
    "cs_allocated = final.groupby(key_cols)[CCS_cols].sum().reset_index()\n",
    "cs_original = cs_emis[key_cols + CCS_cols]\n",
    "check = cs_allocated.merge(cs_original, on=key_cols, suffixes=('_alloc', '_orig'))\n",
    "\n",
    "for col in CCS_cols:\n",
    "    rel_diff = abs(check[f\"{col}_alloc\"] - check[f\"{col}_orig\"]) / (check[f\"{col}_orig\"])\n",
    "    if (rel_diff > 0.00001).any():\n",
    "        print(f\"[Conservation Issue] {col}: max diff = {rel_diff.max()*100:.4f}%\")\n",
    "\n",
    "\n",
    "print(\"=== The number of rows should be matched ===\", gdf.shape, gdf_matched_allocated.shape, gdf_unmatched.shape)\n",
    "print(\"=== The number of rows should be matched ===\", check.shape, matched_keys.shape)\n",
    "\n",
    "# STEP 7: Conservation check for the original and final dataframe (this will be different, if there is unmatched case)\n",
    "total_sum_original = cs_emis[CCS_cols].sum()\n",
    "total_sum_allocated = final[CCS_cols].sum()\n",
    "\n",
    "print(\"\\n=== FINAL CONSERVATION CHECK ===\")\n",
    "print(\"Original Total CCS change Emissions:\")\n",
    "print(total_sum_original)\n",
    "print(\"Allocated Total CCS change Emissions:\")\n",
    "print(total_sum_allocated)\n",
    "print(\"Relative Difference:\")\n",
    "rel_diff = (total_sum_original- total_sum_allocated) / (total_sum_original)\n",
    "print(rel_diff)\n",
    "\n",
    "if any(abs(rel_diff) > 0.0001):\n",
    "    print(\"❌ CONSERVATION FAILED!\")\n",
    "else:\n",
    "    print(\"✅ CONSERVATION PASSED!\")\n",
    "\n",
    "# check if the rows of is_nan matches with allocated cs_emis\n",
    "print(\"=== The number of rows should be matched ===\", gdf.shape, final.shape, gdf_matched_allocated.shape[0] + gdf_unmatched.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Step 3: Assign the unmatched CCS emissions back to the final dataframe. I reset all other emissions to zero except NH3 and VOC CCS emission increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Add unmatched cs_emis groups into final\n",
    "\n",
    "# 1. Identify unmatched (EIS_ID, SCC) in cs_emis that are not in final\n",
    "final_keys = final[['EIS_ID', 'SCC']].drop_duplicates()\n",
    "unmatched_cs_emis = cs_emis.merge(final_keys, on=['EIS_ID', 'SCC'], how='outer', indicator=True).query('_merge == \"left_only\"').drop(columns='_merge')\n",
    "\n",
    "# 2. For each unmatched row, try to get the columns with facility info from final based on EIS_ID (I confirmed that all EIS_ID in the unmatched case are existed in NEI)\n",
    "rest_cols = [col for col in final.columns if col not in CCS_cols + ['SCC', 'EIS_ID']]\n",
    "unmatched_lookup = final.drop_duplicates('EIS_ID')[['EIS_ID'] + rest_cols].set_index('EIS_ID')\n",
    "\n",
    "# Merge the facility columns by EIS_ID\n",
    "unmatched_with_rest = unmatched_cs_emis.merge(unmatched_lookup, on='EIS_ID', how='left')\n",
    "\n",
    "NEI_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "CCS_cols = ['VOC_increase_SCC_tons', 'NOX_reduction_subpart_tons',  'NH3_increase_SCC_tons', 'SO2_reduction_subpart_tons', 'PM25_reduction_subpart_tons'] \n",
    "# 3. Set all NEI emissions and NOx, SO2, and PM2.5 CCS emissions to zero. For VOC and NH3 CCS emissions, ensure they are present\n",
    "for col in CCS_cols:\n",
    "    if col in ['NOX_reduction_subpart_tons', 'SO2_reduction_subpart_tons', 'PM25_reduction_subpart_tons']:\n",
    "        unmatched_with_rest[col] = 0.0\n",
    "\n",
    "for col in NEI_cols:\n",
    "    unmatched_with_rest[col] = 0.0\n",
    "\n",
    "# 4. Keep only columns matching final and reorder\n",
    "final_cols = final.columns\n",
    "unmatched_with_rest = unmatched_with_rest[[col for col in final_cols if col in unmatched_with_rest.columns]]\n",
    "\n",
    "print(\"before adding unmatched_with_rest\", final[CCS_cols].sum(), final.shape)\n",
    "# 5. Append to final\n",
    "final2 = pd.concat([final, unmatched_with_rest], ignore_index=True)\n",
    "\n",
    "print(\"after adding unmatched_with_rest\", final2[CCS_cols].sum(), final2.shape)\n",
    "\n",
    "if final2.shape[0] == gdf_matched_allocated.shape[0] + gdf_unmatched.shape[0]+ unmatched_cs_emis.shape[0]:\n",
    "    print (\"Grat! final # of row are confirmed to be good\")\n",
    "\n",
    "else:\n",
    "    print(\"Bad! something not righ about final # of row using matched and unmatched\", final2.shape[0],  gdf_matched_allocated.shape[0] + gdf_unmatched.shape[0]+ unmatched_cs_emis.shape[0], gdf_matched_allocated.shape[0],  gdf_unmatched.shape[0], unmatched_cs_emis.shape[0])\n",
    "\n",
    "\n",
    "#6 check_conservation between original and final (Since PM2.5, SO2, and NOx unmatched cases are set to zero, abs_diff should catch them.)\n",
    "total_sum_original = cs_emis[CCS_cols].sum()\n",
    "total_sum_allocated = final2[CCS_cols].sum()\n",
    "\n",
    "print(\"\\n=== FINAL CONSERVATION CHECK ===\")\n",
    "print(\"Original Total CCS Emissions:\")\n",
    "print(total_sum_original)\n",
    "print(\"Allocated Total CCS Emissions:\")\n",
    "print(total_sum_allocated)\n",
    "print(\"Absoluate Difference:\")\n",
    "abs_diff = (total_sum_original- total_sum_allocated) \n",
    "print(abs_diff)\n",
    "\n",
    "    \n",
    "# Check if the difference between original and final comes from the \"unmatched\" case (If not, something is wrong)\n",
    "unmatched_sum = unmatched_cs_emis[CCS_cols].sum()\n",
    "for sp in CCS_cols:\n",
    "    if (unmatched_sum [sp] / rel_diff[sp]) > 0.999 :\n",
    "        print(\"✅ CONSERVATION PASSED!\", unmatched_sum [sp], abs_diff[sp] )\n",
    "    else:\n",
    "        print(\"CONSERVATION FAIL!\", unmatched_sum [sp], abs_diff[sp])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Step 4 - Compute the final CCS emissions based on the changed of CCS emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEI_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "CCS_cols = ['VOC_increase_SCC_tons', 'NOX_reduction_subpart_tons',  'NH3_increase_SCC_tons', 'SO2_reduction_subpart_tons', 'PM25_reduction_subpart_tons'] \n",
    "\n",
    "# Step 1: Identify rows where all CCS columns are NaN\n",
    "all_ccs_missing = final2[CCS_cols].isna().all(axis=1) # gdf_match case contains 42 NaN rows due to SCC allocation: gdf_matched[gdf_matched[CCS_cols].isna().all(axis=1)]\n",
    "all_ccs_missing_df = final2[all_ccs_missing]\n",
    "\n",
    "# Save original NEI columns as nei and compute new CCS emissions \n",
    "for nei_col, ccs_col in zip(NEI_cols, CCS_cols):\n",
    "    final2[f'{nei_col}_nei'] = final2[nei_col]\n",
    "\n",
    "    if nei_col in ['NH3', 'VOC']: \n",
    "        final2[nei_col] = final2[f'{nei_col}_nei'] +  final2[ccs_col].fillna(0) \n",
    "    else:\n",
    "        final2[nei_col] = final2[f'{nei_col}_nei'] -  final2[ccs_col].fillna(0)\n",
    "\n",
    "    # Optional: debug first few affected rows\n",
    "    if all_ccs_missing_df[nei_col].sum() > 0:\n",
    "        print(final2.loc[all_ccs_missing, [nei_col, ccs_col]].head())\n",
    "\n",
    "if final2.shape[0] == gdf.shape[0] + unmatched_cs_emis.shape[0]:\n",
    "    print (\"Great! final2 # of row are summed to be gdf # of row and unmatched cs_emis\")\n",
    "else:\n",
    "    print(\"Bad! something not right about final2 # of row using gdf\", final2.shape[0], gdf.shape[0],  unmatched_cs_emis.shape[0])\n",
    "\n",
    "if all_ccs_missing.sum() == gdf_unmatched.shape[0]:\n",
    "    print (\"Great! missing rows are matched with gdf_unmatched case!\")\n",
    "\n",
    "\n",
    "NEI_cols = ['VOC_nei', 'NOx_nei', 'NH3_nei', 'SOx_nei', 'PM2_5_nei']\n",
    "CCS_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']  \n",
    "\n",
    "# Compute total difference for each pollutant\n",
    "diff_dict = {}\n",
    "for nei_col, ccs_col in zip(NEI_cols, CCS_cols):\n",
    "    nei_total = final2[nei_col].sum()\n",
    "    ccs_total = final2[ccs_col].sum()\n",
    "    diff_dict[ccs_col] = ccs_total - nei_total\n",
    "\n",
    "print(diff_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCS_cols = ['VOC_increase_SCC_tons', 'NOX_reduction_subpart_tons',  'NH3_increase_SCC_tons', 'SO2_reduction_subpart_tons', 'PM25_reduction_subpart_tons'] \n",
    "final2[CCS_cols].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Save the final data to the INMAP emission file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCS_cols = ['VOC_increase_SCC_tons', 'NOX_reduction_subpart_tons',  'NH3_increase_SCC_tons', 'SO2_reduction_subpart_tons', 'PM25_reduction_subpart_tons'] \n",
    "final2.drop(CCS_cols, axis=1, inplace=True)\n",
    "\n",
    "filename = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/whole_USA_CCS.shp\"\n",
    "final2.to_file(filename, driver='ESRI Shapefile')\n",
    "print(f\"Saved final data to {filename}\")\n",
    "\n",
    "#reset VOC and NH3 emissions with NEI (no increase)\n",
    "for sp in ['VOC', 'NH3']:\n",
    "    final2[sp+'_ccs'] = final2[sp] \n",
    "    final2[sp] = final2[sp+'_nei'] \n",
    "\n",
    "final2[NEI_cols].sum()\n",
    "\n",
    "filename = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/whole_USA_CCS_wo_NH3_VOC.shp\"\n",
    "final2.to_file(filename, driver='ESRI Shapefile')\n",
    "print(f\"Saved final data to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Plot the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define file paths\n",
    "file_with_ccs = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/whole_USA_CCS.shp\"\n",
    "\n",
    "# Load shapefiles\n",
    "gdf_with_ccs = gpd.read_file(file_with_ccs)\n",
    "gdf_with_ccs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEI_cols = ['VOC_nei', 'NOx_nei', 'NH3_nei', 'SOx_nei', 'PM2_5_nei']\n",
    "CCS_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']  \n",
    "\n",
    "# Compute total difference for each pollutant\n",
    "diff_dict = {}\n",
    "for nei_col, ccs_col in zip(NEI_cols, CCS_cols):\n",
    "    nei_total = gdf_with_ccs[nei_col].sum()\n",
    "    ccs_total = gdf_with_ccs[ccs_col].sum()\n",
    "    diff_dict[ccs_col] = ccs_total - nei_total\n",
    "\n",
    "print(diff_dict)\n",
    "\n",
    "# Convert to Series for plotting\n",
    "diff_series = pd.Series(diff_dict)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(diff_series.index, diff_series.values)\n",
    "plt.ylabel(\"Difference in Total Emissions (CCS - NEI) [tons]\")\n",
    "plt.title(\"Difference Between NEI and CCS Emissions by Pollutant\")\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height,\n",
    "        f'{height:,.2f}',\n",
    "        ha='center',\n",
    "        va='bottom' if height >= 0 else 'top'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"these two emissions sum must be same\")\n",
    "print(gdf_with_ccs[NEI_cols].sum())\n",
    "print(gdf[CCS_cols].sum())\n",
    "\n",
    "print(\"New CCS emissions sum are the followings:\")\n",
    "print(gdf_with_ccs[CCS_cols].sum())\n",
    "\n",
    "\n",
    "print(\"111 difference comes from unmatched CCS emissions, which I reset all emissions to be zero except NH3 and VOC CCS increase emissions\")\n",
    "print(gdf_with_ccs.shape, gdf.shape, gdf_with_ccs.shape[0]- gdf.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Re-create LA_CCS emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define file paths\n",
    "file_with_ccs = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA_CCS.shp\"\n",
    "\n",
    "# Load shapefiles\n",
    "gdf_with_ccs = gpd.read_file(file_with_ccs)\n",
    "gdf_with_ccs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "NEI_cols = ['VOC_nei', 'NOx_nei', 'NH3_nei', 'SOx_nei', 'PM2_5_nei']\n",
    "CCS_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "\n",
    "# Identify Colorado rows using FIPS (Colorado FIPS starts with '08')\n",
    "is_colorado = gdf_with_ccs['FIPS'].str.startswith('22')\n",
    "\n",
    "print(\"after subsetting\", gdf_with_ccs[CCS_cols].sum())\n",
    "\n",
    "# Split into Colorado and other states\n",
    "gdf_STATE = gdf_with_ccs[is_colorado].copy()\n",
    "gdf_other = gdf_with_ccs[~is_colorado].copy()\n",
    "\n",
    "print(\"after subsetting\", gdf_STATE[CCS_cols].sum()+ gdf_other[CCS_cols].sum())\n",
    "\n",
    "# For other states: backup original CCS columns, then replace with NEI columns\n",
    "for nei_col, ccs_col in zip(NEI_cols, CCS_cols):\n",
    "    gdf_other[f\"{ccs_col}_ccs\"] = gdf_other[ccs_col]      # Backup original CCS\n",
    "    gdf_other[ccs_col] = gdf_other[nei_col]              # Overwrite with NEI\n",
    "\n",
    "# Save to file\n",
    "gdf_other.to_file(\"/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA_CCS_without_LA.shp\", driver='ESRI Shapefile')\n",
    "gdf_STATE.to_file(\"/Users/yunhalee/Documents/LOCAETA/CS_emissions/LA_CCS.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "print(\"before reset NH3 and VOC\", gdf_STATE[CCS_cols].sum())\n",
    "\n",
    "#reset VOC and NH3 emissions with NEI (no increase)\n",
    "for sp in ['VOC', 'NH3']:\n",
    "    gdf_STATE[sp+'_ccs'] = gdf_STATE[sp] \n",
    "    gdf_STATE[sp] = gdf_STATE[sp+'_nei'] \n",
    "\n",
    "print(\"after reset NH3 and VOC\", gdf_STATE[CCS_cols].sum())\n",
    "\n",
    "filename = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/LA_CCS_wo_NH3_VOC.shp\"\n",
    "gdf_STATE.to_file(filename, driver='ESRI Shapefile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the subset CCS and plot the emissions\n",
    "filename = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/LA_CCS.shp\"\n",
    "gdf_ccs = gpd.read_file(filename)\n",
    "gdf_ccs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NEI_cols = ['VOC_nei', 'NOx_nei', 'NH3_nei', 'SOx_nei', 'PM2_5_nei']\n",
    "CCS_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']  \n",
    "\n",
    "# Compute total difference for each pollutant\n",
    "diff_dict = {}\n",
    "for nei_col, ccs_col in zip(NEI_cols, CCS_cols):\n",
    "    nei_total = gdf_ccs[nei_col].sum()\n",
    "    ccs_total = gdf_ccs[ccs_col].sum()\n",
    "    diff_dict[ccs_col] = ccs_total - nei_total\n",
    "\n",
    "print(diff_dict)\n",
    "\n",
    "# Convert to Series for plotting\n",
    "diff_series = pd.Series(diff_dict)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(diff_series.index, diff_series.values)\n",
    "plt.ylabel(\"Difference in Total Emissions (CCS - NEI) [tons]\")\n",
    "plt.title(\"Difference Between NEI and CCS Emissions by Pollutant\")\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height,\n",
    "        f'{height:,.2f}',\n",
    "        ha='center',\n",
    "        va='bottom' if height >= 0 else 'top'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Re-create CO_CCS emissions with Kelly's new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define file paths\n",
    "file_with_ccs = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA_CCS.shp\"\n",
    "\n",
    "# Load shapefiles\n",
    "gdf_with_ccs = gpd.read_file(file_with_ccs)\n",
    "gdf_with_ccs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CO_CCS old data to get EIS_ID and SCC for the facilities we need\n",
    "CO_CCS_raw_file = '/Users/yunhalee/Documents/LOCAETA/CS_emissions/final_output_1_manual_update_noLandfill.csv'\n",
    "\n",
    "# process CCS emissions to merge it with NEI emissions\n",
    "co_ccs_old = pd.read_csv(CO_CCS_raw_file)\n",
    "\n",
    "# drop if scc is missing\n",
    "indices_to_drop = co_ccs_old[co_ccs_old['scc'].isna()].index\n",
    "co_ccs_old.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "# drop frs columns (because it introduces duplicates)\n",
    "co_ccs_old.drop(co_ccs_old.filter(regex='frs').columns, axis=1, inplace=True)\n",
    "co_ccs_old.rename(columns={'eis_id': 'EIS_ID', 'scc': 'SCC'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all duplicate rows, including the first occurrence\n",
    "all_duplicates = co_ccs_old[co_ccs_old.duplicated(keep=False)]\n",
    "print(\"All duplicate rows:\")\n",
    "print(all_duplicates)\n",
    "\n",
    "# Identify duplicates\n",
    "duplicate_keys = (\n",
    "    co_ccs_old.groupby(['EIS_ID', 'SCC'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .query('count > 1')[['EIS_ID', 'SCC']]\n",
    ")\n",
    "duplicates = co_ccs_old.merge(duplicate_keys, on=['EIS_ID', 'SCC'], how='inner')\n",
    "duplicates['row_key'] = duplicates.index  # Track original index\n",
    "\n",
    "print(duplicates.shape, co_ccs_old.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any duplicated rows\n",
    "co_ccs_old = co_ccs_old.drop_duplicates()\n",
    "\n",
    "if co_ccs_old.duplicated(subset = ['EIS_ID','SCC']).any():\n",
    "    print(\"Warning: co_ccs_old contains duplicate (EIS_ID, SCC) pairs.\")\n",
    "else:\n",
    "    print(\"All (EIS_ID, SCC) pairs in co_ccs_old are unique.\")\n",
    "\n",
    "# Step 2: Subset gdf_with_ccs using those pairs\n",
    "key_cols = ['EIS_ID', 'SCC']\n",
    "\n",
    "# Merge to get matching rows\n",
    "gdf_subset = gdf_with_ccs.merge(co_ccs_old[key_cols], on=key_cols, how='inner')\n",
    "gdf_rest = gdf_with_ccs.merge(co_ccs_old[key_cols], on=key_cols, how='outer', indicator=True).query('_merge == \"left_only\"').drop(columns=['_merge'])\n",
    "\n",
    "# Optional sanity check\n",
    "assert len(gdf_subset) + len(gdf_rest) == len(gdf_with_ccs), \"❌ Row count mismatch!\"\n",
    "\n",
    "# Preview\n",
    "print(f\"✅ Subset rows: {len(gdf_subset)}\")\n",
    "print(f\"✅ Remaining rows: {len(gdf_rest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there any duplicates in the subset\n",
    "if gdf_subset.duplicated(subset = ['EIS_ID', 'SCC', 'rel_point_', 'source_fil']).any() == True:\n",
    "    print (\"there is duplicated, need an attention\")\n",
    "else:\n",
    "    print(\"all is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "NEI_cols = ['VOC_nei', 'NOx_nei', 'NH3_nei', 'SOx_nei', 'PM2_5_nei']\n",
    "CCS_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']\n",
    "\n",
    "print(\"before subsetting\", gdf_with_ccs[CCS_cols].sum())\n",
    "\n",
    "print(\"after subsetting\", gdf_subset[CCS_cols].sum() + gdf_rest[CCS_cols].sum())\n",
    "\n",
    "# For other states: backup original CCS columns, then replace with NEI columns\n",
    "for nei_col, ccs_col in zip(NEI_cols, CCS_cols):\n",
    "    gdf_rest[f\"{ccs_col}_ccs\"] = gdf_rest[ccs_col]      # Backup original CCS\n",
    "    gdf_rest[ccs_col] = gdf_rest[nei_col]              # Overwrite with NEI\n",
    "\n",
    "# Save to file\n",
    "gdf_rest.to_file(\"/Users/yunhalee/Documents/LOCAETA/CS_emissions/USA_CCS_without_CO_CCS.shp\", driver='ESRI Shapefile')\n",
    "gdf_subset.to_file(\"/Users/yunhalee/Documents/LOCAETA/CS_emissions/CO_CCS.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "print(\"before reset NH3 and VOC\", gdf_subset[CCS_cols].sum())\n",
    "\n",
    "#reset VOC and NH3 emissions with NEI (no increase)\n",
    "for sp in ['VOC', 'NH3']:\n",
    "    gdf_subset[sp+'_ccs'] = gdf_subset[sp] \n",
    "    gdf_subset[sp] = gdf_subset[sp+'_nei'] \n",
    "\n",
    "print(\"after reset NH3 and VOC\", gdf_subset[CCS_cols].sum())\n",
    "\n",
    "filename = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/CO_CCS_wo_NH3_VOC.shp\"\n",
    "gdf_subset.to_file(filename, driver='ESRI Shapefile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the subset CCS and plot the emissions\n",
    "filename = \"/Users/yunhalee/Documents/LOCAETA/CS_emissions/CO_CCS.shp\"\n",
    "gdf_co_ccs = gpd.read_file(filename)\n",
    "gdf_co_ccs.head()\n",
    "\n",
    "NEI_cols = ['VOC_nei', 'NOx_nei', 'NH3_nei', 'SOx_nei', 'PM2_5_nei']\n",
    "CCS_cols = ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']  \n",
    "\n",
    "# Compute total difference for each pollutant\n",
    "diff_dict = {}\n",
    "for nei_col, ccs_col in zip(NEI_cols, CCS_cols):\n",
    "    nei_total = gdf_co_ccs[nei_col].sum()\n",
    "    ccs_total = gdf_co_ccs[ccs_col].sum()\n",
    "    diff_dict[ccs_col] = ccs_total - nei_total\n",
    "\n",
    "print(diff_dict)\n",
    "\n",
    "# Convert to Series for plotting\n",
    "diff_series = pd.Series(diff_dict)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(diff_series.index, diff_series.values)\n",
    "plt.ylabel(\"Difference in Total Emissions (CCS - NEI) [tons]\")\n",
    "plt.title(\"Difference Between NEI and CCS Emissions by Pollutant\")\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height,\n",
    "        f'{height:,.2f}',\n",
    "        ha='center',\n",
    "        va='bottom' if height >= 0 else 'top'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if the subset includes total 9 facilities\n",
    "if len(gdf_subset['EIS_ID'].unique()) == 9:\n",
    "    print(\"All facilities are included\")\n",
    "else:\n",
    "    print(\"Something is not right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
